{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bdd56e-b426-4ede-8be9-593ccdaf023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bf6f94-09a8-407f-823b-4000847ccab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>4.145636</td>\n",
       "      <td>25.017645</td>\n",
       "      <td>-4.061254</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>-3.837345</td>\n",
       "      <td>-13.956994</td>\n",
       "      <td>-2.042957</td>\n",
       "      <td>2.130210</td>\n",
       "      <td>-1.957662</td>\n",
       "      <td>-1.149930</td>\n",
       "      <td>6.082028</td>\n",
       "      <td>0.878612</td>\n",
       "      <td>5.093102</td>\n",
       "      <td>-6.066648</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>-4.060134</td>\n",
       "      <td>2.952843</td>\n",
       "      <td>-5.046353</td>\n",
       "      <td>1.083819</td>\n",
       "      <td>3.978378</td>\n",
       "      <td>-25.072542</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>2.912269</td>\n",
       "      <td>-3.998035</td>\n",
       "      <td>6.069698</td>\n",
       "      <td>4.966187</td>\n",
       "      <td>1.994051</td>\n",
       "      <td>-1.132059</td>\n",
       "      <td>14.906205</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>-5.975194</td>\n",
       "      <td>-23.218408</td>\n",
       "      <td>-9.000630</td>\n",
       "      <td>9.115957</td>\n",
       "      <td>12.097318</td>\n",
       "      <td>-10.954367</td>\n",
       "      <td>-3.930714</td>\n",
       "      <td>-19.069594</td>\n",
       "      <td>-6.118940</td>\n",
       "      <td>-5.001346</td>\n",
       "      <td>-9.105371</td>\n",
       "      <td>-9.894885</td>\n",
       "      <td>10.107614</td>\n",
       "      <td>4.948570</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>2.099845</td>\n",
       "      <td>-15.123774</td>\n",
       "      <td>-0.069867</td>\n",
       "      <td>-0.114247</td>\n",
       "      <td>-1.896109</td>\n",
       "      <td>5.127194</td>\n",
       "      <td>-2.877423</td>\n",
       "      <td>2.970044</td>\n",
       "      <td>-1.099702</td>\n",
       "      <td>3.116767</td>\n",
       "      <td>8.124209</td>\n",
       "      <td>-0.917418</td>\n",
       "      <td>-1.027199</td>\n",
       "      <td>14.048298</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>-4.000506</td>\n",
       "      <td>16.010442</td>\n",
       "      <td>5.961219</td>\n",
       "      <td>9.907115</td>\n",
       "      <td>-0.067754</td>\n",
       "      <td>-9.970728</td>\n",
       "      <td>0.868499</td>\n",
       "      <td>1.892233</td>\n",
       "      <td>-3.161698</td>\n",
       "      <td>-9.225990</td>\n",
       "      <td>3.953956</td>\n",
       "      <td>-17.959652</td>\n",
       "      <td>-3.115491</td>\n",
       "      <td>-6.051674</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>2331</td>\n",
       "      <td>-3.971043</td>\n",
       "      <td>39.913391</td>\n",
       "      <td>16.034626</td>\n",
       "      <td>-19.067697</td>\n",
       "      <td>8.061361</td>\n",
       "      <td>-70.916786</td>\n",
       "      <td>-39.937026</td>\n",
       "      <td>12.834223</td>\n",
       "      <td>-21.937973</td>\n",
       "      <td>14.942994</td>\n",
       "      <td>38.935705</td>\n",
       "      <td>11.739764</td>\n",
       "      <td>12.078842</td>\n",
       "      <td>7.000148</td>\n",
       "      <td>62.908697</td>\n",
       "      <td>-24.981856</td>\n",
       "      <td>-2.804120</td>\n",
       "      <td>-5.847474</td>\n",
       "      <td>-78.034200</td>\n",
       "      <td>-6.052717</td>\n",
       "      <td>-8.027884</td>\n",
       "      <td>27.992994</td>\n",
       "      <td>41.046160</td>\n",
       "      <td>3.086417</td>\n",
       "      <td>-4.954858</td>\n",
       "      <td>-11.106802</td>\n",
       "      <td>-37.863399</td>\n",
       "      <td>31.069292</td>\n",
       "      <td>-4.097017</td>\n",
       "      <td>-13.095192</td>\n",
       "      <td>-5.150284</td>\n",
       "      <td>8.016265</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>2332</td>\n",
       "      <td>-3.011710</td>\n",
       "      <td>-4.060355</td>\n",
       "      <td>-1.046067</td>\n",
       "      <td>4.178137</td>\n",
       "      <td>-2.003243</td>\n",
       "      <td>-2.895017</td>\n",
       "      <td>-2.766757</td>\n",
       "      <td>-29.099123</td>\n",
       "      <td>-4.208953</td>\n",
       "      <td>-4.793855</td>\n",
       "      <td>-1.970525</td>\n",
       "      <td>28.906574</td>\n",
       "      <td>-1.865353</td>\n",
       "      <td>5.974470</td>\n",
       "      <td>-1.957465</td>\n",
       "      <td>-11.014272</td>\n",
       "      <td>-2.107972</td>\n",
       "      <td>-1.883162</td>\n",
       "      <td>-4.918032</td>\n",
       "      <td>0.032044</td>\n",
       "      <td>-1.043068</td>\n",
       "      <td>-3.980494</td>\n",
       "      <td>30.933458</td>\n",
       "      <td>6.871938</td>\n",
       "      <td>-0.134367</td>\n",
       "      <td>-0.867018</td>\n",
       "      <td>23.892336</td>\n",
       "      <td>-11.977934</td>\n",
       "      <td>1.984203</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>28.822082</td>\n",
       "      <td>-0.878670</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>2333</td>\n",
       "      <td>-9.001824</td>\n",
       "      <td>5.985711</td>\n",
       "      <td>-8.146347</td>\n",
       "      <td>-10.902201</td>\n",
       "      <td>5.102105</td>\n",
       "      <td>8.133692</td>\n",
       "      <td>32.877614</td>\n",
       "      <td>-3.017438</td>\n",
       "      <td>-3.174442</td>\n",
       "      <td>-5.724941</td>\n",
       "      <td>17.990090</td>\n",
       "      <td>3.966698</td>\n",
       "      <td>1.066745</td>\n",
       "      <td>3.988601</td>\n",
       "      <td>-40.966145</td>\n",
       "      <td>-1.997519</td>\n",
       "      <td>-5.877931</td>\n",
       "      <td>-5.022308</td>\n",
       "      <td>-25.948680</td>\n",
       "      <td>-15.880140</td>\n",
       "      <td>3.976966</td>\n",
       "      <td>10.996415</td>\n",
       "      <td>57.874418</td>\n",
       "      <td>-7.952857</td>\n",
       "      <td>2.049467</td>\n",
       "      <td>-5.825790</td>\n",
       "      <td>-37.989569</td>\n",
       "      <td>15.014132</td>\n",
       "      <td>1.160272</td>\n",
       "      <td>-11.135889</td>\n",
       "      <td>-7.035763</td>\n",
       "      <td>-0.930067</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>2334</td>\n",
       "      <td>-3.987992</td>\n",
       "      <td>3.011460</td>\n",
       "      <td>-11.949323</td>\n",
       "      <td>-3.810885</td>\n",
       "      <td>16.880234</td>\n",
       "      <td>-5.150117</td>\n",
       "      <td>9.182801</td>\n",
       "      <td>4.960190</td>\n",
       "      <td>-21.002525</td>\n",
       "      <td>-1.881519</td>\n",
       "      <td>-14.979219</td>\n",
       "      <td>-4.015727</td>\n",
       "      <td>-6.948209</td>\n",
       "      <td>-6.052921</td>\n",
       "      <td>65.066757</td>\n",
       "      <td>-8.885683</td>\n",
       "      <td>6.877978</td>\n",
       "      <td>8.861321</td>\n",
       "      <td>-15.161502</td>\n",
       "      <td>2.050648</td>\n",
       "      <td>-0.060606</td>\n",
       "      <td>-1.081436</td>\n",
       "      <td>-6.008398</td>\n",
       "      <td>3.080276</td>\n",
       "      <td>2.054739</td>\n",
       "      <td>-1.052350</td>\n",
       "      <td>-6.019488</td>\n",
       "      <td>-7.075333</td>\n",
       "      <td>-5.826058</td>\n",
       "      <td>-3.989168</td>\n",
       "      <td>14.916905</td>\n",
       "      <td>-12.093426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>2335</td>\n",
       "      <td>-1.838225</td>\n",
       "      <td>-7.023497</td>\n",
       "      <td>-45.877365</td>\n",
       "      <td>20.026927</td>\n",
       "      <td>4.058551</td>\n",
       "      <td>8.062100</td>\n",
       "      <td>19.083782</td>\n",
       "      <td>-21.881795</td>\n",
       "      <td>-9.106341</td>\n",
       "      <td>-1.056355</td>\n",
       "      <td>56.882761</td>\n",
       "      <td>-9.064997</td>\n",
       "      <td>10.027128</td>\n",
       "      <td>-2.950048</td>\n",
       "      <td>-62.952969</td>\n",
       "      <td>2.074686</td>\n",
       "      <td>-26.725765</td>\n",
       "      <td>-9.053823</td>\n",
       "      <td>51.213548</td>\n",
       "      <td>-10.993737</td>\n",
       "      <td>7.971206</td>\n",
       "      <td>7.820410</td>\n",
       "      <td>43.997921</td>\n",
       "      <td>31.130021</td>\n",
       "      <td>5.121935</td>\n",
       "      <td>-1.003704</td>\n",
       "      <td>-58.953961</td>\n",
       "      <td>-22.095226</td>\n",
       "      <td>-0.898581</td>\n",
       "      <td>1.164833</td>\n",
       "      <td>-21.977991</td>\n",
       "      <td>-13.060285</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2335 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   sensor_1   sensor_2   sensor_3   sensor_4   sensor_5   sensor_6  \\\n",
       "0        1  -6.149463  -0.929714   9.058368  -7.017854  -2.958471   0.179233   \n",
       "1        2  -2.238836  -1.003511   5.098079 -10.880357  -0.804562  -2.992123   \n",
       "2        3  19.087934  -2.092514   0.946750 -21.831788   9.119235  17.853587   \n",
       "3        4  -2.211629  -1.930904  21.888406  -3.067560  -0.240634   2.985056   \n",
       "4        5   3.953852   2.964892 -36.044802   0.899838  26.930210  11.004409   \n",
       "...    ...        ...        ...        ...        ...        ...        ...   \n",
       "2330  2331  -3.971043  39.913391  16.034626 -19.067697   8.061361 -70.916786   \n",
       "2331  2332  -3.011710  -4.060355  -1.046067   4.178137  -2.003243  -2.895017   \n",
       "2332  2333  -9.001824   5.985711  -8.146347 -10.902201   5.102105   8.133692   \n",
       "2333  2334  -3.987992   3.011460 -11.949323  -3.810885  16.880234  -5.150117   \n",
       "2334  2335  -1.838225  -7.023497 -45.877365  20.026927   4.058551   8.062100   \n",
       "\n",
       "       sensor_7   sensor_8   sensor_9  sensor_10  sensor_11  sensor_12  \\\n",
       "0     -0.956591  -0.972401   5.956213   4.145636  25.017645  -4.061254   \n",
       "1     26.972724  -8.900861  -5.968298  -4.060134   2.952843  -5.046353   \n",
       "2    -21.069954 -15.933212  -9.016039  -5.975194 -23.218408  -9.000630   \n",
       "3    -29.073369   0.200774  -1.043742   2.099845 -15.123774  -0.069867   \n",
       "4    -21.962423 -11.950189 -20.933785  -4.000506  16.010442   5.961219   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2330 -39.937026  12.834223 -21.937973  14.942994  38.935705  11.739764   \n",
       "2331  -2.766757 -29.099123  -4.208953  -4.793855  -1.970525  28.906574   \n",
       "2332  32.877614  -3.017438  -3.174442  -5.724941  17.990090   3.966698   \n",
       "2333   9.182801   4.960190 -21.002525  -1.881519 -14.979219  -4.015727   \n",
       "2334  19.083782 -21.881795  -9.106341  -1.056355  56.882761  -9.064997   \n",
       "\n",
       "      sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  \\\n",
       "0      0.996632  -3.837345 -13.956994  -2.042957   2.130210  -1.957662   \n",
       "1      1.083819   3.978378 -25.072542  -2.041602   2.912269  -3.998035   \n",
       "2      9.115957  12.097318 -10.954367  -3.930714 -19.069594  -6.118940   \n",
       "3     -0.114247  -1.896109   5.127194  -2.877423   2.970044  -1.099702   \n",
       "4      9.907115  -0.067754  -9.970728   0.868499   1.892233  -3.161698   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2330  12.078842   7.000148  62.908697 -24.981856  -2.804120  -5.847474   \n",
       "2331  -1.865353   5.974470  -1.957465 -11.014272  -2.107972  -1.883162   \n",
       "2332   1.066745   3.988601 -40.966145  -1.997519  -5.877931  -5.022308   \n",
       "2333  -6.948209  -6.052921  65.066757  -8.885683   6.877978   8.861321   \n",
       "2334  10.027128  -2.950048 -62.952969   2.074686 -26.725765  -9.053823   \n",
       "\n",
       "      sensor_19  sensor_20  sensor_21  sensor_22  sensor_23  sensor_24  \\\n",
       "0     -1.149930   6.082028   0.878612   5.093102  -6.066648  -7.026436   \n",
       "1      6.069698   4.966187   1.994051  -1.132059  14.906205  -1.996714   \n",
       "2     -5.001346  -9.105371  -9.894885  10.107614   4.948570  -6.889685   \n",
       "3      3.116767   8.124209  -0.917418  -1.027199  14.048298  -2.126170   \n",
       "4     -9.225990   3.953956 -17.959652  -3.115491  -6.051674  -2.051761   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2330 -78.034200  -6.052717  -8.027884  27.992994  41.046160   3.086417   \n",
       "2331  -4.918032   0.032044  -1.043068  -3.980494  30.933458   6.871938   \n",
       "2332 -25.948680 -15.880140   3.976966  10.996415  57.874418  -7.952857   \n",
       "2333 -15.161502   2.050648  -0.060606  -1.081436  -6.008398   3.080276   \n",
       "2334  51.213548 -10.993737   7.971206   7.820410  43.997921  31.130021   \n",
       "\n",
       "      sensor_25  sensor_26  sensor_27  sensor_28  sensor_29  sensor_30  \\\n",
       "0     -6.006282  -6.005836   7.043084  21.884650  -3.064152  -5.247552   \n",
       "1     -7.933806  -3.136773   8.774211  10.944759   9.858186  -0.969241   \n",
       "2     54.052330  -6.109238  12.154595   6.095989 -40.195088  -3.958124   \n",
       "3     -1.035526   2.178769  10.032723  -1.010897  -3.912848  -2.980338   \n",
       "4     10.917567   1.905335 -13.004707  17.169552   2.105194   3.967986   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2330  -4.954858 -11.106802 -37.863399  31.069292  -4.097017 -13.095192   \n",
       "2331  -0.134367  -0.867018  23.892336 -11.977934   1.984203   0.891666   \n",
       "2332   2.049467  -5.825790 -37.989569  15.014132   1.160272 -11.135889   \n",
       "2333   2.054739  -1.052350  -6.019488  -7.075333  -5.826058  -3.989168   \n",
       "2334   5.121935  -1.003704 -58.953961 -22.095226  -0.898581   1.164833   \n",
       "\n",
       "      sensor_31  sensor_32  target  \n",
       "0     -6.026107 -11.990822       1  \n",
       "1     -3.935553 -15.892421       1  \n",
       "2     -8.079537  -5.160090       0  \n",
       "3    -12.983597  -3.001077       1  \n",
       "4     11.861657 -27.088846       2  \n",
       "...         ...        ...     ...  \n",
       "2330  -5.150284   8.016265       3  \n",
       "2331  28.822082  -0.878670       3  \n",
       "2332  -7.035763  -0.930067       3  \n",
       "2333  14.916905 -12.093426       1  \n",
       "2334 -21.977991 -13.060285       2  \n",
       "\n",
       "[2335 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.067135</td>\n",
       "      <td>5.040383</td>\n",
       "      <td>-2.965405</td>\n",
       "      <td>-12.952880</td>\n",
       "      <td>-1.938465</td>\n",
       "      <td>-10.997802</td>\n",
       "      <td>1.853669</td>\n",
       "      <td>21.077862</td>\n",
       "      <td>7.963131</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>-8.048277</td>\n",
       "      <td>-23.975623</td>\n",
       "      <td>-2.938174</td>\n",
       "      <td>1.757796</td>\n",
       "      <td>3.982687</td>\n",
       "      <td>-18.094173</td>\n",
       "      <td>-0.055343</td>\n",
       "      <td>2.085911</td>\n",
       "      <td>1.885611</td>\n",
       "      <td>3.929806</td>\n",
       "      <td>7.987616</td>\n",
       "      <td>-1.047750</td>\n",
       "      <td>8.128448</td>\n",
       "      <td>5.871833</td>\n",
       "      <td>1.889536</td>\n",
       "      <td>-3.926719</td>\n",
       "      <td>0.929041</td>\n",
       "      <td>4.999071</td>\n",
       "      <td>-7.963270</td>\n",
       "      <td>4.044370</td>\n",
       "      <td>-1.993315</td>\n",
       "      <td>10.855771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.890655</td>\n",
       "      <td>-12.010728</td>\n",
       "      <td>24.881907</td>\n",
       "      <td>38.943245</td>\n",
       "      <td>-4.757496</td>\n",
       "      <td>5.926453</td>\n",
       "      <td>7.923851</td>\n",
       "      <td>3.925545</td>\n",
       "      <td>4.012127</td>\n",
       "      <td>-6.921524</td>\n",
       "      <td>-3.046893</td>\n",
       "      <td>-11.052956</td>\n",
       "      <td>4.912758</td>\n",
       "      <td>16.986902</td>\n",
       "      <td>-11.192310</td>\n",
       "      <td>8.993925</td>\n",
       "      <td>-0.825023</td>\n",
       "      <td>3.969301</td>\n",
       "      <td>-19.889173</td>\n",
       "      <td>-17.971799</td>\n",
       "      <td>0.871808</td>\n",
       "      <td>-8.018209</td>\n",
       "      <td>-7.941582</td>\n",
       "      <td>-27.106928</td>\n",
       "      <td>-16.940167</td>\n",
       "      <td>-11.169073</td>\n",
       "      <td>0.202975</td>\n",
       "      <td>6.846631</td>\n",
       "      <td>7.911875</td>\n",
       "      <td>-2.852819</td>\n",
       "      <td>56.028042</td>\n",
       "      <td>52.967175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-4.809695</td>\n",
       "      <td>0.962863</td>\n",
       "      <td>-1.893647</td>\n",
       "      <td>-6.168084</td>\n",
       "      <td>-3.948282</td>\n",
       "      <td>0.977219</td>\n",
       "      <td>12.960620</td>\n",
       "      <td>-9.048958</td>\n",
       "      <td>5.969053</td>\n",
       "      <td>0.874370</td>\n",
       "      <td>-12.870060</td>\n",
       "      <td>12.018730</td>\n",
       "      <td>-1.972370</td>\n",
       "      <td>-4.989797</td>\n",
       "      <td>7.043337</td>\n",
       "      <td>-1.141632</td>\n",
       "      <td>6.174780</td>\n",
       "      <td>3.030673</td>\n",
       "      <td>-15.931609</td>\n",
       "      <td>9.983416</td>\n",
       "      <td>-3.251407</td>\n",
       "      <td>0.981729</td>\n",
       "      <td>-0.895094</td>\n",
       "      <td>-7.984137</td>\n",
       "      <td>-0.025761</td>\n",
       "      <td>2.981749</td>\n",
       "      <td>4.948704</td>\n",
       "      <td>-10.841968</td>\n",
       "      <td>-4.860024</td>\n",
       "      <td>-6.917495</td>\n",
       "      <td>2.842422</td>\n",
       "      <td>-3.081374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>-3.993114</td>\n",
       "      <td>12.001667</td>\n",
       "      <td>22.960746</td>\n",
       "      <td>-2.121793</td>\n",
       "      <td>-6.002385</td>\n",
       "      <td>2.173744</td>\n",
       "      <td>1.045248</td>\n",
       "      <td>1.073869</td>\n",
       "      <td>-0.019322</td>\n",
       "      <td>-26.977501</td>\n",
       "      <td>-8.132294</td>\n",
       "      <td>-1.062246</td>\n",
       "      <td>-3.028002</td>\n",
       "      <td>27.782447</td>\n",
       "      <td>3.024800</td>\n",
       "      <td>-4.103924</td>\n",
       "      <td>-4.145369</td>\n",
       "      <td>-7.963091</td>\n",
       "      <td>-7.131190</td>\n",
       "      <td>1.070134</td>\n",
       "      <td>1.999624</td>\n",
       "      <td>-10.084452</td>\n",
       "      <td>2.828070</td>\n",
       "      <td>0.903349</td>\n",
       "      <td>5.156367</td>\n",
       "      <td>40.827713</td>\n",
       "      <td>-1.160840</td>\n",
       "      <td>-1.215552</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>-24.968873</td>\n",
       "      <td>-7.148603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-7.841592</td>\n",
       "      <td>-2.129997</td>\n",
       "      <td>7.872448</td>\n",
       "      <td>-8.953428</td>\n",
       "      <td>28.080007</td>\n",
       "      <td>-5.019895</td>\n",
       "      <td>-10.922551</td>\n",
       "      <td>-15.045833</td>\n",
       "      <td>-56.981894</td>\n",
       "      <td>-8.831718</td>\n",
       "      <td>-39.009595</td>\n",
       "      <td>26.919958</td>\n",
       "      <td>-8.052420</td>\n",
       "      <td>13.759281</td>\n",
       "      <td>-18.026002</td>\n",
       "      <td>1.073389</td>\n",
       "      <td>49.029965</td>\n",
       "      <td>-4.794785</td>\n",
       "      <td>37.975778</td>\n",
       "      <td>-1.941377</td>\n",
       "      <td>-21.816534</td>\n",
       "      <td>-7.096878</td>\n",
       "      <td>-2.000975</td>\n",
       "      <td>-23.942689</td>\n",
       "      <td>-6.741350</td>\n",
       "      <td>0.831967</td>\n",
       "      <td>11.905613</td>\n",
       "      <td>3.106004</td>\n",
       "      <td>46.899363</td>\n",
       "      <td>3.001353</td>\n",
       "      <td>-17.881074</td>\n",
       "      <td>-6.882390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>9339</td>\n",
       "      <td>7.963652</td>\n",
       "      <td>7.973099</td>\n",
       "      <td>12.877346</td>\n",
       "      <td>11.015341</td>\n",
       "      <td>-0.963491</td>\n",
       "      <td>-3.926525</td>\n",
       "      <td>-30.875620</td>\n",
       "      <td>6.022538</td>\n",
       "      <td>-1.934085</td>\n",
       "      <td>-2.019713</td>\n",
       "      <td>18.871473</td>\n",
       "      <td>-13.957552</td>\n",
       "      <td>-4.010725</td>\n",
       "      <td>-6.035864</td>\n",
       "      <td>-19.067238</td>\n",
       "      <td>-7.904299</td>\n",
       "      <td>-2.985582</td>\n",
       "      <td>-2.865601</td>\n",
       "      <td>-22.846460</td>\n",
       "      <td>12.993844</td>\n",
       "      <td>-2.084597</td>\n",
       "      <td>-3.056308</td>\n",
       "      <td>-6.913374</td>\n",
       "      <td>-28.872380</td>\n",
       "      <td>-0.822110</td>\n",
       "      <td>3.043326</td>\n",
       "      <td>11.113028</td>\n",
       "      <td>11.160472</td>\n",
       "      <td>2.034703</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-3.959809</td>\n",
       "      <td>22.986533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9339</th>\n",
       "      <td>9340</td>\n",
       "      <td>-7.974772</td>\n",
       "      <td>0.997146</td>\n",
       "      <td>8.011316</td>\n",
       "      <td>-10.007376</td>\n",
       "      <td>1.065419</td>\n",
       "      <td>-15.090632</td>\n",
       "      <td>1.916883</td>\n",
       "      <td>-2.981494</td>\n",
       "      <td>2.948488</td>\n",
       "      <td>12.012327</td>\n",
       "      <td>5.108246</td>\n",
       "      <td>4.060304</td>\n",
       "      <td>2.957766</td>\n",
       "      <td>2.886642</td>\n",
       "      <td>-24.079141</td>\n",
       "      <td>-4.017220</td>\n",
       "      <td>2.159525</td>\n",
       "      <td>2.294092</td>\n",
       "      <td>-7.932833</td>\n",
       "      <td>7.090848</td>\n",
       "      <td>-4.055409</td>\n",
       "      <td>2.042084</td>\n",
       "      <td>0.972399</td>\n",
       "      <td>-4.158987</td>\n",
       "      <td>-4.971529</td>\n",
       "      <td>-9.957906</td>\n",
       "      <td>19.081232</td>\n",
       "      <td>-3.056299</td>\n",
       "      <td>-0.714567</td>\n",
       "      <td>-0.859710</td>\n",
       "      <td>7.950773</td>\n",
       "      <td>2.952029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9340</th>\n",
       "      <td>9341</td>\n",
       "      <td>4.035916</td>\n",
       "      <td>-10.138330</td>\n",
       "      <td>5.943174</td>\n",
       "      <td>8.080807</td>\n",
       "      <td>28.007780</td>\n",
       "      <td>4.981148</td>\n",
       "      <td>23.084630</td>\n",
       "      <td>-30.048268</td>\n",
       "      <td>9.877151</td>\n",
       "      <td>-2.048269</td>\n",
       "      <td>12.057614</td>\n",
       "      <td>28.965250</td>\n",
       "      <td>-28.882513</td>\n",
       "      <td>8.923127</td>\n",
       "      <td>15.983962</td>\n",
       "      <td>-14.007466</td>\n",
       "      <td>-0.879659</td>\n",
       "      <td>-0.053067</td>\n",
       "      <td>-4.005758</td>\n",
       "      <td>-6.065293</td>\n",
       "      <td>-6.823998</td>\n",
       "      <td>-11.014889</td>\n",
       "      <td>-11.013990</td>\n",
       "      <td>-26.176325</td>\n",
       "      <td>23.143486</td>\n",
       "      <td>4.913298</td>\n",
       "      <td>25.066640</td>\n",
       "      <td>4.044110</td>\n",
       "      <td>-1.948533</td>\n",
       "      <td>1.921691</td>\n",
       "      <td>13.952396</td>\n",
       "      <td>11.866490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9341</th>\n",
       "      <td>9342</td>\n",
       "      <td>8.920498</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>20.058309</td>\n",
       "      <td>68.132886</td>\n",
       "      <td>4.037498</td>\n",
       "      <td>19.068331</td>\n",
       "      <td>-4.109146</td>\n",
       "      <td>-4.067740</td>\n",
       "      <td>-11.044263</td>\n",
       "      <td>1.134035</td>\n",
       "      <td>-14.997930</td>\n",
       "      <td>1.094676</td>\n",
       "      <td>1.056345</td>\n",
       "      <td>-7.112650</td>\n",
       "      <td>29.042684</td>\n",
       "      <td>3.040384</td>\n",
       "      <td>-14.068880</td>\n",
       "      <td>-13.890856</td>\n",
       "      <td>-16.047539</td>\n",
       "      <td>-24.053738</td>\n",
       "      <td>4.020378</td>\n",
       "      <td>10.100464</td>\n",
       "      <td>-7.072355</td>\n",
       "      <td>-18.864597</td>\n",
       "      <td>2.902505</td>\n",
       "      <td>-13.936057</td>\n",
       "      <td>3.058599</td>\n",
       "      <td>33.004942</td>\n",
       "      <td>2.932643</td>\n",
       "      <td>9.968502</td>\n",
       "      <td>-3.097104</td>\n",
       "      <td>7.966220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9342</th>\n",
       "      <td>9343</td>\n",
       "      <td>1.042177</td>\n",
       "      <td>-2.928366</td>\n",
       "      <td>-34.982431</td>\n",
       "      <td>-0.114721</td>\n",
       "      <td>-0.127424</td>\n",
       "      <td>2.003162</td>\n",
       "      <td>61.955343</td>\n",
       "      <td>-0.128017</td>\n",
       "      <td>-0.006172</td>\n",
       "      <td>-1.211008</td>\n",
       "      <td>-36.843252</td>\n",
       "      <td>1.082516</td>\n",
       "      <td>2.132128</td>\n",
       "      <td>-5.972903</td>\n",
       "      <td>-32.131849</td>\n",
       "      <td>1.920105</td>\n",
       "      <td>-0.995716</td>\n",
       "      <td>-0.050702</td>\n",
       "      <td>38.991107</td>\n",
       "      <td>-5.979256</td>\n",
       "      <td>-0.086359</td>\n",
       "      <td>2.034965</td>\n",
       "      <td>-12.129050</td>\n",
       "      <td>-1.998121</td>\n",
       "      <td>0.886503</td>\n",
       "      <td>4.933997</td>\n",
       "      <td>-68.115156</td>\n",
       "      <td>13.871766</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>3.811566</td>\n",
       "      <td>77.962102</td>\n",
       "      <td>-22.973813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9343 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sensor_1   sensor_2   sensor_3   sensor_4   sensor_5   sensor_6  \\\n",
       "0        1  0.067135   5.040383  -2.965405 -12.952880  -1.938465 -10.997802   \n",
       "1        2  6.890655 -12.010728  24.881907  38.943245  -4.757496   5.926453   \n",
       "2        3 -4.809695   0.962863  -1.893647  -6.168084  -3.948282   0.977219   \n",
       "3        4  0.018158  -3.993114  12.001667  22.960746  -2.121793  -6.002385   \n",
       "4        5 -7.841592  -2.129997   7.872448  -8.953428  28.080007  -5.019895   \n",
       "...    ...       ...        ...        ...        ...        ...        ...   \n",
       "9338  9339  7.963652   7.973099  12.877346  11.015341  -0.963491  -3.926525   \n",
       "9339  9340 -7.974772   0.997146   8.011316 -10.007376   1.065419 -15.090632   \n",
       "9340  9341  4.035916 -10.138330   5.943174   8.080807  28.007780   4.981148   \n",
       "9341  9342  8.920498   0.033165  20.058309  68.132886   4.037498  19.068331   \n",
       "9342  9343  1.042177  -2.928366 -34.982431  -0.114721  -0.127424   2.003162   \n",
       "\n",
       "       sensor_7   sensor_8   sensor_9  sensor_10  sensor_11  sensor_12  \\\n",
       "0      1.853669  21.077862   7.963131   0.009773  -8.048277 -23.975623   \n",
       "1      7.923851   3.925545   4.012127  -6.921524  -3.046893 -11.052956   \n",
       "2     12.960620  -9.048958   5.969053   0.874370 -12.870060  12.018730   \n",
       "3      2.173744   1.045248   1.073869  -0.019322 -26.977501  -8.132294   \n",
       "4    -10.922551 -15.045833 -56.981894  -8.831718 -39.009595  26.919958   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338 -30.875620   6.022538  -1.934085  -2.019713  18.871473 -13.957552   \n",
       "9339   1.916883  -2.981494   2.948488  12.012327   5.108246   4.060304   \n",
       "9340  23.084630 -30.048268   9.877151  -2.048269  12.057614  28.965250   \n",
       "9341  -4.109146  -4.067740 -11.044263   1.134035 -14.997930   1.094676   \n",
       "9342  61.955343  -0.128017  -0.006172  -1.211008 -36.843252   1.082516   \n",
       "\n",
       "      sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  \\\n",
       "0     -2.938174   1.757796   3.982687 -18.094173  -0.055343   2.085911   \n",
       "1      4.912758  16.986902 -11.192310   8.993925  -0.825023   3.969301   \n",
       "2     -1.972370  -4.989797   7.043337  -1.141632   6.174780   3.030673   \n",
       "3     -1.062246  -3.028002  27.782447   3.024800  -4.103924  -4.145369   \n",
       "4     -8.052420  13.759281 -18.026002   1.073389  49.029965  -4.794785   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338  -4.010725  -6.035864 -19.067238  -7.904299  -2.985582  -2.865601   \n",
       "9339   2.957766   2.886642 -24.079141  -4.017220   2.159525   2.294092   \n",
       "9340 -28.882513   8.923127  15.983962 -14.007466  -0.879659  -0.053067   \n",
       "9341   1.056345  -7.112650  29.042684   3.040384 -14.068880 -13.890856   \n",
       "9342   2.132128  -5.972903 -32.131849   1.920105  -0.995716  -0.050702   \n",
       "\n",
       "      sensor_19  sensor_20  sensor_21  sensor_22  sensor_23  sensor_24  \\\n",
       "0      1.885611   3.929806   7.987616  -1.047750   8.128448   5.871833   \n",
       "1    -19.889173 -17.971799   0.871808  -8.018209  -7.941582 -27.106928   \n",
       "2    -15.931609   9.983416  -3.251407   0.981729  -0.895094  -7.984137   \n",
       "3     -7.963091  -7.131190   1.070134   1.999624 -10.084452   2.828070   \n",
       "4     37.975778  -1.941377 -21.816534  -7.096878  -2.000975 -23.942689   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338 -22.846460  12.993844  -2.084597  -3.056308  -6.913374 -28.872380   \n",
       "9339  -7.932833   7.090848  -4.055409   2.042084   0.972399  -4.158987   \n",
       "9340  -4.005758  -6.065293  -6.823998 -11.014889 -11.013990 -26.176325   \n",
       "9341 -16.047539 -24.053738   4.020378  10.100464  -7.072355 -18.864597   \n",
       "9342  38.991107  -5.979256  -0.086359   2.034965 -12.129050  -1.998121   \n",
       "\n",
       "      sensor_25  sensor_26  sensor_27  sensor_28  sensor_29  sensor_30  \\\n",
       "0      1.889536  -3.926719   0.929041   4.999071  -7.963270   4.044370   \n",
       "1    -16.940167 -11.169073   0.202975   6.846631   7.911875  -2.852819   \n",
       "2     -0.025761   2.981749   4.948704 -10.841968  -4.860024  -6.917495   \n",
       "3      0.903349   5.156367  40.827713  -1.160840  -1.215552   0.037914   \n",
       "4     -6.741350   0.831967  11.905613   3.106004  46.899363   3.001353   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338  -0.822110   3.043326  11.113028  11.160472   2.034703  -0.056564   \n",
       "9339  -4.971529  -9.957906  19.081232  -3.056299  -0.714567  -0.859710   \n",
       "9340  23.143486   4.913298  25.066640   4.044110  -1.948533   1.921691   \n",
       "9341   2.902505 -13.936057   3.058599  33.004942   2.932643   9.968502   \n",
       "9342   0.886503   4.933997 -68.115156  13.871766   0.941964   3.811566   \n",
       "\n",
       "      sensor_31  sensor_32  \n",
       "0     -1.993315  10.855771  \n",
       "1     56.028042  52.967175  \n",
       "2      2.842422  -3.081374  \n",
       "3    -24.968873  -7.148603  \n",
       "4    -17.881074  -6.882390  \n",
       "...         ...        ...  \n",
       "9338  -3.959809  22.986533  \n",
       "9339   7.950773   2.952029  \n",
       "9340  13.952396  11.866490  \n",
       "9341  -3.097104   7.966220  \n",
       "9342  77.962102 -22.973813  \n",
       "\n",
       "[9343 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "display(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a816f9df-4b70-4f8d-baa4-dc82a0459a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>4.145636</td>\n",
       "      <td>25.017645</td>\n",
       "      <td>-4.061254</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>-3.837345</td>\n",
       "      <td>-13.956994</td>\n",
       "      <td>-2.042957</td>\n",
       "      <td>2.130210</td>\n",
       "      <td>-1.957662</td>\n",
       "      <td>-1.149930</td>\n",
       "      <td>6.082028</td>\n",
       "      <td>0.878612</td>\n",
       "      <td>5.093102</td>\n",
       "      <td>-6.066648</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>-4.060134</td>\n",
       "      <td>2.952843</td>\n",
       "      <td>-5.046353</td>\n",
       "      <td>1.083819</td>\n",
       "      <td>3.978378</td>\n",
       "      <td>-25.072542</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>2.912269</td>\n",
       "      <td>-3.998035</td>\n",
       "      <td>6.069698</td>\n",
       "      <td>4.966187</td>\n",
       "      <td>1.994051</td>\n",
       "      <td>-1.132059</td>\n",
       "      <td>14.906205</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>-5.975194</td>\n",
       "      <td>-23.218408</td>\n",
       "      <td>-9.000630</td>\n",
       "      <td>9.115957</td>\n",
       "      <td>12.097318</td>\n",
       "      <td>-10.954367</td>\n",
       "      <td>-3.930714</td>\n",
       "      <td>-19.069594</td>\n",
       "      <td>-6.118940</td>\n",
       "      <td>-5.001346</td>\n",
       "      <td>-9.105371</td>\n",
       "      <td>-9.894885</td>\n",
       "      <td>10.107614</td>\n",
       "      <td>4.948570</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>2.099845</td>\n",
       "      <td>-15.123774</td>\n",
       "      <td>-0.069867</td>\n",
       "      <td>-0.114247</td>\n",
       "      <td>-1.896109</td>\n",
       "      <td>5.127194</td>\n",
       "      <td>-2.877423</td>\n",
       "      <td>2.970044</td>\n",
       "      <td>-1.099702</td>\n",
       "      <td>3.116767</td>\n",
       "      <td>8.124209</td>\n",
       "      <td>-0.917418</td>\n",
       "      <td>-1.027199</td>\n",
       "      <td>14.048298</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>-4.000506</td>\n",
       "      <td>16.010442</td>\n",
       "      <td>5.961219</td>\n",
       "      <td>9.907115</td>\n",
       "      <td>-0.067754</td>\n",
       "      <td>-9.970728</td>\n",
       "      <td>0.868499</td>\n",
       "      <td>1.892233</td>\n",
       "      <td>-3.161698</td>\n",
       "      <td>-9.225990</td>\n",
       "      <td>3.953956</td>\n",
       "      <td>-17.959652</td>\n",
       "      <td>-3.115491</td>\n",
       "      <td>-6.051674</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>9339</td>\n",
       "      <td>7.963652</td>\n",
       "      <td>7.973099</td>\n",
       "      <td>12.877346</td>\n",
       "      <td>11.015341</td>\n",
       "      <td>-0.963491</td>\n",
       "      <td>-3.926525</td>\n",
       "      <td>-30.875620</td>\n",
       "      <td>6.022538</td>\n",
       "      <td>-1.934085</td>\n",
       "      <td>-2.019713</td>\n",
       "      <td>18.871473</td>\n",
       "      <td>-13.957552</td>\n",
       "      <td>-4.010725</td>\n",
       "      <td>-6.035864</td>\n",
       "      <td>-19.067238</td>\n",
       "      <td>-7.904299</td>\n",
       "      <td>-2.985582</td>\n",
       "      <td>-2.865601</td>\n",
       "      <td>-22.846460</td>\n",
       "      <td>12.993844</td>\n",
       "      <td>-2.084597</td>\n",
       "      <td>-3.056308</td>\n",
       "      <td>-6.913374</td>\n",
       "      <td>-28.872380</td>\n",
       "      <td>-0.822110</td>\n",
       "      <td>3.043326</td>\n",
       "      <td>11.113028</td>\n",
       "      <td>11.160472</td>\n",
       "      <td>2.034703</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-3.959809</td>\n",
       "      <td>22.986533</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9339</th>\n",
       "      <td>9340</td>\n",
       "      <td>-7.974772</td>\n",
       "      <td>0.997146</td>\n",
       "      <td>8.011316</td>\n",
       "      <td>-10.007376</td>\n",
       "      <td>1.065419</td>\n",
       "      <td>-15.090632</td>\n",
       "      <td>1.916883</td>\n",
       "      <td>-2.981494</td>\n",
       "      <td>2.948488</td>\n",
       "      <td>12.012327</td>\n",
       "      <td>5.108246</td>\n",
       "      <td>4.060304</td>\n",
       "      <td>2.957766</td>\n",
       "      <td>2.886642</td>\n",
       "      <td>-24.079141</td>\n",
       "      <td>-4.017220</td>\n",
       "      <td>2.159525</td>\n",
       "      <td>2.294092</td>\n",
       "      <td>-7.932833</td>\n",
       "      <td>7.090848</td>\n",
       "      <td>-4.055409</td>\n",
       "      <td>2.042084</td>\n",
       "      <td>0.972399</td>\n",
       "      <td>-4.158987</td>\n",
       "      <td>-4.971529</td>\n",
       "      <td>-9.957906</td>\n",
       "      <td>19.081232</td>\n",
       "      <td>-3.056299</td>\n",
       "      <td>-0.714567</td>\n",
       "      <td>-0.859710</td>\n",
       "      <td>7.950773</td>\n",
       "      <td>2.952029</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9340</th>\n",
       "      <td>9341</td>\n",
       "      <td>4.035916</td>\n",
       "      <td>-10.138330</td>\n",
       "      <td>5.943174</td>\n",
       "      <td>8.080807</td>\n",
       "      <td>28.007780</td>\n",
       "      <td>4.981148</td>\n",
       "      <td>23.084630</td>\n",
       "      <td>-30.048268</td>\n",
       "      <td>9.877151</td>\n",
       "      <td>-2.048269</td>\n",
       "      <td>12.057614</td>\n",
       "      <td>28.965250</td>\n",
       "      <td>-28.882513</td>\n",
       "      <td>8.923127</td>\n",
       "      <td>15.983962</td>\n",
       "      <td>-14.007466</td>\n",
       "      <td>-0.879659</td>\n",
       "      <td>-0.053067</td>\n",
       "      <td>-4.005758</td>\n",
       "      <td>-6.065293</td>\n",
       "      <td>-6.823998</td>\n",
       "      <td>-11.014889</td>\n",
       "      <td>-11.013990</td>\n",
       "      <td>-26.176325</td>\n",
       "      <td>23.143486</td>\n",
       "      <td>4.913298</td>\n",
       "      <td>25.066640</td>\n",
       "      <td>4.044110</td>\n",
       "      <td>-1.948533</td>\n",
       "      <td>1.921691</td>\n",
       "      <td>13.952396</td>\n",
       "      <td>11.866490</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9341</th>\n",
       "      <td>9342</td>\n",
       "      <td>8.920498</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>20.058309</td>\n",
       "      <td>68.132886</td>\n",
       "      <td>4.037498</td>\n",
       "      <td>19.068331</td>\n",
       "      <td>-4.109146</td>\n",
       "      <td>-4.067740</td>\n",
       "      <td>-11.044263</td>\n",
       "      <td>1.134035</td>\n",
       "      <td>-14.997930</td>\n",
       "      <td>1.094676</td>\n",
       "      <td>1.056345</td>\n",
       "      <td>-7.112650</td>\n",
       "      <td>29.042684</td>\n",
       "      <td>3.040384</td>\n",
       "      <td>-14.068880</td>\n",
       "      <td>-13.890856</td>\n",
       "      <td>-16.047539</td>\n",
       "      <td>-24.053738</td>\n",
       "      <td>4.020378</td>\n",
       "      <td>10.100464</td>\n",
       "      <td>-7.072355</td>\n",
       "      <td>-18.864597</td>\n",
       "      <td>2.902505</td>\n",
       "      <td>-13.936057</td>\n",
       "      <td>3.058599</td>\n",
       "      <td>33.004942</td>\n",
       "      <td>2.932643</td>\n",
       "      <td>9.968502</td>\n",
       "      <td>-3.097104</td>\n",
       "      <td>7.966220</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9342</th>\n",
       "      <td>9343</td>\n",
       "      <td>1.042177</td>\n",
       "      <td>-2.928366</td>\n",
       "      <td>-34.982431</td>\n",
       "      <td>-0.114721</td>\n",
       "      <td>-0.127424</td>\n",
       "      <td>2.003162</td>\n",
       "      <td>61.955343</td>\n",
       "      <td>-0.128017</td>\n",
       "      <td>-0.006172</td>\n",
       "      <td>-1.211008</td>\n",
       "      <td>-36.843252</td>\n",
       "      <td>1.082516</td>\n",
       "      <td>2.132128</td>\n",
       "      <td>-5.972903</td>\n",
       "      <td>-32.131849</td>\n",
       "      <td>1.920105</td>\n",
       "      <td>-0.995716</td>\n",
       "      <td>-0.050702</td>\n",
       "      <td>38.991107</td>\n",
       "      <td>-5.979256</td>\n",
       "      <td>-0.086359</td>\n",
       "      <td>2.034965</td>\n",
       "      <td>-12.129050</td>\n",
       "      <td>-1.998121</td>\n",
       "      <td>0.886503</td>\n",
       "      <td>4.933997</td>\n",
       "      <td>-68.115156</td>\n",
       "      <td>13.871766</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>3.811566</td>\n",
       "      <td>77.962102</td>\n",
       "      <td>-22.973813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11678 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   sensor_1   sensor_2   sensor_3   sensor_4   sensor_5   sensor_6  \\\n",
       "0        1  -6.149463  -0.929714   9.058368  -7.017854  -2.958471   0.179233   \n",
       "1        2  -2.238836  -1.003511   5.098079 -10.880357  -0.804562  -2.992123   \n",
       "2        3  19.087934  -2.092514   0.946750 -21.831788   9.119235  17.853587   \n",
       "3        4  -2.211629  -1.930904  21.888406  -3.067560  -0.240634   2.985056   \n",
       "4        5   3.953852   2.964892 -36.044802   0.899838  26.930210  11.004409   \n",
       "...    ...        ...        ...        ...        ...        ...        ...   \n",
       "9338  9339   7.963652   7.973099  12.877346  11.015341  -0.963491  -3.926525   \n",
       "9339  9340  -7.974772   0.997146   8.011316 -10.007376   1.065419 -15.090632   \n",
       "9340  9341   4.035916 -10.138330   5.943174   8.080807  28.007780   4.981148   \n",
       "9341  9342   8.920498   0.033165  20.058309  68.132886   4.037498  19.068331   \n",
       "9342  9343   1.042177  -2.928366 -34.982431  -0.114721  -0.127424   2.003162   \n",
       "\n",
       "       sensor_7   sensor_8   sensor_9  sensor_10  sensor_11  sensor_12  \\\n",
       "0     -0.956591  -0.972401   5.956213   4.145636  25.017645  -4.061254   \n",
       "1     26.972724  -8.900861  -5.968298  -4.060134   2.952843  -5.046353   \n",
       "2    -21.069954 -15.933212  -9.016039  -5.975194 -23.218408  -9.000630   \n",
       "3    -29.073369   0.200774  -1.043742   2.099845 -15.123774  -0.069867   \n",
       "4    -21.962423 -11.950189 -20.933785  -4.000506  16.010442   5.961219   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338 -30.875620   6.022538  -1.934085  -2.019713  18.871473 -13.957552   \n",
       "9339   1.916883  -2.981494   2.948488  12.012327   5.108246   4.060304   \n",
       "9340  23.084630 -30.048268   9.877151  -2.048269  12.057614  28.965250   \n",
       "9341  -4.109146  -4.067740 -11.044263   1.134035 -14.997930   1.094676   \n",
       "9342  61.955343  -0.128017  -0.006172  -1.211008 -36.843252   1.082516   \n",
       "\n",
       "      sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  \\\n",
       "0      0.996632  -3.837345 -13.956994  -2.042957   2.130210  -1.957662   \n",
       "1      1.083819   3.978378 -25.072542  -2.041602   2.912269  -3.998035   \n",
       "2      9.115957  12.097318 -10.954367  -3.930714 -19.069594  -6.118940   \n",
       "3     -0.114247  -1.896109   5.127194  -2.877423   2.970044  -1.099702   \n",
       "4      9.907115  -0.067754  -9.970728   0.868499   1.892233  -3.161698   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338  -4.010725  -6.035864 -19.067238  -7.904299  -2.985582  -2.865601   \n",
       "9339   2.957766   2.886642 -24.079141  -4.017220   2.159525   2.294092   \n",
       "9340 -28.882513   8.923127  15.983962 -14.007466  -0.879659  -0.053067   \n",
       "9341   1.056345  -7.112650  29.042684   3.040384 -14.068880 -13.890856   \n",
       "9342   2.132128  -5.972903 -32.131849   1.920105  -0.995716  -0.050702   \n",
       "\n",
       "      sensor_19  sensor_20  sensor_21  sensor_22  sensor_23  sensor_24  \\\n",
       "0     -1.149930   6.082028   0.878612   5.093102  -6.066648  -7.026436   \n",
       "1      6.069698   4.966187   1.994051  -1.132059  14.906205  -1.996714   \n",
       "2     -5.001346  -9.105371  -9.894885  10.107614   4.948570  -6.889685   \n",
       "3      3.116767   8.124209  -0.917418  -1.027199  14.048298  -2.126170   \n",
       "4     -9.225990   3.953956 -17.959652  -3.115491  -6.051674  -2.051761   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338 -22.846460  12.993844  -2.084597  -3.056308  -6.913374 -28.872380   \n",
       "9339  -7.932833   7.090848  -4.055409   2.042084   0.972399  -4.158987   \n",
       "9340  -4.005758  -6.065293  -6.823998 -11.014889 -11.013990 -26.176325   \n",
       "9341 -16.047539 -24.053738   4.020378  10.100464  -7.072355 -18.864597   \n",
       "9342  38.991107  -5.979256  -0.086359   2.034965 -12.129050  -1.998121   \n",
       "\n",
       "      sensor_25  sensor_26  sensor_27  sensor_28  sensor_29  sensor_30  \\\n",
       "0     -6.006282  -6.005836   7.043084  21.884650  -3.064152  -5.247552   \n",
       "1     -7.933806  -3.136773   8.774211  10.944759   9.858186  -0.969241   \n",
       "2     54.052330  -6.109238  12.154595   6.095989 -40.195088  -3.958124   \n",
       "3     -1.035526   2.178769  10.032723  -1.010897  -3.912848  -2.980338   \n",
       "4     10.917567   1.905335 -13.004707  17.169552   2.105194   3.967986   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338  -0.822110   3.043326  11.113028  11.160472   2.034703  -0.056564   \n",
       "9339  -4.971529  -9.957906  19.081232  -3.056299  -0.714567  -0.859710   \n",
       "9340  23.143486   4.913298  25.066640   4.044110  -1.948533   1.921691   \n",
       "9341   2.902505 -13.936057   3.058599  33.004942   2.932643   9.968502   \n",
       "9342   0.886503   4.933997 -68.115156  13.871766   0.941964   3.811566   \n",
       "\n",
       "      sensor_31  sensor_32  target  \n",
       "0     -6.026107 -11.990822     1.0  \n",
       "1     -3.935553 -15.892421     1.0  \n",
       "2     -8.079537  -5.160090     0.0  \n",
       "3    -12.983597  -3.001077     1.0  \n",
       "4     11.861657 -27.088846     2.0  \n",
       "...         ...        ...     ...  \n",
       "9338  -3.959809  22.986533     NaN  \n",
       "9339   7.950773   2.952029     NaN  \n",
       "9340  13.952396  11.866490     NaN  \n",
       "9341  -3.097104   7.966220     NaN  \n",
       "9342  77.962102 -22.973813     NaN  \n",
       "\n",
       "[11678 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train,test])\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089e01fc-7b44-4ccd-83df-624cd640f6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"target\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3097c0-2f48-4f8f-aace-a97da4635440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>2335.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3971.380031</td>\n",
       "      <td>-0.727016</td>\n",
       "      <td>-0.730355</td>\n",
       "      <td>-0.554850</td>\n",
       "      <td>-0.662290</td>\n",
       "      <td>-0.652891</td>\n",
       "      <td>-0.726206</td>\n",
       "      <td>-0.170430</td>\n",
       "      <td>-0.834182</td>\n",
       "      <td>-0.643565</td>\n",
       "      <td>-0.733348</td>\n",
       "      <td>-0.198107</td>\n",
       "      <td>-0.601434</td>\n",
       "      <td>-0.719430</td>\n",
       "      <td>-0.678917</td>\n",
       "      <td>-0.415666</td>\n",
       "      <td>-0.825397</td>\n",
       "      <td>-0.763773</td>\n",
       "      <td>-0.654712</td>\n",
       "      <td>-0.201069</td>\n",
       "      <td>-0.803224</td>\n",
       "      <td>-0.841546</td>\n",
       "      <td>-0.646985</td>\n",
       "      <td>-0.504992</td>\n",
       "      <td>-0.681878</td>\n",
       "      <td>-0.818142</td>\n",
       "      <td>-0.722025</td>\n",
       "      <td>-0.366451</td>\n",
       "      <td>-0.931796</td>\n",
       "      <td>-0.740038</td>\n",
       "      <td>-0.704565</td>\n",
       "      <td>-0.374873</td>\n",
       "      <td>-0.609712</td>\n",
       "      <td>1.523340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2806.321866</td>\n",
       "      <td>11.765649</td>\n",
       "      <td>7.442849</td>\n",
       "      <td>25.810352</td>\n",
       "      <td>15.409677</td>\n",
       "      <td>11.840976</td>\n",
       "      <td>7.376507</td>\n",
       "      <td>25.984272</td>\n",
       "      <td>15.383893</td>\n",
       "      <td>11.924486</td>\n",
       "      <td>7.394169</td>\n",
       "      <td>25.696557</td>\n",
       "      <td>15.001639</td>\n",
       "      <td>11.962160</td>\n",
       "      <td>7.316656</td>\n",
       "      <td>26.122490</td>\n",
       "      <td>15.471871</td>\n",
       "      <td>12.147932</td>\n",
       "      <td>7.331455</td>\n",
       "      <td>26.348896</td>\n",
       "      <td>15.398272</td>\n",
       "      <td>12.151591</td>\n",
       "      <td>7.347026</td>\n",
       "      <td>26.166904</td>\n",
       "      <td>15.503727</td>\n",
       "      <td>11.961120</td>\n",
       "      <td>7.364072</td>\n",
       "      <td>25.932845</td>\n",
       "      <td>15.159545</td>\n",
       "      <td>12.006291</td>\n",
       "      <td>7.384697</td>\n",
       "      <td>25.552111</td>\n",
       "      <td>15.530098</td>\n",
       "      <td>1.118221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-104.044027</td>\n",
       "      <td>-75.059695</td>\n",
       "      <td>-122.195138</td>\n",
       "      <td>-127.943995</td>\n",
       "      <td>-127.983210</td>\n",
       "      <td>-84.030386</td>\n",
       "      <td>-128.033637</td>\n",
       "      <td>-128.036004</td>\n",
       "      <td>-109.116907</td>\n",
       "      <td>-99.014745</td>\n",
       "      <td>-119.891322</td>\n",
       "      <td>-117.967624</td>\n",
       "      <td>-115.053373</td>\n",
       "      <td>-94.968659</td>\n",
       "      <td>-128.199543</td>\n",
       "      <td>-127.970392</td>\n",
       "      <td>-112.103291</td>\n",
       "      <td>-84.957935</td>\n",
       "      <td>-127.996994</td>\n",
       "      <td>-127.933887</td>\n",
       "      <td>-120.099697</td>\n",
       "      <td>-79.099258</td>\n",
       "      <td>-123.101594</td>\n",
       "      <td>-127.893767</td>\n",
       "      <td>-119.994449</td>\n",
       "      <td>-86.193378</td>\n",
       "      <td>-128.013570</td>\n",
       "      <td>-128.067795</td>\n",
       "      <td>-128.032877</td>\n",
       "      <td>-74.006065</td>\n",
       "      <td>-127.952703</td>\n",
       "      <td>-123.876153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1460.250000</td>\n",
       "      <td>-3.988903</td>\n",
       "      <td>-3.980199</td>\n",
       "      <td>-14.963603</td>\n",
       "      <td>-7.945621</td>\n",
       "      <td>-3.970806</td>\n",
       "      <td>-3.972839</td>\n",
       "      <td>-14.067929</td>\n",
       "      <td>-8.014155</td>\n",
       "      <td>-4.008357</td>\n",
       "      <td>-4.005383</td>\n",
       "      <td>-14.127904</td>\n",
       "      <td>-7.944890</td>\n",
       "      <td>-3.976636</td>\n",
       "      <td>-3.976943</td>\n",
       "      <td>-14.961986</td>\n",
       "      <td>-7.997644</td>\n",
       "      <td>-4.015668</td>\n",
       "      <td>-3.978889</td>\n",
       "      <td>-14.919953</td>\n",
       "      <td>-7.984190</td>\n",
       "      <td>-4.016240</td>\n",
       "      <td>-3.975171</td>\n",
       "      <td>-15.009738</td>\n",
       "      <td>-7.982558</td>\n",
       "      <td>-3.997602</td>\n",
       "      <td>-3.995569</td>\n",
       "      <td>-14.828934</td>\n",
       "      <td>-8.015451</td>\n",
       "      <td>-3.982694</td>\n",
       "      <td>-3.988517</td>\n",
       "      <td>-14.004409</td>\n",
       "      <td>-7.854529</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3504.500000</td>\n",
       "      <td>-0.915240</td>\n",
       "      <td>-0.917024</td>\n",
       "      <td>-1.001158</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.899541</td>\n",
       "      <td>-0.925277</td>\n",
       "      <td>-0.199239</td>\n",
       "      <td>-1.099563</td>\n",
       "      <td>-0.915495</td>\n",
       "      <td>-0.925006</td>\n",
       "      <td>-0.878926</td>\n",
       "      <td>-0.997821</td>\n",
       "      <td>-0.895948</td>\n",
       "      <td>-0.908733</td>\n",
       "      <td>-1.000138</td>\n",
       "      <td>-1.039025</td>\n",
       "      <td>-0.927905</td>\n",
       "      <td>-0.928718</td>\n",
       "      <td>-0.892632</td>\n",
       "      <td>-1.052232</td>\n",
       "      <td>-0.945319</td>\n",
       "      <td>-0.914367</td>\n",
       "      <td>-1.031537</td>\n",
       "      <td>-1.006642</td>\n",
       "      <td>-0.908444</td>\n",
       "      <td>-0.912992</td>\n",
       "      <td>-0.995650</td>\n",
       "      <td>-1.033289</td>\n",
       "      <td>-0.913607</td>\n",
       "      <td>-0.933605</td>\n",
       "      <td>-0.924503</td>\n",
       "      <td>-1.009265</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6423.750000</td>\n",
       "      <td>2.966837</td>\n",
       "      <td>2.909855</td>\n",
       "      <td>13.051898</td>\n",
       "      <td>5.946509</td>\n",
       "      <td>2.968175</td>\n",
       "      <td>2.897539</td>\n",
       "      <td>13.852921</td>\n",
       "      <td>5.072981</td>\n",
       "      <td>2.928966</td>\n",
       "      <td>2.939861</td>\n",
       "      <td>13.124240</td>\n",
       "      <td>5.953424</td>\n",
       "      <td>2.939364</td>\n",
       "      <td>2.929367</td>\n",
       "      <td>13.051995</td>\n",
       "      <td>5.847071</td>\n",
       "      <td>2.915047</td>\n",
       "      <td>2.917989</td>\n",
       "      <td>13.854033</td>\n",
       "      <td>5.745392</td>\n",
       "      <td>2.927610</td>\n",
       "      <td>2.949240</td>\n",
       "      <td>13.011863</td>\n",
       "      <td>5.830494</td>\n",
       "      <td>2.899169</td>\n",
       "      <td>2.905454</td>\n",
       "      <td>13.827493</td>\n",
       "      <td>5.747046</td>\n",
       "      <td>2.948139</td>\n",
       "      <td>2.851940</td>\n",
       "      <td>13.040168</td>\n",
       "      <td>5.928788</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9343.000000</td>\n",
       "      <td>90.037923</td>\n",
       "      <td>54.977145</td>\n",
       "      <td>127.124171</td>\n",
       "      <td>126.130615</td>\n",
       "      <td>106.023697</td>\n",
       "      <td>61.102433</td>\n",
       "      <td>127.202233</td>\n",
       "      <td>126.957325</td>\n",
       "      <td>118.063610</td>\n",
       "      <td>47.030119</td>\n",
       "      <td>127.110419</td>\n",
       "      <td>108.129028</td>\n",
       "      <td>122.980740</td>\n",
       "      <td>44.035605</td>\n",
       "      <td>127.034984</td>\n",
       "      <td>126.983316</td>\n",
       "      <td>102.021476</td>\n",
       "      <td>55.890417</td>\n",
       "      <td>126.927507</td>\n",
       "      <td>127.050356</td>\n",
       "      <td>106.047582</td>\n",
       "      <td>54.009478</td>\n",
       "      <td>127.139450</td>\n",
       "      <td>127.161055</td>\n",
       "      <td>110.988863</td>\n",
       "      <td>76.122970</td>\n",
       "      <td>126.826318</td>\n",
       "      <td>114.034898</td>\n",
       "      <td>105.070831</td>\n",
       "      <td>51.066778</td>\n",
       "      <td>127.040313</td>\n",
       "      <td>126.967283</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      sensor_1      sensor_2      sensor_3      sensor_4  \\\n",
       "count  11678.000000  11678.000000  11678.000000  11678.000000  11678.000000   \n",
       "mean    3971.380031     -0.727016     -0.730355     -0.554850     -0.662290   \n",
       "std     2806.321866     11.765649      7.442849     25.810352     15.409677   \n",
       "min        1.000000   -104.044027    -75.059695   -122.195138   -127.943995   \n",
       "25%     1460.250000     -3.988903     -3.980199    -14.963603     -7.945621   \n",
       "50%     3504.500000     -0.915240     -0.917024     -1.001158     -0.999980   \n",
       "75%     6423.750000      2.966837      2.909855     13.051898      5.946509   \n",
       "max     9343.000000     90.037923     54.977145    127.124171    126.130615   \n",
       "\n",
       "           sensor_5      sensor_6      sensor_7      sensor_8      sensor_9  \\\n",
       "count  11678.000000  11678.000000  11678.000000  11678.000000  11678.000000   \n",
       "mean      -0.652891     -0.726206     -0.170430     -0.834182     -0.643565   \n",
       "std       11.840976      7.376507     25.984272     15.383893     11.924486   \n",
       "min     -127.983210    -84.030386   -128.033637   -128.036004   -109.116907   \n",
       "25%       -3.970806     -3.972839    -14.067929     -8.014155     -4.008357   \n",
       "50%       -0.899541     -0.925277     -0.199239     -1.099563     -0.915495   \n",
       "75%        2.968175      2.897539     13.852921      5.072981      2.928966   \n",
       "max      106.023697     61.102433    127.202233    126.957325    118.063610   \n",
       "\n",
       "          sensor_10     sensor_11     sensor_12     sensor_13     sensor_14  \\\n",
       "count  11678.000000  11678.000000  11678.000000  11678.000000  11678.000000   \n",
       "mean      -0.733348     -0.198107     -0.601434     -0.719430     -0.678917   \n",
       "std        7.394169     25.696557     15.001639     11.962160      7.316656   \n",
       "min      -99.014745   -119.891322   -117.967624   -115.053373    -94.968659   \n",
       "25%       -4.005383    -14.127904     -7.944890     -3.976636     -3.976943   \n",
       "50%       -0.925006     -0.878926     -0.997821     -0.895948     -0.908733   \n",
       "75%        2.939861     13.124240      5.953424      2.939364      2.929367   \n",
       "max       47.030119    127.110419    108.129028    122.980740     44.035605   \n",
       "\n",
       "          sensor_15     sensor_16     sensor_17     sensor_18     sensor_19  \\\n",
       "count  11678.000000  11678.000000  11678.000000  11678.000000  11678.000000   \n",
       "mean      -0.415666     -0.825397     -0.763773     -0.654712     -0.201069   \n",
       "std       26.122490     15.471871     12.147932      7.331455     26.348896   \n",
       "min     -128.199543   -127.970392   -112.103291    -84.957935   -127.996994   \n",
       "25%      -14.961986     -7.997644     -4.015668     -3.978889    -14.919953   \n",
       "50%       -1.000138     -1.039025     -0.927905     -0.928718     -0.892632   \n",
       "75%       13.051995      5.847071      2.915047      2.917989     13.854033   \n",
       "max      127.034984    126.983316    102.021476     55.890417    126.927507   \n",
       "\n",
       "          sensor_20     sensor_21     sensor_22     sensor_23     sensor_24  \\\n",
       "count  11678.000000  11678.000000  11678.000000  11678.000000  11678.000000   \n",
       "mean      -0.803224     -0.841546     -0.646985     -0.504992     -0.681878   \n",
       "std       15.398272     12.151591      7.347026     26.166904     15.503727   \n",
       "min     -127.933887   -120.099697    -79.099258   -123.101594   -127.893767   \n",
       "25%       -7.984190     -4.016240     -3.975171    -15.009738     -7.982558   \n",
       "50%       -1.052232     -0.945319     -0.914367     -1.031537     -1.006642   \n",
       "75%        5.745392      2.927610      2.949240     13.011863      5.830494   \n",
       "max      127.050356    106.047582     54.009478    127.139450    127.161055   \n",
       "\n",
       "          sensor_25     sensor_26     sensor_27     sensor_28     sensor_29  \\\n",
       "count  11678.000000  11678.000000  11678.000000  11678.000000  11678.000000   \n",
       "mean      -0.818142     -0.722025     -0.366451     -0.931796     -0.740038   \n",
       "std       11.961120      7.364072     25.932845     15.159545     12.006291   \n",
       "min     -119.994449    -86.193378   -128.013570   -128.067795   -128.032877   \n",
       "25%       -3.997602     -3.995569    -14.828934     -8.015451     -3.982694   \n",
       "50%       -0.908444     -0.912992     -0.995650     -1.033289     -0.913607   \n",
       "75%        2.899169      2.905454     13.827493      5.747046      2.948139   \n",
       "max      110.988863     76.122970    126.826318    114.034898    105.070831   \n",
       "\n",
       "          sensor_30     sensor_31     sensor_32       target  \n",
       "count  11678.000000  11678.000000  11678.000000  2335.000000  \n",
       "mean      -0.704565     -0.374873     -0.609712     1.523340  \n",
       "std        7.384697     25.552111     15.530098     1.118221  \n",
       "min      -74.006065   -127.952703   -123.876153     0.000000  \n",
       "25%       -3.988517    -14.004409     -7.854529     1.000000  \n",
       "50%       -0.933605     -0.924503     -1.009265     2.000000  \n",
       "75%        2.851940     13.040168      5.928788     3.000000  \n",
       "max       51.066778    127.040313    126.967283     3.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054531cd-cce8-4a7c-b518-c68b2725eebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>sensor_22</th>\n",
       "      <th>sensor_23</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>4.145636</td>\n",
       "      <td>25.017645</td>\n",
       "      <td>-4.061254</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>-3.837345</td>\n",
       "      <td>-13.956994</td>\n",
       "      <td>-2.042957</td>\n",
       "      <td>2.130210</td>\n",
       "      <td>-1.957662</td>\n",
       "      <td>-1.149930</td>\n",
       "      <td>6.082028</td>\n",
       "      <td>0.878612</td>\n",
       "      <td>5.093102</td>\n",
       "      <td>-6.066648</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>-4.060134</td>\n",
       "      <td>2.952843</td>\n",
       "      <td>-5.046353</td>\n",
       "      <td>1.083819</td>\n",
       "      <td>3.978378</td>\n",
       "      <td>-25.072542</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>2.912269</td>\n",
       "      <td>-3.998035</td>\n",
       "      <td>6.069698</td>\n",
       "      <td>4.966187</td>\n",
       "      <td>1.994051</td>\n",
       "      <td>-1.132059</td>\n",
       "      <td>14.906205</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>-5.975194</td>\n",
       "      <td>-23.218408</td>\n",
       "      <td>-9.000630</td>\n",
       "      <td>9.115957</td>\n",
       "      <td>12.097318</td>\n",
       "      <td>-10.954367</td>\n",
       "      <td>-3.930714</td>\n",
       "      <td>-19.069594</td>\n",
       "      <td>-6.118940</td>\n",
       "      <td>-5.001346</td>\n",
       "      <td>-9.105371</td>\n",
       "      <td>-9.894885</td>\n",
       "      <td>10.107614</td>\n",
       "      <td>4.948570</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>2.099845</td>\n",
       "      <td>-15.123774</td>\n",
       "      <td>-0.069867</td>\n",
       "      <td>-0.114247</td>\n",
       "      <td>-1.896109</td>\n",
       "      <td>5.127194</td>\n",
       "      <td>-2.877423</td>\n",
       "      <td>2.970044</td>\n",
       "      <td>-1.099702</td>\n",
       "      <td>3.116767</td>\n",
       "      <td>8.124209</td>\n",
       "      <td>-0.917418</td>\n",
       "      <td>-1.027199</td>\n",
       "      <td>14.048298</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>-4.000506</td>\n",
       "      <td>16.010442</td>\n",
       "      <td>5.961219</td>\n",
       "      <td>9.907115</td>\n",
       "      <td>-0.067754</td>\n",
       "      <td>-9.970728</td>\n",
       "      <td>0.868499</td>\n",
       "      <td>1.892233</td>\n",
       "      <td>-3.161698</td>\n",
       "      <td>-9.225990</td>\n",
       "      <td>3.953956</td>\n",
       "      <td>-17.959652</td>\n",
       "      <td>-3.115491</td>\n",
       "      <td>-6.051674</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>7.963652</td>\n",
       "      <td>7.973099</td>\n",
       "      <td>12.877346</td>\n",
       "      <td>11.015341</td>\n",
       "      <td>-0.963491</td>\n",
       "      <td>-3.926525</td>\n",
       "      <td>-30.875620</td>\n",
       "      <td>6.022538</td>\n",
       "      <td>-1.934085</td>\n",
       "      <td>-2.019713</td>\n",
       "      <td>18.871473</td>\n",
       "      <td>-13.957552</td>\n",
       "      <td>-4.010725</td>\n",
       "      <td>-6.035864</td>\n",
       "      <td>-19.067238</td>\n",
       "      <td>-7.904299</td>\n",
       "      <td>-2.985582</td>\n",
       "      <td>-2.865601</td>\n",
       "      <td>-22.846460</td>\n",
       "      <td>12.993844</td>\n",
       "      <td>-2.084597</td>\n",
       "      <td>-3.056308</td>\n",
       "      <td>-6.913374</td>\n",
       "      <td>-28.872380</td>\n",
       "      <td>-0.822110</td>\n",
       "      <td>3.043326</td>\n",
       "      <td>11.113028</td>\n",
       "      <td>11.160472</td>\n",
       "      <td>2.034703</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-3.959809</td>\n",
       "      <td>22.986533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9339</th>\n",
       "      <td>-7.974772</td>\n",
       "      <td>0.997146</td>\n",
       "      <td>8.011316</td>\n",
       "      <td>-10.007376</td>\n",
       "      <td>1.065419</td>\n",
       "      <td>-15.090632</td>\n",
       "      <td>1.916883</td>\n",
       "      <td>-2.981494</td>\n",
       "      <td>2.948488</td>\n",
       "      <td>12.012327</td>\n",
       "      <td>5.108246</td>\n",
       "      <td>4.060304</td>\n",
       "      <td>2.957766</td>\n",
       "      <td>2.886642</td>\n",
       "      <td>-24.079141</td>\n",
       "      <td>-4.017220</td>\n",
       "      <td>2.159525</td>\n",
       "      <td>2.294092</td>\n",
       "      <td>-7.932833</td>\n",
       "      <td>7.090848</td>\n",
       "      <td>-4.055409</td>\n",
       "      <td>2.042084</td>\n",
       "      <td>0.972399</td>\n",
       "      <td>-4.158987</td>\n",
       "      <td>-4.971529</td>\n",
       "      <td>-9.957906</td>\n",
       "      <td>19.081232</td>\n",
       "      <td>-3.056299</td>\n",
       "      <td>-0.714567</td>\n",
       "      <td>-0.859710</td>\n",
       "      <td>7.950773</td>\n",
       "      <td>2.952029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9340</th>\n",
       "      <td>4.035916</td>\n",
       "      <td>-10.138330</td>\n",
       "      <td>5.943174</td>\n",
       "      <td>8.080807</td>\n",
       "      <td>28.007780</td>\n",
       "      <td>4.981148</td>\n",
       "      <td>23.084630</td>\n",
       "      <td>-30.048268</td>\n",
       "      <td>9.877151</td>\n",
       "      <td>-2.048269</td>\n",
       "      <td>12.057614</td>\n",
       "      <td>28.965250</td>\n",
       "      <td>-28.882513</td>\n",
       "      <td>8.923127</td>\n",
       "      <td>15.983962</td>\n",
       "      <td>-14.007466</td>\n",
       "      <td>-0.879659</td>\n",
       "      <td>-0.053067</td>\n",
       "      <td>-4.005758</td>\n",
       "      <td>-6.065293</td>\n",
       "      <td>-6.823998</td>\n",
       "      <td>-11.014889</td>\n",
       "      <td>-11.013990</td>\n",
       "      <td>-26.176325</td>\n",
       "      <td>23.143486</td>\n",
       "      <td>4.913298</td>\n",
       "      <td>25.066640</td>\n",
       "      <td>4.044110</td>\n",
       "      <td>-1.948533</td>\n",
       "      <td>1.921691</td>\n",
       "      <td>13.952396</td>\n",
       "      <td>11.866490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9341</th>\n",
       "      <td>8.920498</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>20.058309</td>\n",
       "      <td>68.132886</td>\n",
       "      <td>4.037498</td>\n",
       "      <td>19.068331</td>\n",
       "      <td>-4.109146</td>\n",
       "      <td>-4.067740</td>\n",
       "      <td>-11.044263</td>\n",
       "      <td>1.134035</td>\n",
       "      <td>-14.997930</td>\n",
       "      <td>1.094676</td>\n",
       "      <td>1.056345</td>\n",
       "      <td>-7.112650</td>\n",
       "      <td>29.042684</td>\n",
       "      <td>3.040384</td>\n",
       "      <td>-14.068880</td>\n",
       "      <td>-13.890856</td>\n",
       "      <td>-16.047539</td>\n",
       "      <td>-24.053738</td>\n",
       "      <td>4.020378</td>\n",
       "      <td>10.100464</td>\n",
       "      <td>-7.072355</td>\n",
       "      <td>-18.864597</td>\n",
       "      <td>2.902505</td>\n",
       "      <td>-13.936057</td>\n",
       "      <td>3.058599</td>\n",
       "      <td>33.004942</td>\n",
       "      <td>2.932643</td>\n",
       "      <td>9.968502</td>\n",
       "      <td>-3.097104</td>\n",
       "      <td>7.966220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9342</th>\n",
       "      <td>1.042177</td>\n",
       "      <td>-2.928366</td>\n",
       "      <td>-34.982431</td>\n",
       "      <td>-0.114721</td>\n",
       "      <td>-0.127424</td>\n",
       "      <td>2.003162</td>\n",
       "      <td>61.955343</td>\n",
       "      <td>-0.128017</td>\n",
       "      <td>-0.006172</td>\n",
       "      <td>-1.211008</td>\n",
       "      <td>-36.843252</td>\n",
       "      <td>1.082516</td>\n",
       "      <td>2.132128</td>\n",
       "      <td>-5.972903</td>\n",
       "      <td>-32.131849</td>\n",
       "      <td>1.920105</td>\n",
       "      <td>-0.995716</td>\n",
       "      <td>-0.050702</td>\n",
       "      <td>38.991107</td>\n",
       "      <td>-5.979256</td>\n",
       "      <td>-0.086359</td>\n",
       "      <td>2.034965</td>\n",
       "      <td>-12.129050</td>\n",
       "      <td>-1.998121</td>\n",
       "      <td>0.886503</td>\n",
       "      <td>4.933997</td>\n",
       "      <td>-68.115156</td>\n",
       "      <td>13.871766</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>3.811566</td>\n",
       "      <td>77.962102</td>\n",
       "      <td>-22.973813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11678 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sensor_1   sensor_2   sensor_3   sensor_4   sensor_5   sensor_6  \\\n",
       "0     -6.149463  -0.929714   9.058368  -7.017854  -2.958471   0.179233   \n",
       "1     -2.238836  -1.003511   5.098079 -10.880357  -0.804562  -2.992123   \n",
       "2     19.087934  -2.092514   0.946750 -21.831788   9.119235  17.853587   \n",
       "3     -2.211629  -1.930904  21.888406  -3.067560  -0.240634   2.985056   \n",
       "4      3.953852   2.964892 -36.044802   0.899838  26.930210  11.004409   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338   7.963652   7.973099  12.877346  11.015341  -0.963491  -3.926525   \n",
       "9339  -7.974772   0.997146   8.011316 -10.007376   1.065419 -15.090632   \n",
       "9340   4.035916 -10.138330   5.943174   8.080807  28.007780   4.981148   \n",
       "9341   8.920498   0.033165  20.058309  68.132886   4.037498  19.068331   \n",
       "9342   1.042177  -2.928366 -34.982431  -0.114721  -0.127424   2.003162   \n",
       "\n",
       "       sensor_7   sensor_8   sensor_9  sensor_10  sensor_11  sensor_12  \\\n",
       "0     -0.956591  -0.972401   5.956213   4.145636  25.017645  -4.061254   \n",
       "1     26.972724  -8.900861  -5.968298  -4.060134   2.952843  -5.046353   \n",
       "2    -21.069954 -15.933212  -9.016039  -5.975194 -23.218408  -9.000630   \n",
       "3    -29.073369   0.200774  -1.043742   2.099845 -15.123774  -0.069867   \n",
       "4    -21.962423 -11.950189 -20.933785  -4.000506  16.010442   5.961219   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338 -30.875620   6.022538  -1.934085  -2.019713  18.871473 -13.957552   \n",
       "9339   1.916883  -2.981494   2.948488  12.012327   5.108246   4.060304   \n",
       "9340  23.084630 -30.048268   9.877151  -2.048269  12.057614  28.965250   \n",
       "9341  -4.109146  -4.067740 -11.044263   1.134035 -14.997930   1.094676   \n",
       "9342  61.955343  -0.128017  -0.006172  -1.211008 -36.843252   1.082516   \n",
       "\n",
       "      sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  \\\n",
       "0      0.996632  -3.837345 -13.956994  -2.042957   2.130210  -1.957662   \n",
       "1      1.083819   3.978378 -25.072542  -2.041602   2.912269  -3.998035   \n",
       "2      9.115957  12.097318 -10.954367  -3.930714 -19.069594  -6.118940   \n",
       "3     -0.114247  -1.896109   5.127194  -2.877423   2.970044  -1.099702   \n",
       "4      9.907115  -0.067754  -9.970728   0.868499   1.892233  -3.161698   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338  -4.010725  -6.035864 -19.067238  -7.904299  -2.985582  -2.865601   \n",
       "9339   2.957766   2.886642 -24.079141  -4.017220   2.159525   2.294092   \n",
       "9340 -28.882513   8.923127  15.983962 -14.007466  -0.879659  -0.053067   \n",
       "9341   1.056345  -7.112650  29.042684   3.040384 -14.068880 -13.890856   \n",
       "9342   2.132128  -5.972903 -32.131849   1.920105  -0.995716  -0.050702   \n",
       "\n",
       "      sensor_19  sensor_20  sensor_21  sensor_22  sensor_23  sensor_24  \\\n",
       "0     -1.149930   6.082028   0.878612   5.093102  -6.066648  -7.026436   \n",
       "1      6.069698   4.966187   1.994051  -1.132059  14.906205  -1.996714   \n",
       "2     -5.001346  -9.105371  -9.894885  10.107614   4.948570  -6.889685   \n",
       "3      3.116767   8.124209  -0.917418  -1.027199  14.048298  -2.126170   \n",
       "4     -9.225990   3.953956 -17.959652  -3.115491  -6.051674  -2.051761   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338 -22.846460  12.993844  -2.084597  -3.056308  -6.913374 -28.872380   \n",
       "9339  -7.932833   7.090848  -4.055409   2.042084   0.972399  -4.158987   \n",
       "9340  -4.005758  -6.065293  -6.823998 -11.014889 -11.013990 -26.176325   \n",
       "9341 -16.047539 -24.053738   4.020378  10.100464  -7.072355 -18.864597   \n",
       "9342  38.991107  -5.979256  -0.086359   2.034965 -12.129050  -1.998121   \n",
       "\n",
       "      sensor_25  sensor_26  sensor_27  sensor_28  sensor_29  sensor_30  \\\n",
       "0     -6.006282  -6.005836   7.043084  21.884650  -3.064152  -5.247552   \n",
       "1     -7.933806  -3.136773   8.774211  10.944759   9.858186  -0.969241   \n",
       "2     54.052330  -6.109238  12.154595   6.095989 -40.195088  -3.958124   \n",
       "3     -1.035526   2.178769  10.032723  -1.010897  -3.912848  -2.980338   \n",
       "4     10.917567   1.905335 -13.004707  17.169552   2.105194   3.967986   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9338  -0.822110   3.043326  11.113028  11.160472   2.034703  -0.056564   \n",
       "9339  -4.971529  -9.957906  19.081232  -3.056299  -0.714567  -0.859710   \n",
       "9340  23.143486   4.913298  25.066640   4.044110  -1.948533   1.921691   \n",
       "9341   2.902505 -13.936057   3.058599  33.004942   2.932643   9.968502   \n",
       "9342   0.886503   4.933997 -68.115156  13.871766   0.941964   3.811566   \n",
       "\n",
       "      sensor_31  sensor_32  \n",
       "0     -6.026107 -11.990822  \n",
       "1     -3.935553 -15.892421  \n",
       "2     -8.079537  -5.160090  \n",
       "3    -12.983597  -3.001077  \n",
       "4     11.861657 -27.088846  \n",
       "...         ...        ...  \n",
       "9338  -3.959809  22.986533  \n",
       "9339   7.950773   2.952029  \n",
       "9340  13.952396  11.866490  \n",
       "9341  -3.097104   7.966220  \n",
       "9342  77.962102 -22.973813  \n",
       "\n",
       "[11678 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data2 = all_data.drop(columns = [\"id\",\"target\"])\n",
    "all_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f61518-f988-4231-8969-64b42ff7e736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11678, 8, 4, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 흑백 차원으로 변경\n",
    "all_data3 = np.array(all_data2).reshape(-1,8,4,1)\n",
    "all_data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4240550-b9c1-48aa-b0f7-c133960702a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22136749370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(all_data3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6eac08d-eadf-472e-93e2-94e337107777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.4017756"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data3.max() + -all_data3.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47576744-460c-4176-9f9b-8f0111451f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data3 = (all_data3 + 128.199543) / 255.401756 # 전체 데이터가 0과 1사이로 변경, 한번만 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4dd459-833b-461f-a4f9-d5c1306480e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000767418373"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb15a80f-a9f1-44db-bebe-8afbcb752fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data3.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa718340-1463-4535-b632-4d3e07e68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = all_data3[:len(train)]\n",
    "test2 = all_data3[len(train):]\n",
    "#display(train2, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7583a37-df09-4ba2-83e1-fada43ceb43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9694 - acc: 0.6104 - val_loss: 1.5130 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5849 - acc: 0.7935 - val_loss: 2.1445 - val_acc: 0.2500\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4867 - acc: 0.8330 - val_loss: 2.6727 - val_acc: 0.2500\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4247 - acc: 0.8430 - val_loss: 3.6602 - val_acc: 0.2692\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4282 - acc: 0.8472 - val_loss: 4.6687 - val_acc: 0.2500\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.3957 - acc: 0.8577 - val_loss: 4.8245 - val_acc: 0.3013\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3923 - acc: 0.8564 - val_loss: 3.7298 - val_acc: 0.3846\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3765 - acc: 0.8605 - val_loss: 1.5202 - val_acc: 0.6090\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3658 - acc: 0.8697 - val_loss: 1.7597 - val_acc: 0.4231\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3518 - acc: 0.8687 - val_loss: 1.3933 - val_acc: 0.5641\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3549 - acc: 0.8720 - val_loss: 1.6473 - val_acc: 0.6538\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3305 - acc: 0.8816 - val_loss: 0.5644 - val_acc: 0.8013\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3092 - acc: 0.8848 - val_loss: 0.3297 - val_acc: 0.8910\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3336 - acc: 0.8798 - val_loss: 0.6408 - val_acc: 0.7436\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3092 - acc: 0.8922 - val_loss: 0.3731 - val_acc: 0.8590\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.3123 - acc: 0.8844 - val_loss: 0.9652 - val_acc: 0.6731\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2944 - acc: 0.8862 - val_loss: 1.8493 - val_acc: 0.5769\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2908 - acc: 0.8967 - val_loss: 0.4462 - val_acc: 0.8526\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2859 - acc: 0.8931 - val_loss: 0.4674 - val_acc: 0.8333\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2809 - acc: 0.8995 - val_loss: 0.3100 - val_acc: 0.8846\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2567 - acc: 0.9091 - val_loss: 0.4589 - val_acc: 0.8205\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2603 - acc: 0.8990 - val_loss: 2.0426 - val_acc: 0.6154\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2832 - acc: 0.8958 - val_loss: 0.4049 - val_acc: 0.8526\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2438 - acc: 0.9156 - val_loss: 2.0910 - val_acc: 0.5321\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2885 - acc: 0.8935 - val_loss: 1.1068 - val_acc: 0.6667\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2495 - acc: 0.9045 - val_loss: 0.3593 - val_acc: 0.8654\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2190 - acc: 0.9257 - val_loss: 0.3864 - val_acc: 0.8462\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2169 - acc: 0.9229 - val_loss: 0.4676 - val_acc: 0.8141\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2167 - acc: 0.9206 - val_loss: 0.3524 - val_acc: 0.8974\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.2084 - acc: 0.9243 - val_loss: 0.3220 - val_acc: 0.8910\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2154 - acc: 0.9197 - val_loss: 0.3925 - val_acc: 0.8782\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.2147 - acc: 0.9224 - val_loss: 0.3684 - val_acc: 0.8974\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.2043 - acc: 0.9261 - val_loss: 0.3731 - val_acc: 0.8782\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1909 - acc: 0.9344 - val_loss: 0.3968 - val_acc: 0.8462\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1889 - acc: 0.9312 - val_loss: 0.5623 - val_acc: 0.7821\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1854 - acc: 0.9371 - val_loss: 0.7329 - val_acc: 0.7372\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1665 - acc: 0.9413 - val_loss: 0.4759 - val_acc: 0.8397\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1640 - acc: 0.9358 - val_loss: 0.3166 - val_acc: 0.8846\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1618 - acc: 0.9413 - val_loss: 0.3085 - val_acc: 0.8974\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.2242 - acc: 0.9211 - val_loss: 0.8936 - val_acc: 0.6795\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1759 - acc: 0.9367 - val_loss: 0.6173 - val_acc: 0.7308\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1767 - acc: 0.9330 - val_loss: 0.5196 - val_acc: 0.8077\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1721 - acc: 0.9422 - val_loss: 0.3194 - val_acc: 0.9038\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1395 - acc: 0.9518 - val_loss: 0.4684 - val_acc: 0.8462\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1237 - acc: 0.9619 - val_loss: 0.6972 - val_acc: 0.7628\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1181 - acc: 0.9573 - val_loss: 0.7043 - val_acc: 0.7564\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1107 - acc: 0.9615 - val_loss: 0.4157 - val_acc: 0.8846\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1382 - acc: 0.9486 - val_loss: 0.5670 - val_acc: 0.7949\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1120 - acc: 0.9624 - val_loss: 0.3447 - val_acc: 0.8910\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.1338 - acc: 0.9592 - val_loss: 0.5156 - val_acc: 0.8397\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.1048 - acc: 0.9711 - val_loss: 0.3254 - val_acc: 0.8846\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1086 - acc: 0.9624 - val_loss: 0.3050 - val_acc: 0.9103\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0860 - acc: 0.9743 - val_loss: 1.3647 - val_acc: 0.7244\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0806 - acc: 0.9798 - val_loss: 1.0309 - val_acc: 0.7308\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.1074 - acc: 0.9619 - val_loss: 0.7402 - val_acc: 0.7628\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1123 - acc: 0.9656 - val_loss: 0.6977 - val_acc: 0.7821\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0878 - acc: 0.9752 - val_loss: 0.4216 - val_acc: 0.8782\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0803 - acc: 0.9734 - val_loss: 0.4261 - val_acc: 0.8590\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.1173 - acc: 0.9610 - val_loss: 1.0025 - val_acc: 0.6731\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.1301 - acc: 0.9596 - val_loss: 0.4739 - val_acc: 0.8526\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0954 - acc: 0.9693 - val_loss: 0.5479 - val_acc: 0.8205\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0840 - acc: 0.9748 - val_loss: 0.7482 - val_acc: 0.7885\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0577 - acc: 0.9844 - val_loss: 1.2234 - val_acc: 0.6603\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.1164 - acc: 0.9624 - val_loss: 0.4585 - val_acc: 0.8462\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0925 - acc: 0.9725 - val_loss: 0.3640 - val_acc: 0.8846\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0800 - acc: 0.9793 - val_loss: 0.7895 - val_acc: 0.8013\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0656 - acc: 0.9807 - val_loss: 1.0384 - val_acc: 0.7115\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0649 - acc: 0.9807 - val_loss: 0.6498 - val_acc: 0.7949\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.1005 - acc: 0.9697 - val_loss: 0.6370 - val_acc: 0.8077\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0674 - acc: 0.9807 - val_loss: 0.5806 - val_acc: 0.8077\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0583 - acc: 0.9844 - val_loss: 0.4488 - val_acc: 0.8526\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0889 - acc: 0.9729 - val_loss: 0.4354 - val_acc: 0.8654\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0903 - acc: 0.9715 - val_loss: 0.4909 - val_acc: 0.8654\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0717 - acc: 0.9821 - val_loss: 1.0020 - val_acc: 0.7500\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0721 - acc: 0.9780 - val_loss: 0.6750 - val_acc: 0.7821\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0620 - acc: 0.9784 - val_loss: 0.4909 - val_acc: 0.8526\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0797 - acc: 0.9775 - val_loss: 0.5066 - val_acc: 0.8205\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0369 - acc: 0.9940 - val_loss: 0.3970 - val_acc: 0.9038\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0363 - acc: 0.9913 - val_loss: 0.4850 - val_acc: 0.8590\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0425 - acc: 0.9899 - val_loss: 0.5037 - val_acc: 0.8654\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0364 - acc: 0.9936 - val_loss: 0.5632 - val_acc: 0.8333\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.1107 - acc: 0.9624 - val_loss: 0.4361 - val_acc: 0.8782\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0487 - acc: 0.9862 - val_loss: 0.4787 - val_acc: 0.8526\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0399 - acc: 0.9936 - val_loss: 0.4814 - val_acc: 0.8590\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0589 - acc: 0.9826 - val_loss: 0.8475 - val_acc: 0.7628\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0955 - acc: 0.9706 - val_loss: 0.6901 - val_acc: 0.8013\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0514 - acc: 0.9849 - val_loss: 0.5362 - val_acc: 0.8269\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0342 - acc: 0.9922 - val_loss: 0.5493 - val_acc: 0.8654\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0412 - acc: 0.9881 - val_loss: 0.5434 - val_acc: 0.8397\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0296 - acc: 0.9950 - val_loss: 0.5663 - val_acc: 0.8462\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0322 - acc: 0.9936 - val_loss: 0.4772 - val_acc: 0.8782\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0468 - acc: 0.9872 - val_loss: 0.4828 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0242 - acc: 0.9968 - val_loss: 0.4323 - val_acc: 0.8910\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0214 - acc: 0.9977 - val_loss: 0.4443 - val_acc: 0.8846\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0216 - acc: 0.9977 - val_loss: 0.4427 - val_acc: 0.8782\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0142 - acc: 0.9991 - val_loss: 0.4451 - val_acc: 0.8718\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0192 - acc: 0.9982 - val_loss: 0.4637 - val_acc: 0.8782\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0153 - acc: 0.9995 - val_loss: 0.4785 - val_acc: 0.8718\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0209 - acc: 0.9968 - val_loss: 0.4517 - val_acc: 0.8782\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0168 - acc: 0.9982 - val_loss: 0.4583 - val_acc: 0.8654\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0169 - acc: 0.9986 - val_loss: 0.4558 - val_acc: 0.8846\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0139 - acc: 1.0000 - val_loss: 0.4493 - val_acc: 0.8782\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9556 - acc: 0.6214 - val_loss: 1.4238 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5989 - acc: 0.7829 - val_loss: 1.6463 - val_acc: 0.2500\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4935 - acc: 0.8206 - val_loss: 2.3139 - val_acc: 0.2500\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4381 - acc: 0.8408 - val_loss: 3.2027 - val_acc: 0.2500\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4081 - acc: 0.8463 - val_loss: 4.8799 - val_acc: 0.2500\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4184 - acc: 0.8421 - val_loss: 5.9174 - val_acc: 0.2628\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3785 - acc: 0.8614 - val_loss: 4.6803 - val_acc: 0.2628\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3585 - acc: 0.8697 - val_loss: 2.8512 - val_acc: 0.3782\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3811 - acc: 0.8591 - val_loss: 1.6454 - val_acc: 0.6731\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3567 - acc: 0.8669 - val_loss: 0.8196 - val_acc: 0.6987\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3467 - acc: 0.8752 - val_loss: 0.6540 - val_acc: 0.7436\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3222 - acc: 0.8765 - val_loss: 1.3430 - val_acc: 0.5705\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3058 - acc: 0.8871 - val_loss: 0.9074 - val_acc: 0.6923\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3023 - acc: 0.8885 - val_loss: 0.6181 - val_acc: 0.7500\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3069 - acc: 0.8885 - val_loss: 0.5068 - val_acc: 0.8333\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2695 - acc: 0.8926 - val_loss: 0.5010 - val_acc: 0.8462\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2632 - acc: 0.9055 - val_loss: 0.4903 - val_acc: 0.8013\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2489 - acc: 0.9018 - val_loss: 0.5526 - val_acc: 0.8077\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2438 - acc: 0.9160 - val_loss: 0.7711 - val_acc: 0.7949\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2429 - acc: 0.9105 - val_loss: 0.8457 - val_acc: 0.7500\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2331 - acc: 0.9101 - val_loss: 0.5216 - val_acc: 0.8333\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2586 - acc: 0.9032 - val_loss: 0.6657 - val_acc: 0.7885\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2213 - acc: 0.9179 - val_loss: 0.8098 - val_acc: 0.7372\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2017 - acc: 0.9293 - val_loss: 0.7107 - val_acc: 0.7500\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.1855 - acc: 0.9385 - val_loss: 1.0577 - val_acc: 0.6731\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2177 - acc: 0.9234 - val_loss: 0.9443 - val_acc: 0.7244\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.1970 - acc: 0.9371 - val_loss: 0.9121 - val_acc: 0.7115\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2072 - acc: 0.9215 - val_loss: 0.9952 - val_acc: 0.7308\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1841 - acc: 0.9321 - val_loss: 0.4870 - val_acc: 0.8141\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1769 - acc: 0.9367 - val_loss: 0.5908 - val_acc: 0.7949\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1457 - acc: 0.9532 - val_loss: 0.5913 - val_acc: 0.8141\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1662 - acc: 0.9422 - val_loss: 0.6207 - val_acc: 0.7885\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1836 - acc: 0.9312 - val_loss: 0.8698 - val_acc: 0.6667\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1877 - acc: 0.9358 - val_loss: 0.7856 - val_acc: 0.7756\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1594 - acc: 0.9536 - val_loss: 0.5676 - val_acc: 0.8333\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1267 - acc: 0.9601 - val_loss: 0.5737 - val_acc: 0.8333\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1385 - acc: 0.9518 - val_loss: 0.7580 - val_acc: 0.7885\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1229 - acc: 0.9592 - val_loss: 0.5754 - val_acc: 0.8205\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1274 - acc: 0.9592 - val_loss: 0.7121 - val_acc: 0.7885\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.0962 - acc: 0.9734 - val_loss: 0.6473 - val_acc: 0.8077\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1175 - acc: 0.9642 - val_loss: 0.6806 - val_acc: 0.7821\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1359 - acc: 0.9564 - val_loss: 1.2181 - val_acc: 0.7115\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1294 - acc: 0.9559 - val_loss: 0.7463 - val_acc: 0.8077\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1382 - acc: 0.9559 - val_loss: 0.5988 - val_acc: 0.8205\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.0978 - acc: 0.9693 - val_loss: 0.7053 - val_acc: 0.8077\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1011 - acc: 0.9624 - val_loss: 0.7205 - val_acc: 0.8205\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1034 - acc: 0.9665 - val_loss: 0.8171 - val_acc: 0.7628\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1185 - acc: 0.9582 - val_loss: 0.7989 - val_acc: 0.7949\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1060 - acc: 0.9624 - val_loss: 0.8116 - val_acc: 0.7436\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0668 - acc: 0.9803 - val_loss: 0.8147 - val_acc: 0.7692\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0847 - acc: 0.9715 - val_loss: 0.6158 - val_acc: 0.8077\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1302 - acc: 0.9601 - val_loss: 0.7292 - val_acc: 0.7821\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0933 - acc: 0.9715 - val_loss: 1.3542 - val_acc: 0.6987\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0733 - acc: 0.9807 - val_loss: 0.9017 - val_acc: 0.7564\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.1240 - acc: 0.9582 - val_loss: 0.6910 - val_acc: 0.8077\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0741 - acc: 0.9757 - val_loss: 0.6793 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0452 - acc: 0.9908 - val_loss: 0.6790 - val_acc: 0.8077\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0539 - acc: 0.9862 - val_loss: 0.6758 - val_acc: 0.8141\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0415 - acc: 0.9940 - val_loss: 0.6931 - val_acc: 0.8269\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0408 - acc: 0.9931 - val_loss: 0.6864 - val_acc: 0.8141\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0401 - acc: 0.9936 - val_loss: 0.7115 - val_acc: 0.8205\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0382 - acc: 0.9954 - val_loss: 0.6756 - val_acc: 0.8333\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0361 - acc: 0.9963 - val_loss: 0.6761 - val_acc: 0.8205\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0381 - acc: 0.9954 - val_loss: 0.6729 - val_acc: 0.8205\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0397 - acc: 0.9927 - val_loss: 0.6671 - val_acc: 0.8077\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0357 - acc: 0.9945 - val_loss: 0.6878 - val_acc: 0.8269\n",
      "Epoch 1/1000\n",
      "69/69 - 1s - loss: 0.9972 - acc: 0.5920 - val_loss: 1.4220 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5983 - acc: 0.7820 - val_loss: 1.4714 - val_acc: 0.2500\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4985 - acc: 0.8242 - val_loss: 2.4370 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4390 - acc: 0.8444 - val_loss: 4.6487 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4215 - acc: 0.8435 - val_loss: 5.8888 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4028 - acc: 0.8614 - val_loss: 3.8636 - val_acc: 0.2436\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3884 - acc: 0.8518 - val_loss: 2.3231 - val_acc: 0.3205\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3524 - acc: 0.8678 - val_loss: 1.7133 - val_acc: 0.5769\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3427 - acc: 0.8747 - val_loss: 1.3462 - val_acc: 0.4872\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3437 - acc: 0.8674 - val_loss: 0.7608 - val_acc: 0.6603\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3231 - acc: 0.8811 - val_loss: 0.7471 - val_acc: 0.7372\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3131 - acc: 0.8857 - val_loss: 0.3949 - val_acc: 0.8718\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3096 - acc: 0.8821 - val_loss: 0.3705 - val_acc: 0.8526\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3098 - acc: 0.8894 - val_loss: 0.5294 - val_acc: 0.8013\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3295 - acc: 0.8770 - val_loss: 0.3998 - val_acc: 0.8526\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2853 - acc: 0.8931 - val_loss: 0.4530 - val_acc: 0.8590\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2829 - acc: 0.9045 - val_loss: 0.3959 - val_acc: 0.8526\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2614 - acc: 0.8990 - val_loss: 0.8712 - val_acc: 0.6923\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2526 - acc: 0.9082 - val_loss: 0.3669 - val_acc: 0.8526\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2516 - acc: 0.9087 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2571 - acc: 0.9050 - val_loss: 0.4367 - val_acc: 0.8397\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2362 - acc: 0.9128 - val_loss: 0.8577 - val_acc: 0.7372\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2378 - acc: 0.9114 - val_loss: 0.5038 - val_acc: 0.8013\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2184 - acc: 0.9234 - val_loss: 0.4287 - val_acc: 0.8590\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2531 - acc: 0.9059 - val_loss: 0.3101 - val_acc: 0.9038\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2551 - acc: 0.9128 - val_loss: 0.3872 - val_acc: 0.8269\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2249 - acc: 0.9192 - val_loss: 0.4997 - val_acc: 0.8269\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2327 - acc: 0.9201 - val_loss: 0.4103 - val_acc: 0.8590\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2159 - acc: 0.9220 - val_loss: 0.3832 - val_acc: 0.8526\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.2334 - acc: 0.9183 - val_loss: 0.4645 - val_acc: 0.8333\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2195 - acc: 0.9247 - val_loss: 0.4037 - val_acc: 0.8526\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1903 - acc: 0.9325 - val_loss: 0.3786 - val_acc: 0.8462\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1838 - acc: 0.9348 - val_loss: 0.3117 - val_acc: 0.8654\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1570 - acc: 0.9440 - val_loss: 0.4140 - val_acc: 0.8462\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1679 - acc: 0.9408 - val_loss: 0.4903 - val_acc: 0.8462\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1881 - acc: 0.9325 - val_loss: 0.3193 - val_acc: 0.8718\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1568 - acc: 0.9463 - val_loss: 0.3451 - val_acc: 0.8846\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1304 - acc: 0.9592 - val_loss: 0.3427 - val_acc: 0.8782\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1359 - acc: 0.9518 - val_loss: 0.3796 - val_acc: 0.8654\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1211 - acc: 0.9601 - val_loss: 0.4127 - val_acc: 0.8590\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1308 - acc: 0.9546 - val_loss: 0.3520 - val_acc: 0.8654\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1353 - acc: 0.9578 - val_loss: 0.3334 - val_acc: 0.9038\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1101 - acc: 0.9688 - val_loss: 0.3331 - val_acc: 0.8910\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.0952 - acc: 0.9688 - val_loss: 0.4174 - val_acc: 0.8718\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.0972 - acc: 0.9683 - val_loss: 0.3030 - val_acc: 0.9038\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1078 - acc: 0.9660 - val_loss: 0.3785 - val_acc: 0.8718\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.0928 - acc: 0.9706 - val_loss: 0.3505 - val_acc: 0.8590\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0896 - acc: 0.9743 - val_loss: 0.4205 - val_acc: 0.8526\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1560 - acc: 0.9440 - val_loss: 0.4205 - val_acc: 0.8654\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0861 - acc: 0.9766 - val_loss: 0.7556 - val_acc: 0.7756\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0806 - acc: 0.9775 - val_loss: 0.3032 - val_acc: 0.9038\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0752 - acc: 0.9775 - val_loss: 0.4191 - val_acc: 0.8397\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.1045 - acc: 0.9660 - val_loss: 0.6097 - val_acc: 0.7821\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0562 - acc: 0.9885 - val_loss: 0.3673 - val_acc: 0.8654\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0762 - acc: 0.9789 - val_loss: 0.2867 - val_acc: 0.9103\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1235 - acc: 0.9578 - val_loss: 0.4466 - val_acc: 0.8654\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0727 - acc: 0.9780 - val_loss: 0.4583 - val_acc: 0.8397\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0700 - acc: 0.9812 - val_loss: 0.5657 - val_acc: 0.8269\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0579 - acc: 0.9853 - val_loss: 0.3952 - val_acc: 0.8526\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0606 - acc: 0.9839 - val_loss: 0.5378 - val_acc: 0.8462\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0527 - acc: 0.9890 - val_loss: 0.4100 - val_acc: 0.8974\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0954 - acc: 0.9711 - val_loss: 0.5727 - val_acc: 0.8141\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0591 - acc: 0.9826 - val_loss: 0.4673 - val_acc: 0.8269\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0498 - acc: 0.9885 - val_loss: 0.4293 - val_acc: 0.8654\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0697 - acc: 0.9766 - val_loss: 0.6235 - val_acc: 0.8077\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0946 - acc: 0.9660 - val_loss: 0.5800 - val_acc: 0.8333\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0799 - acc: 0.9738 - val_loss: 0.4278 - val_acc: 0.8654\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0609 - acc: 0.9844 - val_loss: 0.3540 - val_acc: 0.8846\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0371 - acc: 0.9922 - val_loss: 0.4075 - val_acc: 0.8654\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0732 - acc: 0.9784 - val_loss: 0.4758 - val_acc: 0.8526\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0649 - acc: 0.9839 - val_loss: 0.4841 - val_acc: 0.8526\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0971 - acc: 0.9651 - val_loss: 0.4970 - val_acc: 0.8397\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.1337 - acc: 0.9514 - val_loss: 0.4713 - val_acc: 0.8269\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.1032 - acc: 0.9647 - val_loss: 0.4453 - val_acc: 0.8846\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0573 - acc: 0.9853 - val_loss: 0.3665 - val_acc: 0.8910\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0494 - acc: 0.9853 - val_loss: 0.3695 - val_acc: 0.8974\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0368 - acc: 0.9913 - val_loss: 0.3759 - val_acc: 0.8718\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0650 - acc: 0.9816 - val_loss: 0.4337 - val_acc: 0.8782\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0481 - acc: 0.9862 - val_loss: 0.4839 - val_acc: 0.8654\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0633 - acc: 0.9821 - val_loss: 0.4655 - val_acc: 0.8462\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0344 - acc: 0.9927 - val_loss: 0.4340 - val_acc: 0.8846\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0246 - acc: 0.9982 - val_loss: 0.3922 - val_acc: 0.8846\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0625 - acc: 0.9803 - val_loss: 0.3592 - val_acc: 0.8718\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0404 - acc: 0.9917 - val_loss: 0.4393 - val_acc: 0.8526\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0807 - acc: 0.9743 - val_loss: 0.4791 - val_acc: 0.8333\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0353 - acc: 0.9931 - val_loss: 0.3629 - val_acc: 0.8718\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0411 - acc: 0.9890 - val_loss: 0.5277 - val_acc: 0.8397\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0889 - acc: 0.9665 - val_loss: 0.4840 - val_acc: 0.8462\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0453 - acc: 0.9876 - val_loss: 0.3720 - val_acc: 0.8910\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0660 - acc: 0.9798 - val_loss: 0.4965 - val_acc: 0.8397\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0798 - acc: 0.9761 - val_loss: 0.4196 - val_acc: 0.8782\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.1242 - acc: 0.9532 - val_loss: 0.5369 - val_acc: 0.8590\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0594 - acc: 0.9835 - val_loss: 0.4047 - val_acc: 0.8782\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0390 - acc: 0.9936 - val_loss: 0.3966 - val_acc: 0.8974\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0449 - acc: 0.9894 - val_loss: 0.3819 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0321 - acc: 0.9931 - val_loss: 0.3767 - val_acc: 0.8974\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0205 - acc: 0.9986 - val_loss: 0.3787 - val_acc: 0.8974\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0272 - acc: 0.9954 - val_loss: 0.3839 - val_acc: 0.8910\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0258 - acc: 0.9963 - val_loss: 0.3822 - val_acc: 0.8846\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0199 - acc: 0.9977 - val_loss: 0.3785 - val_acc: 0.9038\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0203 - acc: 0.9972 - val_loss: 0.3773 - val_acc: 0.9038\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0189 - acc: 0.9995 - val_loss: 0.3828 - val_acc: 0.9038\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0182 - acc: 0.9977 - val_loss: 0.3968 - val_acc: 0.8974\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0191 - acc: 0.9977 - val_loss: 0.3973 - val_acc: 0.8846\n",
      "Epoch 105/1000\n",
      "69/69 - 0s - loss: 0.0155 - acc: 0.9986 - val_loss: 0.3964 - val_acc: 0.8846\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9785 - acc: 0.5925 - val_loss: 1.4210 - val_acc: 0.2372\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6415 - acc: 0.7650 - val_loss: 1.7308 - val_acc: 0.2372\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5286 - acc: 0.8077 - val_loss: 2.4915 - val_acc: 0.2372\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4647 - acc: 0.8256 - val_loss: 3.5025 - val_acc: 0.2372\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4428 - acc: 0.8385 - val_loss: 4.6300 - val_acc: 0.2372\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4241 - acc: 0.8417 - val_loss: 5.3217 - val_acc: 0.2372\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.4175 - acc: 0.8486 - val_loss: 4.2681 - val_acc: 0.2436\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3936 - acc: 0.8476 - val_loss: 2.8005 - val_acc: 0.3077\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3660 - acc: 0.8614 - val_loss: 2.0688 - val_acc: 0.3526\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3464 - acc: 0.8692 - val_loss: 1.0583 - val_acc: 0.7051\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3504 - acc: 0.8692 - val_loss: 1.0091 - val_acc: 0.6218\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3452 - acc: 0.8770 - val_loss: 1.2583 - val_acc: 0.6218\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3498 - acc: 0.8697 - val_loss: 0.5249 - val_acc: 0.7885\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3372 - acc: 0.8706 - val_loss: 0.8507 - val_acc: 0.7115\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3123 - acc: 0.8862 - val_loss: 0.5144 - val_acc: 0.8205\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2916 - acc: 0.8880 - val_loss: 0.6067 - val_acc: 0.7628\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.3006 - acc: 0.8876 - val_loss: 0.9243 - val_acc: 0.6154\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2828 - acc: 0.8963 - val_loss: 0.5339 - val_acc: 0.7949\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2844 - acc: 0.8977 - val_loss: 1.1624 - val_acc: 0.6282\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2559 - acc: 0.9055 - val_loss: 0.5683 - val_acc: 0.7885\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2436 - acc: 0.9059 - val_loss: 0.4589 - val_acc: 0.8269\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2396 - acc: 0.9128 - val_loss: 0.4673 - val_acc: 0.8013\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2357 - acc: 0.9096 - val_loss: 0.6602 - val_acc: 0.7949\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2157 - acc: 0.9192 - val_loss: 0.5440 - val_acc: 0.7885\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2274 - acc: 0.9169 - val_loss: 0.7910 - val_acc: 0.7115\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2140 - acc: 0.9284 - val_loss: 0.8829 - val_acc: 0.7051\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.1964 - acc: 0.9371 - val_loss: 0.6816 - val_acc: 0.7372\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2367 - acc: 0.9165 - val_loss: 0.5633 - val_acc: 0.8077\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2315 - acc: 0.9156 - val_loss: 0.4441 - val_acc: 0.8141\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.2256 - acc: 0.9156 - val_loss: 0.4720 - val_acc: 0.8397\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2018 - acc: 0.9266 - val_loss: 1.2044 - val_acc: 0.6795\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1841 - acc: 0.9298 - val_loss: 1.4351 - val_acc: 0.7179\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1710 - acc: 0.9422 - val_loss: 0.4822 - val_acc: 0.8333\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1562 - acc: 0.9472 - val_loss: 0.7106 - val_acc: 0.7885\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1358 - acc: 0.9605 - val_loss: 0.6273 - val_acc: 0.7756\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1645 - acc: 0.9445 - val_loss: 1.1218 - val_acc: 0.6538\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1439 - acc: 0.9536 - val_loss: 0.5630 - val_acc: 0.8077\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1694 - acc: 0.9417 - val_loss: 0.5684 - val_acc: 0.8526\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1485 - acc: 0.9477 - val_loss: 0.4907 - val_acc: 0.8397\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1133 - acc: 0.9651 - val_loss: 0.7101 - val_acc: 0.7692\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1242 - acc: 0.9555 - val_loss: 0.4768 - val_acc: 0.8718\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1028 - acc: 0.9702 - val_loss: 0.4870 - val_acc: 0.8333\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.0936 - acc: 0.9711 - val_loss: 0.9463 - val_acc: 0.7436\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1000 - acc: 0.9665 - val_loss: 0.4722 - val_acc: 0.8782\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1166 - acc: 0.9642 - val_loss: 0.5216 - val_acc: 0.8462\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1429 - acc: 0.9468 - val_loss: 0.6273 - val_acc: 0.7756\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1032 - acc: 0.9715 - val_loss: 0.6832 - val_acc: 0.8397\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0897 - acc: 0.9743 - val_loss: 0.4712 - val_acc: 0.8718\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1284 - acc: 0.9541 - val_loss: 0.5827 - val_acc: 0.8269\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.1092 - acc: 0.9642 - val_loss: 0.6928 - val_acc: 0.7756\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.1114 - acc: 0.9642 - val_loss: 0.6454 - val_acc: 0.8077\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0759 - acc: 0.9798 - val_loss: 0.4915 - val_acc: 0.8590\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0596 - acc: 0.9867 - val_loss: 0.6261 - val_acc: 0.8205\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0947 - acc: 0.9697 - val_loss: 0.5472 - val_acc: 0.8654\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0808 - acc: 0.9757 - val_loss: 0.6508 - val_acc: 0.8013\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1046 - acc: 0.9693 - val_loss: 0.6531 - val_acc: 0.8397\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0897 - acc: 0.9706 - val_loss: 0.5261 - val_acc: 0.8269\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0630 - acc: 0.9816 - val_loss: 0.5547 - val_acc: 0.8462\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0432 - acc: 0.9913 - val_loss: 0.5161 - val_acc: 0.8590\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0656 - acc: 0.9816 - val_loss: 0.5958 - val_acc: 0.8269\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0898 - acc: 0.9734 - val_loss: 0.5913 - val_acc: 0.8269\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0772 - acc: 0.9761 - val_loss: 0.5777 - val_acc: 0.8077\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0533 - acc: 0.9890 - val_loss: 0.7309 - val_acc: 0.7885\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0523 - acc: 0.9872 - val_loss: 0.7009 - val_acc: 0.8205\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0666 - acc: 0.9826 - val_loss: 0.5992 - val_acc: 0.8397\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0922 - acc: 0.9706 - val_loss: 0.9035 - val_acc: 0.8013\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.1275 - acc: 0.9550 - val_loss: 0.7556 - val_acc: 0.7692\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0576 - acc: 0.9835 - val_loss: 0.7785 - val_acc: 0.7500\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0670 - acc: 0.9793 - val_loss: 0.8223 - val_acc: 0.7308\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0914 - acc: 0.9720 - val_loss: 0.7847 - val_acc: 0.7372\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0852 - acc: 0.9743 - val_loss: 0.7174 - val_acc: 0.8141\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0845 - acc: 0.9729 - val_loss: 0.7071 - val_acc: 0.7756\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0666 - acc: 0.9844 - val_loss: 0.5763 - val_acc: 0.8333\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0732 - acc: 0.9775 - val_loss: 0.5340 - val_acc: 0.8333\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0461 - acc: 0.9876 - val_loss: 0.6246 - val_acc: 0.8333\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0572 - acc: 0.9835 - val_loss: 0.6770 - val_acc: 0.8141\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0464 - acc: 0.9899 - val_loss: 0.6901 - val_acc: 0.8141\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0285 - acc: 0.9954 - val_loss: 0.8234 - val_acc: 0.8013\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0291 - acc: 0.9940 - val_loss: 0.6384 - val_acc: 0.8141\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0794 - acc: 0.9734 - val_loss: 1.1031 - val_acc: 0.7051\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0923 - acc: 0.9683 - val_loss: 1.3569 - val_acc: 0.7051\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0983 - acc: 0.9670 - val_loss: 0.7495 - val_acc: 0.8077\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0805 - acc: 0.9738 - val_loss: 0.6655 - val_acc: 0.8269\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0330 - acc: 0.9945 - val_loss: 0.7404 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0333 - acc: 0.9927 - val_loss: 0.6315 - val_acc: 0.8397\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0203 - acc: 0.9995 - val_loss: 0.6406 - val_acc: 0.8526\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0254 - acc: 0.9968 - val_loss: 0.6348 - val_acc: 0.8333\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0218 - acc: 0.9982 - val_loss: 0.6568 - val_acc: 0.8333\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0221 - acc: 0.9977 - val_loss: 0.6350 - val_acc: 0.8654\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0287 - acc: 0.9959 - val_loss: 0.6567 - val_acc: 0.8397\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0201 - acc: 0.9977 - val_loss: 0.6692 - val_acc: 0.8333\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0190 - acc: 0.9991 - val_loss: 0.6782 - val_acc: 0.8269\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0196 - acc: 0.9982 - val_loss: 0.6653 - val_acc: 0.8269\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0205 - acc: 0.9982 - val_loss: 0.6711 - val_acc: 0.8397\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9350 - acc: 0.6251 - val_loss: 1.4276 - val_acc: 0.2564\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5901 - acc: 0.7788 - val_loss: 1.7970 - val_acc: 0.3013\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4984 - acc: 0.8151 - val_loss: 3.0130 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4466 - acc: 0.8421 - val_loss: 4.3505 - val_acc: 0.3013\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4276 - acc: 0.8362 - val_loss: 5.8365 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.3908 - acc: 0.8573 - val_loss: 5.2756 - val_acc: 0.3077\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3779 - acc: 0.8655 - val_loss: 3.0773 - val_acc: 0.4167\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3672 - acc: 0.8683 - val_loss: 1.7488 - val_acc: 0.5000\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3581 - acc: 0.8669 - val_loss: 1.3644 - val_acc: 0.6346\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3616 - acc: 0.8697 - val_loss: 1.9374 - val_acc: 0.6218\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3435 - acc: 0.8784 - val_loss: 5.6500 - val_acc: 0.3205\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3392 - acc: 0.8752 - val_loss: 1.4145 - val_acc: 0.6474\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3016 - acc: 0.8949 - val_loss: 2.2936 - val_acc: 0.5769\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3230 - acc: 0.8793 - val_loss: 2.2348 - val_acc: 0.6346\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3149 - acc: 0.8853 - val_loss: 1.0818 - val_acc: 0.7115\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2739 - acc: 0.9013 - val_loss: 0.5532 - val_acc: 0.8077\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2736 - acc: 0.9055 - val_loss: 0.9784 - val_acc: 0.6346\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2930 - acc: 0.8926 - val_loss: 1.0911 - val_acc: 0.6859\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2739 - acc: 0.9013 - val_loss: 0.6521 - val_acc: 0.7436\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2677 - acc: 0.9105 - val_loss: 0.8192 - val_acc: 0.7308\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2550 - acc: 0.9064 - val_loss: 0.7681 - val_acc: 0.7436\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2546 - acc: 0.9013 - val_loss: 0.9401 - val_acc: 0.7115\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2803 - acc: 0.8922 - val_loss: 0.7216 - val_acc: 0.7436\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2654 - acc: 0.9059 - val_loss: 0.9363 - val_acc: 0.6090\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2451 - acc: 0.9110 - val_loss: 0.4245 - val_acc: 0.8397\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2259 - acc: 0.9220 - val_loss: 0.5747 - val_acc: 0.7821\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2414 - acc: 0.9142 - val_loss: 0.4310 - val_acc: 0.8333\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2247 - acc: 0.9197 - val_loss: 0.6916 - val_acc: 0.7051\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2121 - acc: 0.9293 - val_loss: 0.5437 - val_acc: 0.7949\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.2258 - acc: 0.9201 - val_loss: 0.4708 - val_acc: 0.8269\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2180 - acc: 0.9224 - val_loss: 0.7552 - val_acc: 0.7756\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1994 - acc: 0.9257 - val_loss: 0.4312 - val_acc: 0.8397\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1772 - acc: 0.9413 - val_loss: 0.7130 - val_acc: 0.7244\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1944 - acc: 0.9385 - val_loss: 0.4516 - val_acc: 0.8013\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1974 - acc: 0.9261 - val_loss: 1.4967 - val_acc: 0.6474\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1918 - acc: 0.9307 - val_loss: 0.5760 - val_acc: 0.7756\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1649 - acc: 0.9454 - val_loss: 0.5333 - val_acc: 0.7692\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1501 - acc: 0.9477 - val_loss: 0.4753 - val_acc: 0.8205\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1562 - acc: 0.9463 - val_loss: 0.5697 - val_acc: 0.7949\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1443 - acc: 0.9509 - val_loss: 0.5722 - val_acc: 0.7949\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1235 - acc: 0.9656 - val_loss: 0.5231 - val_acc: 0.8077\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1312 - acc: 0.9569 - val_loss: 0.6719 - val_acc: 0.7692\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1489 - acc: 0.9458 - val_loss: 0.5077 - val_acc: 0.8077\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1277 - acc: 0.9596 - val_loss: 0.5206 - val_acc: 0.8077\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1853 - acc: 0.9348 - val_loss: 0.5143 - val_acc: 0.8013\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1585 - acc: 0.9445 - val_loss: 0.5012 - val_acc: 0.8269\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1225 - acc: 0.9619 - val_loss: 0.4905 - val_acc: 0.7949\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1087 - acc: 0.9601 - val_loss: 0.6152 - val_acc: 0.7628\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0921 - acc: 0.9693 - val_loss: 0.5517 - val_acc: 0.7949\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0918 - acc: 0.9715 - val_loss: 1.2677 - val_acc: 0.6603\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.1623 - acc: 0.9367 - val_loss: 0.6195 - val_acc: 0.7628\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1064 - acc: 0.9651 - val_loss: 0.4905 - val_acc: 0.7949\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.1031 - acc: 0.9651 - val_loss: 0.7603 - val_acc: 0.7564\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.1388 - acc: 0.9536 - val_loss: 0.7160 - val_acc: 0.7692\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0938 - acc: 0.9720 - val_loss: 0.3981 - val_acc: 0.8654\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1138 - acc: 0.9633 - val_loss: 0.5572 - val_acc: 0.8077\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0812 - acc: 0.9784 - val_loss: 0.5814 - val_acc: 0.7692\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.1059 - acc: 0.9651 - val_loss: 0.4518 - val_acc: 0.8077\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.1156 - acc: 0.9637 - val_loss: 0.5329 - val_acc: 0.8013\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0731 - acc: 0.9821 - val_loss: 0.5012 - val_acc: 0.8269\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0519 - acc: 0.9899 - val_loss: 0.5072 - val_acc: 0.8269\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0573 - acc: 0.9872 - val_loss: 0.4455 - val_acc: 0.8141\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0525 - acc: 0.9867 - val_loss: 0.4381 - val_acc: 0.8205\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0637 - acc: 0.9807 - val_loss: 0.6447 - val_acc: 0.8077\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0450 - acc: 0.9917 - val_loss: 0.5561 - val_acc: 0.8141\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0687 - acc: 0.9803 - val_loss: 0.6183 - val_acc: 0.7949\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0557 - acc: 0.9872 - val_loss: 0.5447 - val_acc: 0.7692\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0474 - acc: 0.9881 - val_loss: 0.4114 - val_acc: 0.8526\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0747 - acc: 0.9780 - val_loss: 0.5126 - val_acc: 0.8333\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0931 - acc: 0.9693 - val_loss: 0.5716 - val_acc: 0.7821\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0698 - acc: 0.9803 - val_loss: 0.5153 - val_acc: 0.8013\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0672 - acc: 0.9803 - val_loss: 0.5135 - val_acc: 0.8077\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0859 - acc: 0.9711 - val_loss: 0.5727 - val_acc: 0.7628\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0496 - acc: 0.9839 - val_loss: 0.4844 - val_acc: 0.8077\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0461 - acc: 0.9913 - val_loss: 2.0907 - val_acc: 0.6538\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.1202 - acc: 0.9582 - val_loss: 1.5637 - val_acc: 0.6346\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0683 - acc: 0.9807 - val_loss: 0.5755 - val_acc: 0.7949\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0364 - acc: 0.9931 - val_loss: 0.4750 - val_acc: 0.7949\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0486 - acc: 0.9858 - val_loss: 0.7034 - val_acc: 0.7628\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0302 - acc: 0.9940 - val_loss: 0.4676 - val_acc: 0.8397\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0321 - acc: 0.9931 - val_loss: 1.0665 - val_acc: 0.6859\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0744 - acc: 0.9738 - val_loss: 0.5046 - val_acc: 0.8269\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0486 - acc: 0.9881 - val_loss: 0.5550 - val_acc: 0.8013\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0729 - acc: 0.9766 - val_loss: 0.6196 - val_acc: 0.7885\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0794 - acc: 0.9738 - val_loss: 0.5094 - val_acc: 0.8269\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.1001 - acc: 0.9660 - val_loss: 0.6768 - val_acc: 0.8013\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0611 - acc: 0.9862 - val_loss: 0.5210 - val_acc: 0.8333\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0960 - acc: 0.9683 - val_loss: 0.6554 - val_acc: 0.7821\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.1095 - acc: 0.9651 - val_loss: 0.7779 - val_acc: 0.7692\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0720 - acc: 0.9775 - val_loss: 0.5145 - val_acc: 0.8077\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0395 - acc: 0.9931 - val_loss: 0.4776 - val_acc: 0.8269\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0301 - acc: 0.9959 - val_loss: 0.4997 - val_acc: 0.8333\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0243 - acc: 0.9968 - val_loss: 0.4741 - val_acc: 0.8462\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0558 - acc: 0.9812 - val_loss: 0.5096 - val_acc: 0.8333\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0324 - acc: 0.9931 - val_loss: 0.5039 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0230 - acc: 0.9972 - val_loss: 0.4858 - val_acc: 0.8333\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0201 - acc: 0.9991 - val_loss: 0.4898 - val_acc: 0.8397\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0252 - acc: 0.9968 - val_loss: 0.4756 - val_acc: 0.8397\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0214 - acc: 0.9968 - val_loss: 0.4715 - val_acc: 0.8397\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0167 - acc: 0.9995 - val_loss: 0.4733 - val_acc: 0.8333\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0199 - acc: 0.9968 - val_loss: 0.4793 - val_acc: 0.8333\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0176 - acc: 0.9995 - val_loss: 0.4737 - val_acc: 0.8333\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0147 - acc: 0.9991 - val_loss: 0.4751 - val_acc: 0.8397\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0160 - acc: 0.9991 - val_loss: 0.4862 - val_acc: 0.8397\n",
      "Epoch 105/1000\n",
      "69/69 - 0s - loss: 0.0172 - acc: 0.9972 - val_loss: 0.4925 - val_acc: 0.8397\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9543 - acc: 0.6209 - val_loss: 1.4148 - val_acc: 0.2436\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5916 - acc: 0.7962 - val_loss: 1.8628 - val_acc: 0.2436\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5003 - acc: 0.8183 - val_loss: 3.2131 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4674 - acc: 0.8343 - val_loss: 5.2273 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4266 - acc: 0.8467 - val_loss: 6.0082 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4328 - acc: 0.8486 - val_loss: 6.8199 - val_acc: 0.2436\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3884 - acc: 0.8531 - val_loss: 5.3745 - val_acc: 0.2436\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3839 - acc: 0.8614 - val_loss: 2.9592 - val_acc: 0.3013\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3492 - acc: 0.8715 - val_loss: 2.2562 - val_acc: 0.4615\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3539 - acc: 0.8683 - val_loss: 0.9147 - val_acc: 0.6923\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3309 - acc: 0.8793 - val_loss: 0.6310 - val_acc: 0.7564\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3277 - acc: 0.8775 - val_loss: 0.4721 - val_acc: 0.8397\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3275 - acc: 0.8848 - val_loss: 0.4513 - val_acc: 0.8333\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3094 - acc: 0.8931 - val_loss: 0.4893 - val_acc: 0.7821\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2863 - acc: 0.8967 - val_loss: 1.1188 - val_acc: 0.6090\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.3163 - acc: 0.8821 - val_loss: 0.5836 - val_acc: 0.7949\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2913 - acc: 0.8917 - val_loss: 0.4384 - val_acc: 0.8205\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2722 - acc: 0.8977 - val_loss: 0.3815 - val_acc: 0.8397\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2865 - acc: 0.8990 - val_loss: 0.4359 - val_acc: 0.8462\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2822 - acc: 0.8967 - val_loss: 1.3097 - val_acc: 0.6603\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2630 - acc: 0.8986 - val_loss: 0.9340 - val_acc: 0.6859\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2350 - acc: 0.9151 - val_loss: 0.5890 - val_acc: 0.7564\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2447 - acc: 0.9105 - val_loss: 0.3832 - val_acc: 0.8846\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2114 - acc: 0.9275 - val_loss: 0.3999 - val_acc: 0.8590\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2105 - acc: 0.9325 - val_loss: 0.4117 - val_acc: 0.8462\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2573 - acc: 0.9101 - val_loss: 0.6726 - val_acc: 0.7500\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2007 - acc: 0.9321 - val_loss: 1.1084 - val_acc: 0.6923\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.1971 - acc: 0.9261 - val_loss: 0.8068 - val_acc: 0.7179\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1802 - acc: 0.9408 - val_loss: 0.4227 - val_acc: 0.8526\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1898 - acc: 0.9330 - val_loss: 0.5370 - val_acc: 0.7821\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2315 - acc: 0.9156 - val_loss: 0.4237 - val_acc: 0.8462\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1926 - acc: 0.9266 - val_loss: 2.6236 - val_acc: 0.6410\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1674 - acc: 0.9436 - val_loss: 0.5918 - val_acc: 0.8269\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1389 - acc: 0.9536 - val_loss: 0.5093 - val_acc: 0.8013\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1487 - acc: 0.9486 - val_loss: 0.5956 - val_acc: 0.7885\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1342 - acc: 0.9550 - val_loss: 1.5640 - val_acc: 0.5769\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1219 - acc: 0.9619 - val_loss: 0.4983 - val_acc: 0.8269\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1452 - acc: 0.9491 - val_loss: 0.5407 - val_acc: 0.8269\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1742 - acc: 0.9390 - val_loss: 0.6556 - val_acc: 0.8077\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1573 - acc: 0.9458 - val_loss: 0.6502 - val_acc: 0.8077\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1437 - acc: 0.9500 - val_loss: 0.7024 - val_acc: 0.7885\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1466 - acc: 0.9495 - val_loss: 0.4413 - val_acc: 0.8590\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1092 - acc: 0.9679 - val_loss: 0.4906 - val_acc: 0.8269\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.0972 - acc: 0.9697 - val_loss: 0.5537 - val_acc: 0.8013\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.0890 - acc: 0.9725 - val_loss: 0.8318 - val_acc: 0.7179\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.0858 - acc: 0.9752 - val_loss: 0.4292 - val_acc: 0.8590\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1275 - acc: 0.9573 - val_loss: 0.5359 - val_acc: 0.8269\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0966 - acc: 0.9720 - val_loss: 0.4982 - val_acc: 0.8526\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1086 - acc: 0.9642 - val_loss: 0.7965 - val_acc: 0.7436\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.1316 - acc: 0.9550 - val_loss: 1.5332 - val_acc: 0.5769\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0918 - acc: 0.9706 - val_loss: 0.5607 - val_acc: 0.8333\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0752 - acc: 0.9771 - val_loss: 0.7033 - val_acc: 0.8269\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0812 - acc: 0.9761 - val_loss: 0.9349 - val_acc: 0.7692\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0786 - acc: 0.9780 - val_loss: 0.5147 - val_acc: 0.8269\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.1289 - acc: 0.9546 - val_loss: 0.6931 - val_acc: 0.8141\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0895 - acc: 0.9697 - val_loss: 0.4712 - val_acc: 0.8526\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0902 - acc: 0.9711 - val_loss: 0.6772 - val_acc: 0.8269\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0580 - acc: 0.9872 - val_loss: 0.5599 - val_acc: 0.8269\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0541 - acc: 0.9849 - val_loss: 0.5585 - val_acc: 0.8397\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0698 - acc: 0.9835 - val_loss: 0.7131 - val_acc: 0.8205\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.1019 - acc: 0.9656 - val_loss: 0.5952 - val_acc: 0.8718\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0656 - acc: 0.9812 - val_loss: 0.9535 - val_acc: 0.7885\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0519 - acc: 0.9894 - val_loss: 0.6642 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0318 - acc: 0.9954 - val_loss: 0.6221 - val_acc: 0.8462\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0310 - acc: 0.9945 - val_loss: 0.6033 - val_acc: 0.8462\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0321 - acc: 0.9950 - val_loss: 0.6058 - val_acc: 0.8462\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0280 - acc: 0.9972 - val_loss: 0.6004 - val_acc: 0.8462\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0271 - acc: 0.9959 - val_loss: 0.6068 - val_acc: 0.8526\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0299 - acc: 0.9950 - val_loss: 0.5837 - val_acc: 0.8462\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0248 - acc: 0.9968 - val_loss: 0.6072 - val_acc: 0.8526\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0216 - acc: 0.9986 - val_loss: 0.6237 - val_acc: 0.8526\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0281 - acc: 0.9982 - val_loss: 0.6148 - val_acc: 0.8397\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0264 - acc: 0.9968 - val_loss: 0.6001 - val_acc: 0.8526\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9714 - acc: 0.6108 - val_loss: 1.4101 - val_acc: 0.2564\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6258 - acc: 0.7783 - val_loss: 1.4927 - val_acc: 0.2564\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5395 - acc: 0.7972 - val_loss: 1.4942 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.5156 - acc: 0.8054 - val_loss: 2.2576 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4760 - acc: 0.8270 - val_loss: 2.1607 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4543 - acc: 0.8348 - val_loss: 4.1613 - val_acc: 0.2436\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.4367 - acc: 0.8366 - val_loss: 4.1337 - val_acc: 0.2436\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.4500 - acc: 0.8320 - val_loss: 2.7352 - val_acc: 0.2949\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.4210 - acc: 0.8536 - val_loss: 1.5594 - val_acc: 0.4936\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3782 - acc: 0.8665 - val_loss: 0.6287 - val_acc: 0.7628\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3828 - acc: 0.8550 - val_loss: 0.8757 - val_acc: 0.6859\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3565 - acc: 0.8637 - val_loss: 1.3340 - val_acc: 0.4936\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3612 - acc: 0.8678 - val_loss: 1.2001 - val_acc: 0.5385\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3402 - acc: 0.8710 - val_loss: 1.0557 - val_acc: 0.5897\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3924 - acc: 0.8531 - val_loss: 0.7907 - val_acc: 0.6538\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.3299 - acc: 0.8816 - val_loss: 0.6096 - val_acc: 0.7756\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.3017 - acc: 0.8848 - val_loss: 0.5340 - val_acc: 0.8013\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.3081 - acc: 0.8903 - val_loss: 0.8211 - val_acc: 0.7308\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2972 - acc: 0.8903 - val_loss: 0.6360 - val_acc: 0.7564\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.3098 - acc: 0.8880 - val_loss: 0.6580 - val_acc: 0.7436\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.3030 - acc: 0.8899 - val_loss: 0.6428 - val_acc: 0.7628\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2767 - acc: 0.9009 - val_loss: 0.6426 - val_acc: 0.7885\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2612 - acc: 0.8990 - val_loss: 0.9222 - val_acc: 0.6538\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2600 - acc: 0.9055 - val_loss: 0.4980 - val_acc: 0.8333\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2327 - acc: 0.9146 - val_loss: 0.6438 - val_acc: 0.7308\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2514 - acc: 0.9119 - val_loss: 0.7280 - val_acc: 0.7756\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2119 - acc: 0.9206 - val_loss: 0.5703 - val_acc: 0.8205\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2311 - acc: 0.9128 - val_loss: 0.5766 - val_acc: 0.8141\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2129 - acc: 0.9220 - val_loss: 0.6825 - val_acc: 0.7372\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1929 - acc: 0.9289 - val_loss: 0.7577 - val_acc: 0.7564\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2141 - acc: 0.9261 - val_loss: 0.7471 - val_acc: 0.7821\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1857 - acc: 0.9335 - val_loss: 0.6388 - val_acc: 0.7821\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.2024 - acc: 0.9238 - val_loss: 0.6018 - val_acc: 0.7949\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.2024 - acc: 0.9243 - val_loss: 0.5965 - val_acc: 0.7756\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1818 - acc: 0.9330 - val_loss: 0.6446 - val_acc: 0.7628\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.2163 - acc: 0.9146 - val_loss: 0.6890 - val_acc: 0.7564\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1563 - acc: 0.9458 - val_loss: 0.6089 - val_acc: 0.7821\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1736 - acc: 0.9440 - val_loss: 0.4676 - val_acc: 0.8269\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1363 - acc: 0.9546 - val_loss: 0.5200 - val_acc: 0.8205\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1331 - acc: 0.9569 - val_loss: 0.5139 - val_acc: 0.7821\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1283 - acc: 0.9615 - val_loss: 0.5406 - val_acc: 0.7885\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1212 - acc: 0.9628 - val_loss: 0.5560 - val_acc: 0.7949\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1088 - acc: 0.9683 - val_loss: 0.6063 - val_acc: 0.8269\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1115 - acc: 0.9624 - val_loss: 0.6046 - val_acc: 0.7756\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1039 - acc: 0.9738 - val_loss: 1.2567 - val_acc: 0.6218\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1547 - acc: 0.9491 - val_loss: 0.7109 - val_acc: 0.7628\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1340 - acc: 0.9582 - val_loss: 1.1670 - val_acc: 0.6346\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1052 - acc: 0.9665 - val_loss: 0.6320 - val_acc: 0.7628\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0878 - acc: 0.9766 - val_loss: 0.5526 - val_acc: 0.8205\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0815 - acc: 0.9757 - val_loss: 0.6221 - val_acc: 0.7821\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.1649 - acc: 0.9399 - val_loss: 2.2038 - val_acc: 0.5064\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1565 - acc: 0.9454 - val_loss: 0.6420 - val_acc: 0.7949\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0900 - acc: 0.9697 - val_loss: 0.9747 - val_acc: 0.7692\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0779 - acc: 0.9812 - val_loss: 0.5707 - val_acc: 0.8205\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.1084 - acc: 0.9637 - val_loss: 0.6222 - val_acc: 0.8205\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1121 - acc: 0.9615 - val_loss: 0.5343 - val_acc: 0.8397\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0817 - acc: 0.9734 - val_loss: 0.8797 - val_acc: 0.7372\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0677 - acc: 0.9821 - val_loss: 1.2678 - val_acc: 0.6410\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.1194 - acc: 0.9601 - val_loss: 1.2563 - val_acc: 0.6795\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0794 - acc: 0.9757 - val_loss: 0.5187 - val_acc: 0.8141\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0918 - acc: 0.9720 - val_loss: 0.7483 - val_acc: 0.8077\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0918 - acc: 0.9720 - val_loss: 0.6612 - val_acc: 0.8013\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0676 - acc: 0.9793 - val_loss: 0.6770 - val_acc: 0.7500\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.1070 - acc: 0.9615 - val_loss: 1.0348 - val_acc: 0.6859\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0829 - acc: 0.9757 - val_loss: 0.5696 - val_acc: 0.8397\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0874 - acc: 0.9688 - val_loss: 0.7054 - val_acc: 0.7821\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0865 - acc: 0.9734 - val_loss: 0.5446 - val_acc: 0.8141\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0579 - acc: 0.9835 - val_loss: 0.6362 - val_acc: 0.8141\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0960 - acc: 0.9665 - val_loss: 0.5704 - val_acc: 0.8205\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0483 - acc: 0.9881 - val_loss: 0.5326 - val_acc: 0.8462\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0697 - acc: 0.9798 - val_loss: 0.6167 - val_acc: 0.8269\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0611 - acc: 0.9844 - val_loss: 0.5607 - val_acc: 0.8462\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0459 - acc: 0.9904 - val_loss: 0.5760 - val_acc: 0.8141\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0396 - acc: 0.9913 - val_loss: 0.5804 - val_acc: 0.7692\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0326 - acc: 0.9940 - val_loss: 0.5344 - val_acc: 0.8141\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0311 - acc: 0.9954 - val_loss: 0.5261 - val_acc: 0.8462\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0288 - acc: 0.9959 - val_loss: 0.5628 - val_acc: 0.8462\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0190 - acc: 0.9982 - val_loss: 0.5417 - val_acc: 0.8333\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0320 - acc: 0.9931 - val_loss: 0.7213 - val_acc: 0.7756\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0433 - acc: 0.9858 - val_loss: 0.6871 - val_acc: 0.7949\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0362 - acc: 0.9936 - val_loss: 0.6636 - val_acc: 0.8013\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0693 - acc: 0.9812 - val_loss: 1.1854 - val_acc: 0.7308\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0396 - acc: 0.9890 - val_loss: 0.7851 - val_acc: 0.7628\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0272 - acc: 0.9963 - val_loss: 0.7101 - val_acc: 0.7949\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0234 - acc: 0.9959 - val_loss: 0.6309 - val_acc: 0.8205\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0338 - acc: 0.9936 - val_loss: 0.5949 - val_acc: 0.8205\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.1297 - acc: 0.9559 - val_loss: 1.3226 - val_acc: 0.5897\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0669 - acc: 0.9784 - val_loss: 0.5886 - val_acc: 0.8141\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.1732 - acc: 0.9348 - val_loss: 0.7901 - val_acc: 0.7564\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.1006 - acc: 0.9656 - val_loss: 1.0088 - val_acc: 0.7821\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0790 - acc: 0.9752 - val_loss: 0.6531 - val_acc: 0.8269\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0497 - acc: 0.9872 - val_loss: 0.9879 - val_acc: 0.7244\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0833 - acc: 0.9734 - val_loss: 0.6371 - val_acc: 0.7756\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0656 - acc: 0.9816 - val_loss: 0.7544 - val_acc: 0.7308\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0727 - acc: 0.9775 - val_loss: 0.9301 - val_acc: 0.8013\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0548 - acc: 0.9885 - val_loss: 0.5310 - val_acc: 0.8205\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0770 - acc: 0.9725 - val_loss: 0.6038 - val_acc: 0.8141\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0585 - acc: 0.9807 - val_loss: 0.6582 - val_acc: 0.8077\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0330 - acc: 0.9940 - val_loss: 0.6468 - val_acc: 0.8269\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0287 - acc: 0.9959 - val_loss: 0.6443 - val_acc: 0.8077\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0596 - acc: 0.9803 - val_loss: 0.7447 - val_acc: 0.7500\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0627 - acc: 0.9816 - val_loss: 0.4930 - val_acc: 0.8333\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0303 - acc: 0.9945 - val_loss: 0.5452 - val_acc: 0.8205\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0273 - acc: 0.9954 - val_loss: 0.5134 - val_acc: 0.8333\n",
      "Epoch 105/1000\n",
      "69/69 - 0s - loss: 0.0630 - acc: 0.9793 - val_loss: 0.6240 - val_acc: 0.8141\n",
      "Epoch 106/1000\n",
      "69/69 - 0s - loss: 0.0811 - acc: 0.9752 - val_loss: 0.7594 - val_acc: 0.8077\n",
      "Epoch 107/1000\n",
      "69/69 - 0s - loss: 0.0994 - acc: 0.9660 - val_loss: 0.5909 - val_acc: 0.8397\n",
      "Epoch 108/1000\n",
      "69/69 - 0s - loss: 0.0585 - acc: 0.9803 - val_loss: 0.6763 - val_acc: 0.7885\n",
      "Epoch 109/1000\n",
      "69/69 - 0s - loss: 0.0268 - acc: 0.9954 - val_loss: 0.5936 - val_acc: 0.8333\n",
      "Epoch 110/1000\n",
      "69/69 - 0s - loss: 0.0344 - acc: 0.9890 - val_loss: 0.7139 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 111/1000\n",
      "69/69 - 0s - loss: 0.0201 - acc: 0.9977 - val_loss: 0.6604 - val_acc: 0.8269\n",
      "Epoch 112/1000\n",
      "69/69 - 0s - loss: 0.0184 - acc: 0.9968 - val_loss: 0.6258 - val_acc: 0.8205\n",
      "Epoch 113/1000\n",
      "69/69 - 0s - loss: 0.0182 - acc: 0.9977 - val_loss: 0.6269 - val_acc: 0.8141\n",
      "Epoch 114/1000\n",
      "69/69 - 0s - loss: 0.0169 - acc: 0.9991 - val_loss: 0.6277 - val_acc: 0.8141\n",
      "Epoch 115/1000\n",
      "69/69 - 0s - loss: 0.0170 - acc: 0.9982 - val_loss: 0.6070 - val_acc: 0.8205\n",
      "Epoch 116/1000\n",
      "69/69 - 0s - loss: 0.0184 - acc: 0.9972 - val_loss: 0.5972 - val_acc: 0.8205\n",
      "Epoch 117/1000\n",
      "69/69 - 0s - loss: 0.0148 - acc: 0.9991 - val_loss: 0.6099 - val_acc: 0.8269\n",
      "Epoch 118/1000\n",
      "69/69 - 0s - loss: 0.0144 - acc: 1.0000 - val_loss: 0.6200 - val_acc: 0.8205\n",
      "Epoch 119/1000\n",
      "69/69 - 0s - loss: 0.0181 - acc: 0.9977 - val_loss: 0.6064 - val_acc: 0.8269\n",
      "Epoch 120/1000\n",
      "69/69 - 0s - loss: 0.0193 - acc: 0.9982 - val_loss: 0.5796 - val_acc: 0.8205\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9615 - acc: 0.6205 - val_loss: 1.3939 - val_acc: 0.2564\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5653 - acc: 0.7976 - val_loss: 1.5308 - val_acc: 0.2564\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4848 - acc: 0.8265 - val_loss: 1.7259 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4528 - acc: 0.8284 - val_loss: 1.6976 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4085 - acc: 0.8495 - val_loss: 3.7108 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.3822 - acc: 0.8632 - val_loss: 2.7634 - val_acc: 0.2500\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3774 - acc: 0.8646 - val_loss: 1.6925 - val_acc: 0.3846\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3366 - acc: 0.8857 - val_loss: 2.4113 - val_acc: 0.3590\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3361 - acc: 0.8720 - val_loss: 0.7117 - val_acc: 0.7244\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3176 - acc: 0.8825 - val_loss: 0.5244 - val_acc: 0.8013\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3279 - acc: 0.8743 - val_loss: 0.7176 - val_acc: 0.7372\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3057 - acc: 0.8857 - val_loss: 0.3345 - val_acc: 0.8590\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.2999 - acc: 0.8908 - val_loss: 1.3295 - val_acc: 0.5769\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3020 - acc: 0.8862 - val_loss: 0.3491 - val_acc: 0.8526\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2733 - acc: 0.9013 - val_loss: 0.3900 - val_acc: 0.8654\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2744 - acc: 0.8995 - val_loss: 0.4216 - val_acc: 0.8333\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2869 - acc: 0.8944 - val_loss: 0.8205 - val_acc: 0.7115\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2729 - acc: 0.9013 - val_loss: 0.4412 - val_acc: 0.8397\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2559 - acc: 0.9110 - val_loss: 0.4176 - val_acc: 0.8397\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2486 - acc: 0.9169 - val_loss: 0.4778 - val_acc: 0.8397\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2425 - acc: 0.9087 - val_loss: 0.4770 - val_acc: 0.8013\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2392 - acc: 0.9142 - val_loss: 0.4923 - val_acc: 0.8205\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2286 - acc: 0.9133 - val_loss: 0.4557 - val_acc: 0.8590\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2352 - acc: 0.9160 - val_loss: 0.3314 - val_acc: 0.8526\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2090 - acc: 0.9275 - val_loss: 0.4152 - val_acc: 0.8526\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2676 - acc: 0.9004 - val_loss: 0.4954 - val_acc: 0.7949\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2273 - acc: 0.9119 - val_loss: 0.3377 - val_acc: 0.8590\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2097 - acc: 0.9252 - val_loss: 0.3935 - val_acc: 0.8590\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2047 - acc: 0.9270 - val_loss: 0.3708 - val_acc: 0.8590\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.2013 - acc: 0.9325 - val_loss: 0.4023 - val_acc: 0.8333\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1915 - acc: 0.9367 - val_loss: 0.6799 - val_acc: 0.7308\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1643 - acc: 0.9454 - val_loss: 0.3729 - val_acc: 0.8526\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1525 - acc: 0.9445 - val_loss: 0.3803 - val_acc: 0.8654\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.2225 - acc: 0.9179 - val_loss: 0.6487 - val_acc: 0.7692\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1595 - acc: 0.9504 - val_loss: 0.4078 - val_acc: 0.8526\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1736 - acc: 0.9380 - val_loss: 0.3718 - val_acc: 0.8718\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1442 - acc: 0.9518 - val_loss: 0.4748 - val_acc: 0.8333\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1445 - acc: 0.9541 - val_loss: 0.6272 - val_acc: 0.7756\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1397 - acc: 0.9527 - val_loss: 0.4418 - val_acc: 0.8526\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1414 - acc: 0.9514 - val_loss: 0.5921 - val_acc: 0.7821\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1423 - acc: 0.9486 - val_loss: 0.3774 - val_acc: 0.8654\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1498 - acc: 0.9514 - val_loss: 0.4186 - val_acc: 0.8590\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1223 - acc: 0.9596 - val_loss: 0.3175 - val_acc: 0.8654\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1192 - acc: 0.9601 - val_loss: 0.4112 - val_acc: 0.8333\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1128 - acc: 0.9674 - val_loss: 0.5704 - val_acc: 0.7885\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1012 - acc: 0.9706 - val_loss: 0.5557 - val_acc: 0.8269\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.0906 - acc: 0.9702 - val_loss: 0.4443 - val_acc: 0.8526\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0832 - acc: 0.9729 - val_loss: 0.4406 - val_acc: 0.8462\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0967 - acc: 0.9670 - val_loss: 0.4181 - val_acc: 0.8846\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.1058 - acc: 0.9642 - val_loss: 0.4244 - val_acc: 0.8590\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0848 - acc: 0.9725 - val_loss: 0.6501 - val_acc: 0.7885\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1072 - acc: 0.9633 - val_loss: 0.4386 - val_acc: 0.8526\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.1142 - acc: 0.9637 - val_loss: 0.5283 - val_acc: 0.8269\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0744 - acc: 0.9775 - val_loss: 0.5702 - val_acc: 0.7949\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0616 - acc: 0.9826 - val_loss: 0.6440 - val_acc: 0.7821\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0952 - acc: 0.9715 - val_loss: 0.5664 - val_acc: 0.8462\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.1579 - acc: 0.9413 - val_loss: 0.5037 - val_acc: 0.8205\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.1261 - acc: 0.9564 - val_loss: 0.5450 - val_acc: 0.8462\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0898 - acc: 0.9720 - val_loss: 0.5301 - val_acc: 0.8269\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0895 - acc: 0.9743 - val_loss: 0.8457 - val_acc: 0.7308\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0624 - acc: 0.9830 - val_loss: 0.6325 - val_acc: 0.7885\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0582 - acc: 0.9867 - val_loss: 0.4529 - val_acc: 0.8846\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.1047 - acc: 0.9619 - val_loss: 0.5968 - val_acc: 0.8462\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0930 - acc: 0.9720 - val_loss: 0.7355 - val_acc: 0.7436\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0973 - acc: 0.9683 - val_loss: 0.4906 - val_acc: 0.8654\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0692 - acc: 0.9793 - val_loss: 0.4228 - val_acc: 0.8718\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0897 - acc: 0.9734 - val_loss: 0.5078 - val_acc: 0.8205\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0773 - acc: 0.9761 - val_loss: 0.4011 - val_acc: 0.8526\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0568 - acc: 0.9849 - val_loss: 0.4580 - val_acc: 0.8590\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0635 - acc: 0.9830 - val_loss: 0.6249 - val_acc: 0.8205\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0488 - acc: 0.9890 - val_loss: 0.5254 - val_acc: 0.8782\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0430 - acc: 0.9917 - val_loss: 0.5019 - val_acc: 0.8397\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0442 - acc: 0.9904 - val_loss: 0.4746 - val_acc: 0.8462\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0641 - acc: 0.9803 - val_loss: 0.6389 - val_acc: 0.8590\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0442 - acc: 0.9904 - val_loss: 0.4694 - val_acc: 0.8397\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0752 - acc: 0.9775 - val_loss: 0.5669 - val_acc: 0.8333\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0942 - acc: 0.9674 - val_loss: 0.6513 - val_acc: 0.8269\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0423 - acc: 0.9904 - val_loss: 0.6168 - val_acc: 0.8333\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0882 - acc: 0.9715 - val_loss: 0.4591 - val_acc: 0.8654\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0749 - acc: 0.9793 - val_loss: 0.5803 - val_acc: 0.8269\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0427 - acc: 0.9904 - val_loss: 0.5234 - val_acc: 0.8590\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0350 - acc: 0.9945 - val_loss: 0.5242 - val_acc: 0.8782\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0345 - acc: 0.9904 - val_loss: 0.5202 - val_acc: 0.8526\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0693 - acc: 0.9752 - val_loss: 0.5211 - val_acc: 0.8397\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0694 - acc: 0.9780 - val_loss: 0.4831 - val_acc: 0.8526\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0321 - acc: 0.9931 - val_loss: 0.5335 - val_acc: 0.8718\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0283 - acc: 0.9945 - val_loss: 0.4933 - val_acc: 0.8462\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0898 - acc: 0.9670 - val_loss: 0.5613 - val_acc: 0.8462\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0409 - acc: 0.9913 - val_loss: 0.4178 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0356 - acc: 0.9936 - val_loss: 0.4036 - val_acc: 0.8718\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0284 - acc: 0.9972 - val_loss: 0.4272 - val_acc: 0.8846\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0287 - acc: 0.9940 - val_loss: 0.4469 - val_acc: 0.8782\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0329 - acc: 0.9954 - val_loss: 0.4541 - val_acc: 0.8846\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0239 - acc: 0.9963 - val_loss: 0.4689 - val_acc: 0.8782\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0218 - acc: 0.9982 - val_loss: 0.4460 - val_acc: 0.8846\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0208 - acc: 0.9986 - val_loss: 0.4397 - val_acc: 0.8846\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0228 - acc: 0.9986 - val_loss: 0.4217 - val_acc: 0.8846\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0222 - acc: 0.9972 - val_loss: 0.4291 - val_acc: 0.8910\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0226 - acc: 0.9963 - val_loss: 0.4336 - val_acc: 0.8846\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0198 - acc: 0.9982 - val_loss: 0.4112 - val_acc: 0.8782\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0173 - acc: 0.9977 - val_loss: 0.4274 - val_acc: 0.8846\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0173 - acc: 0.9995 - val_loss: 0.4453 - val_acc: 0.8846\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0181 - acc: 0.9982 - val_loss: 0.4334 - val_acc: 0.8782\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0180 - acc: 0.9986 - val_loss: 0.4374 - val_acc: 0.8782\n",
      "Epoch 105/1000\n",
      "69/69 - 0s - loss: 0.0175 - acc: 0.9991 - val_loss: 0.4325 - val_acc: 0.8782\n",
      "Epoch 106/1000\n",
      "69/69 - 0s - loss: 0.0143 - acc: 0.9991 - val_loss: 0.4365 - val_acc: 0.8718\n",
      "Epoch 107/1000\n",
      "69/69 - 0s - loss: 0.0171 - acc: 0.9977 - val_loss: 0.4285 - val_acc: 0.8782\n",
      "Epoch 108/1000\n",
      "69/69 - 0s - loss: 0.0193 - acc: 0.9986 - val_loss: 0.4345 - val_acc: 0.8718\n",
      "Epoch 109/1000\n",
      "69/69 - 0s - loss: 0.0193 - acc: 0.9977 - val_loss: 0.4275 - val_acc: 0.8782\n",
      "Epoch 110/1000\n",
      "69/69 - 0s - loss: 0.0139 - acc: 0.9982 - val_loss: 0.4303 - val_acc: 0.8782\n",
      "Epoch 111/1000\n",
      "69/69 - 0s - loss: 0.0163 - acc: 0.9977 - val_loss: 0.4347 - val_acc: 0.8910\n",
      "Epoch 112/1000\n",
      "69/69 - 0s - loss: 0.0185 - acc: 0.9972 - val_loss: 0.4373 - val_acc: 0.8910\n",
      "Epoch 113/1000\n",
      "69/69 - 0s - loss: 0.0158 - acc: 0.9995 - val_loss: 0.4674 - val_acc: 0.8782\n",
      "Epoch 114/1000\n",
      "69/69 - 0s - loss: 0.0132 - acc: 0.9995 - val_loss: 0.4620 - val_acc: 0.8782\n",
      "Epoch 115/1000\n",
      "69/69 - 0s - loss: 0.0166 - acc: 0.9991 - val_loss: 0.4687 - val_acc: 0.8846\n",
      "Epoch 116/1000\n",
      "69/69 - 0s - loss: 0.0145 - acc: 0.9986 - val_loss: 0.4609 - val_acc: 0.8846\n",
      "Epoch 117/1000\n",
      "69/69 - 0s - loss: 0.0153 - acc: 0.9977 - val_loss: 0.4522 - val_acc: 0.8846\n",
      "Epoch 118/1000\n",
      "69/69 - 0s - loss: 0.0135 - acc: 0.9995 - val_loss: 0.4494 - val_acc: 0.8654\n",
      "Epoch 119/1000\n",
      "69/69 - 0s - loss: 0.0144 - acc: 0.9991 - val_loss: 0.4544 - val_acc: 0.8782\n",
      "Epoch 120/1000\n",
      "69/69 - 0s - loss: 0.0125 - acc: 0.9991 - val_loss: 0.4648 - val_acc: 0.8846\n",
      "Epoch 121/1000\n",
      "69/69 - 0s - loss: 0.0133 - acc: 0.9991 - val_loss: 0.4771 - val_acc: 0.8846\n",
      "Epoch 122/1000\n",
      "69/69 - 0s - loss: 0.0149 - acc: 0.9986 - val_loss: 0.4525 - val_acc: 0.8846\n",
      "Epoch 123/1000\n",
      "69/69 - 0s - loss: 0.0114 - acc: 1.0000 - val_loss: 0.4633 - val_acc: 0.8846\n",
      "Epoch 124/1000\n",
      "69/69 - 0s - loss: 0.0192 - acc: 0.9977 - val_loss: 0.4763 - val_acc: 0.8782\n",
      "Epoch 125/1000\n",
      "69/69 - 0s - loss: 0.0129 - acc: 0.9995 - val_loss: 0.4750 - val_acc: 0.8782\n",
      "Epoch 126/1000\n",
      "69/69 - 0s - loss: 0.0141 - acc: 0.9995 - val_loss: 0.4732 - val_acc: 0.8782\n",
      "Epoch 127/1000\n",
      "69/69 - 0s - loss: 0.0164 - acc: 0.9982 - val_loss: 0.4637 - val_acc: 0.8846\n",
      "Epoch 128/1000\n",
      "69/69 - 0s - loss: 0.0135 - acc: 0.9995 - val_loss: 0.4770 - val_acc: 0.8846\n",
      "Epoch 129/1000\n",
      "69/69 - 0s - loss: 0.0147 - acc: 0.9986 - val_loss: 0.4656 - val_acc: 0.8910\n",
      "Epoch 130/1000\n",
      "69/69 - 0s - loss: 0.0175 - acc: 0.9977 - val_loss: 0.4698 - val_acc: 0.8910\n",
      "Epoch 131/1000\n",
      "69/69 - 0s - loss: 0.0149 - acc: 0.9991 - val_loss: 0.4675 - val_acc: 0.8782\n",
      "Epoch 132/1000\n",
      "69/69 - 0s - loss: 0.0111 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 0.8718\n",
      "Epoch 133/1000\n",
      "69/69 - 0s - loss: 0.0119 - acc: 1.0000 - val_loss: 0.4730 - val_acc: 0.8718\n",
      "Epoch 134/1000\n",
      "69/69 - 0s - loss: 0.0108 - acc: 0.9995 - val_loss: 0.4718 - val_acc: 0.8782\n",
      "Epoch 135/1000\n",
      "69/69 - 0s - loss: 0.0114 - acc: 0.9991 - val_loss: 0.4643 - val_acc: 0.8718\n",
      "Epoch 136/1000\n",
      "69/69 - 0s - loss: 0.0119 - acc: 0.9995 - val_loss: 0.4751 - val_acc: 0.8718\n",
      "Epoch 137/1000\n",
      "69/69 - 0s - loss: 0.0123 - acc: 0.9982 - val_loss: 0.4821 - val_acc: 0.8910\n",
      "Epoch 138/1000\n",
      "69/69 - 0s - loss: 0.0120 - acc: 0.9995 - val_loss: 0.4486 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 139/1000\n",
      "69/69 - 0s - loss: 0.0139 - acc: 0.9995 - val_loss: 0.4659 - val_acc: 0.8782\n",
      "Epoch 140/1000\n",
      "69/69 - 0s - loss: 0.0111 - acc: 1.0000 - val_loss: 0.4772 - val_acc: 0.8782\n",
      "Epoch 141/1000\n",
      "69/69 - 0s - loss: 0.0117 - acc: 0.9995 - val_loss: 0.4810 - val_acc: 0.8782\n",
      "Epoch 142/1000\n",
      "69/69 - 0s - loss: 0.0123 - acc: 0.9986 - val_loss: 0.4811 - val_acc: 0.8782\n",
      "Epoch 143/1000\n",
      "69/69 - 0s - loss: 0.0143 - acc: 0.9986 - val_loss: 0.4835 - val_acc: 0.8782\n",
      "Epoch 144/1000\n",
      "69/69 - 0s - loss: 0.0110 - acc: 0.9995 - val_loss: 0.4835 - val_acc: 0.8782\n",
      "Epoch 145/1000\n",
      "69/69 - 0s - loss: 0.0108 - acc: 1.0000 - val_loss: 0.4877 - val_acc: 0.8782\n",
      "Epoch 146/1000\n",
      "69/69 - 0s - loss: 0.0127 - acc: 0.9995 - val_loss: 0.4842 - val_acc: 0.8782\n",
      "Epoch 147/1000\n",
      "69/69 - 0s - loss: 0.0121 - acc: 1.0000 - val_loss: 0.4847 - val_acc: 0.8782\n",
      "Epoch 148/1000\n",
      "69/69 - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 0.4836 - val_acc: 0.8782\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 1.0624 - acc: 0.5493 - val_loss: 1.4217 - val_acc: 0.2564\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6675 - acc: 0.7705 - val_loss: 1.4640 - val_acc: 0.2564\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5094 - acc: 0.8155 - val_loss: 1.5575 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4503 - acc: 0.8435 - val_loss: 1.7399 - val_acc: 0.3013\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4409 - acc: 0.8270 - val_loss: 2.8933 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4094 - acc: 0.8472 - val_loss: 2.5860 - val_acc: 0.2628\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3668 - acc: 0.8646 - val_loss: 1.9368 - val_acc: 0.4231\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3733 - acc: 0.8665 - val_loss: 0.8854 - val_acc: 0.6154\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3471 - acc: 0.8692 - val_loss: 0.6494 - val_acc: 0.7179\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3385 - acc: 0.8733 - val_loss: 1.0622 - val_acc: 0.5833\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3356 - acc: 0.8761 - val_loss: 0.7138 - val_acc: 0.7308\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3225 - acc: 0.8793 - val_loss: 0.4397 - val_acc: 0.8141\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.2932 - acc: 0.8926 - val_loss: 0.4079 - val_acc: 0.8462\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3008 - acc: 0.8825 - val_loss: 0.4813 - val_acc: 0.8333\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3009 - acc: 0.8876 - val_loss: 0.6981 - val_acc: 0.7564\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2895 - acc: 0.8894 - val_loss: 0.4308 - val_acc: 0.8333\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2695 - acc: 0.8972 - val_loss: 0.4668 - val_acc: 0.8333\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2598 - acc: 0.9032 - val_loss: 0.5703 - val_acc: 0.7821\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2575 - acc: 0.9101 - val_loss: 0.4568 - val_acc: 0.8333\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2623 - acc: 0.9050 - val_loss: 0.4099 - val_acc: 0.8590\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2360 - acc: 0.9146 - val_loss: 0.4957 - val_acc: 0.8205\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2423 - acc: 0.9128 - val_loss: 0.4560 - val_acc: 0.8333\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2351 - acc: 0.9197 - val_loss: 1.1737 - val_acc: 0.5513\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2522 - acc: 0.9119 - val_loss: 0.4574 - val_acc: 0.8333\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2171 - acc: 0.9224 - val_loss: 0.5957 - val_acc: 0.8269\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2115 - acc: 0.9247 - val_loss: 0.7446 - val_acc: 0.7244\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.1886 - acc: 0.9339 - val_loss: 0.4822 - val_acc: 0.8077\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2010 - acc: 0.9247 - val_loss: 0.9521 - val_acc: 0.6987\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2030 - acc: 0.9312 - val_loss: 0.6820 - val_acc: 0.7564\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1910 - acc: 0.9266 - val_loss: 0.4869 - val_acc: 0.8269\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1782 - acc: 0.9422 - val_loss: 0.7456 - val_acc: 0.7051\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1639 - acc: 0.9436 - val_loss: 0.5282 - val_acc: 0.7949\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1570 - acc: 0.9445 - val_loss: 0.6589 - val_acc: 0.7756\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1592 - acc: 0.9403 - val_loss: 0.8104 - val_acc: 0.7244\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1361 - acc: 0.9523 - val_loss: 0.5324 - val_acc: 0.8205\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1474 - acc: 0.9468 - val_loss: 0.8572 - val_acc: 0.7436\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1445 - acc: 0.9514 - val_loss: 0.6936 - val_acc: 0.7885\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1436 - acc: 0.9541 - val_loss: 1.0105 - val_acc: 0.7115\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1300 - acc: 0.9592 - val_loss: 0.6034 - val_acc: 0.8077\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1420 - acc: 0.9514 - val_loss: 0.4795 - val_acc: 0.8205\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1406 - acc: 0.9518 - val_loss: 0.4609 - val_acc: 0.8333\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1417 - acc: 0.9504 - val_loss: 0.5116 - val_acc: 0.8205\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1056 - acc: 0.9697 - val_loss: 0.4793 - val_acc: 0.8526\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.0936 - acc: 0.9720 - val_loss: 0.5832 - val_acc: 0.7692\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.0958 - acc: 0.9693 - val_loss: 0.5370 - val_acc: 0.8205\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1632 - acc: 0.9458 - val_loss: 0.4459 - val_acc: 0.8397\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1001 - acc: 0.9729 - val_loss: 0.5177 - val_acc: 0.8462\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1138 - acc: 0.9628 - val_loss: 0.5349 - val_acc: 0.8397\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1072 - acc: 0.9706 - val_loss: 0.6729 - val_acc: 0.7821\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0826 - acc: 0.9766 - val_loss: 0.5254 - val_acc: 0.8333\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0862 - acc: 0.9761 - val_loss: 0.6164 - val_acc: 0.8269\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0837 - acc: 0.9720 - val_loss: 0.4951 - val_acc: 0.8333\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0704 - acc: 0.9789 - val_loss: 0.6919 - val_acc: 0.8013\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0677 - acc: 0.9784 - val_loss: 0.4994 - val_acc: 0.8462\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0865 - acc: 0.9738 - val_loss: 0.6195 - val_acc: 0.8141\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1105 - acc: 0.9619 - val_loss: 0.5634 - val_acc: 0.8205\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0650 - acc: 0.9789 - val_loss: 0.7009 - val_acc: 0.8333\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0676 - acc: 0.9821 - val_loss: 0.6068 - val_acc: 0.8397\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0871 - acc: 0.9702 - val_loss: 0.7160 - val_acc: 0.7436\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0835 - acc: 0.9757 - val_loss: 0.6025 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0590 - acc: 0.9849 - val_loss: 0.5439 - val_acc: 0.8526\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0450 - acc: 0.9899 - val_loss: 0.5473 - val_acc: 0.8462\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0383 - acc: 0.9950 - val_loss: 0.5461 - val_acc: 0.8526\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0393 - acc: 0.9904 - val_loss: 0.5529 - val_acc: 0.8526\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0375 - acc: 0.9936 - val_loss: 0.5483 - val_acc: 0.8526\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0393 - acc: 0.9917 - val_loss: 0.5406 - val_acc: 0.8526\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0336 - acc: 0.9959 - val_loss: 0.5405 - val_acc: 0.8526\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0318 - acc: 0.9968 - val_loss: 0.5411 - val_acc: 0.8526\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0371 - acc: 0.9922 - val_loss: 0.5489 - val_acc: 0.8397\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0312 - acc: 0.9954 - val_loss: 0.5511 - val_acc: 0.8526\n",
      "Epoch 1/1000\n",
      "69/69 - 1s - loss: 0.9206 - acc: 0.6347 - val_loss: 1.4089 - val_acc: 0.2436\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6209 - acc: 0.7724 - val_loss: 1.5874 - val_acc: 0.2436\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5268 - acc: 0.8050 - val_loss: 2.3689 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4615 - acc: 0.8371 - val_loss: 3.6796 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4423 - acc: 0.8421 - val_loss: 7.1755 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4117 - acc: 0.8444 - val_loss: 6.1426 - val_acc: 0.2436\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.4121 - acc: 0.8499 - val_loss: 4.9017 - val_acc: 0.2436\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3800 - acc: 0.8628 - val_loss: 3.6455 - val_acc: 0.2949\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3642 - acc: 0.8669 - val_loss: 3.3310 - val_acc: 0.3013\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3573 - acc: 0.8632 - val_loss: 1.3884 - val_acc: 0.5321\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3391 - acc: 0.8697 - val_loss: 0.5486 - val_acc: 0.8141\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3150 - acc: 0.8816 - val_loss: 1.1361 - val_acc: 0.6346\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3137 - acc: 0.8816 - val_loss: 0.5125 - val_acc: 0.8205\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3232 - acc: 0.8807 - val_loss: 0.8352 - val_acc: 0.6923\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2974 - acc: 0.8871 - val_loss: 1.1453 - val_acc: 0.5769\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2814 - acc: 0.8949 - val_loss: 0.5196 - val_acc: 0.8205\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.3240 - acc: 0.8770 - val_loss: 0.5868 - val_acc: 0.7692\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2906 - acc: 0.8908 - val_loss: 0.5277 - val_acc: 0.8077\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2581 - acc: 0.9101 - val_loss: 0.5945 - val_acc: 0.8077\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2459 - acc: 0.9105 - val_loss: 0.6484 - val_acc: 0.7628\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2567 - acc: 0.9073 - val_loss: 0.4527 - val_acc: 0.8397\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2373 - acc: 0.9146 - val_loss: 0.5817 - val_acc: 0.8077\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2381 - acc: 0.9133 - val_loss: 0.4609 - val_acc: 0.8397\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2195 - acc: 0.9224 - val_loss: 0.8328 - val_acc: 0.6987\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2188 - acc: 0.9206 - val_loss: 0.6146 - val_acc: 0.8013\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2484 - acc: 0.9082 - val_loss: 0.5401 - val_acc: 0.8141\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2030 - acc: 0.9344 - val_loss: 0.5711 - val_acc: 0.7949\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2373 - acc: 0.9101 - val_loss: 1.0097 - val_acc: 0.7051\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1900 - acc: 0.9302 - val_loss: 0.5086 - val_acc: 0.8141\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1879 - acc: 0.9371 - val_loss: 0.5109 - val_acc: 0.8141\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.2137 - acc: 0.9243 - val_loss: 0.5896 - val_acc: 0.7949\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1792 - acc: 0.9358 - val_loss: 1.2824 - val_acc: 0.6346\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1903 - acc: 0.9316 - val_loss: 0.5370 - val_acc: 0.8269\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1818 - acc: 0.9348 - val_loss: 0.5517 - val_acc: 0.8077\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1565 - acc: 0.9468 - val_loss: 0.8252 - val_acc: 0.7372\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1713 - acc: 0.9413 - val_loss: 0.5037 - val_acc: 0.8077\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1580 - acc: 0.9477 - val_loss: 0.5498 - val_acc: 0.8141\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1648 - acc: 0.9440 - val_loss: 0.5584 - val_acc: 0.7949\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1726 - acc: 0.9431 - val_loss: 0.5380 - val_acc: 0.8462\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1448 - acc: 0.9518 - val_loss: 0.6483 - val_acc: 0.7756\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1268 - acc: 0.9569 - val_loss: 0.6370 - val_acc: 0.7692\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1637 - acc: 0.9431 - val_loss: 0.6093 - val_acc: 0.7821\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1785 - acc: 0.9394 - val_loss: 0.5357 - val_acc: 0.8013\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1596 - acc: 0.9394 - val_loss: 0.6733 - val_acc: 0.7500\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1106 - acc: 0.9679 - val_loss: 0.5902 - val_acc: 0.8397\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1297 - acc: 0.9582 - val_loss: 0.6102 - val_acc: 0.8205\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1332 - acc: 0.9578 - val_loss: 0.7536 - val_acc: 0.7436\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1349 - acc: 0.9527 - val_loss: 0.5127 - val_acc: 0.8333\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.1047 - acc: 0.9679 - val_loss: 0.6790 - val_acc: 0.8205\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0939 - acc: 0.9738 - val_loss: 0.8970 - val_acc: 0.7179\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.1270 - acc: 0.9624 - val_loss: 0.6791 - val_acc: 0.8269\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1160 - acc: 0.9637 - val_loss: 0.7847 - val_acc: 0.7564\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.1283 - acc: 0.9559 - val_loss: 0.7193 - val_acc: 0.7756\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0886 - acc: 0.9771 - val_loss: 0.5404 - val_acc: 0.8269\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0742 - acc: 0.9807 - val_loss: 1.0181 - val_acc: 0.7436\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1655 - acc: 0.9413 - val_loss: 0.8269 - val_acc: 0.7949\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.1006 - acc: 0.9693 - val_loss: 0.7069 - val_acc: 0.8141\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0758 - acc: 0.9784 - val_loss: 0.6125 - val_acc: 0.8141\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0757 - acc: 0.9784 - val_loss: 0.6779 - val_acc: 0.8462\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0861 - acc: 0.9729 - val_loss: 0.5790 - val_acc: 0.8269\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0715 - acc: 0.9789 - val_loss: 0.7409 - val_acc: 0.7821\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0666 - acc: 0.9793 - val_loss: 0.6332 - val_acc: 0.8141\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0632 - acc: 0.9844 - val_loss: 0.6688 - val_acc: 0.8269\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.1341 - acc: 0.9536 - val_loss: 0.7297 - val_acc: 0.7949\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0920 - acc: 0.9748 - val_loss: 0.6350 - val_acc: 0.7949\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.1054 - acc: 0.9651 - val_loss: 0.6616 - val_acc: 0.8013\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0614 - acc: 0.9862 - val_loss: 0.8627 - val_acc: 0.7692\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0765 - acc: 0.9766 - val_loss: 0.5804 - val_acc: 0.8397\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0584 - acc: 0.9858 - val_loss: 0.6443 - val_acc: 0.8205\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0667 - acc: 0.9803 - val_loss: 0.7207 - val_acc: 0.8077\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0556 - acc: 0.9899 - val_loss: 0.5555 - val_acc: 0.8205\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.1201 - acc: 0.9569 - val_loss: 1.8005 - val_acc: 0.6410\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.1099 - acc: 0.9610 - val_loss: 0.7532 - val_acc: 0.7885\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0831 - acc: 0.9734 - val_loss: 0.7763 - val_acc: 0.7756\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0937 - acc: 0.9679 - val_loss: 0.6820 - val_acc: 0.8205\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0578 - acc: 0.9858 - val_loss: 0.7389 - val_acc: 0.8333\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0576 - acc: 0.9844 - val_loss: 0.6434 - val_acc: 0.8269\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0407 - acc: 0.9927 - val_loss: 0.7127 - val_acc: 0.8269\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0809 - acc: 0.9734 - val_loss: 1.0135 - val_acc: 0.7885\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0458 - acc: 0.9890 - val_loss: 0.8125 - val_acc: 0.8141\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0407 - acc: 0.9904 - val_loss: 0.7488 - val_acc: 0.8333\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0401 - acc: 0.9950 - val_loss: 0.7058 - val_acc: 0.8462\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0342 - acc: 0.9959 - val_loss: 0.6803 - val_acc: 0.8462\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0322 - acc: 0.9945 - val_loss: 0.6784 - val_acc: 0.8397\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0292 - acc: 0.9968 - val_loss: 0.6739 - val_acc: 0.8397\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0288 - acc: 0.9945 - val_loss: 0.6855 - val_acc: 0.8269\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0288 - acc: 0.9977 - val_loss: 0.6902 - val_acc: 0.8333\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0292 - acc: 0.9963 - val_loss: 0.6819 - val_acc: 0.8333\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0247 - acc: 0.9977 - val_loss: 0.6815 - val_acc: 0.8333\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9758 - acc: 0.6014 - val_loss: 1.4086 - val_acc: 0.2516\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6298 - acc: 0.7748 - val_loss: 1.5235 - val_acc: 0.2581\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5010 - acc: 0.8248 - val_loss: 2.0158 - val_acc: 0.2581\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4400 - acc: 0.8376 - val_loss: 3.1587 - val_acc: 0.2581\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4356 - acc: 0.8394 - val_loss: 4.7710 - val_acc: 0.2581\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4028 - acc: 0.8550 - val_loss: 3.1107 - val_acc: 0.2581\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3986 - acc: 0.8587 - val_loss: 1.8957 - val_acc: 0.5097\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3596 - acc: 0.8661 - val_loss: 1.3814 - val_acc: 0.4774\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3526 - acc: 0.8807 - val_loss: 1.0571 - val_acc: 0.6645\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3598 - acc: 0.8606 - val_loss: 0.7884 - val_acc: 0.6323\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3458 - acc: 0.8775 - val_loss: 0.3444 - val_acc: 0.8774\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3184 - acc: 0.8885 - val_loss: 0.4178 - val_acc: 0.8452\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3068 - acc: 0.8890 - val_loss: 0.4613 - val_acc: 0.8000\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3106 - acc: 0.8890 - val_loss: 0.4685 - val_acc: 0.8258\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2879 - acc: 0.8927 - val_loss: 0.3974 - val_acc: 0.8323\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2899 - acc: 0.8922 - val_loss: 0.6220 - val_acc: 0.7290\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2704 - acc: 0.9041 - val_loss: 0.2784 - val_acc: 0.8839\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2609 - acc: 0.9055 - val_loss: 0.7035 - val_acc: 0.7290\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2547 - acc: 0.9092 - val_loss: 0.4141 - val_acc: 0.8645\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2497 - acc: 0.9046 - val_loss: 0.5220 - val_acc: 0.8129\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2476 - acc: 0.9092 - val_loss: 0.3905 - val_acc: 0.8452\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2227 - acc: 0.9220 - val_loss: 0.3982 - val_acc: 0.8581\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2162 - acc: 0.9188 - val_loss: 0.4532 - val_acc: 0.8323\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2143 - acc: 0.9271 - val_loss: 0.4150 - val_acc: 0.8452\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2091 - acc: 0.9257 - val_loss: 0.4589 - val_acc: 0.8516\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.1916 - acc: 0.9344 - val_loss: 0.5221 - val_acc: 0.8129\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.1914 - acc: 0.9372 - val_loss: 0.4923 - val_acc: 0.8387\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.1980 - acc: 0.9289 - val_loss: 0.8481 - val_acc: 0.7355\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1554 - acc: 0.9450 - val_loss: 0.4047 - val_acc: 0.8581\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1696 - acc: 0.9413 - val_loss: 0.4163 - val_acc: 0.8645\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1718 - acc: 0.9362 - val_loss: 0.3403 - val_acc: 0.8774\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1823 - acc: 0.9326 - val_loss: 0.3061 - val_acc: 0.8839\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1728 - acc: 0.9413 - val_loss: 0.4322 - val_acc: 0.8516\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1484 - acc: 0.9495 - val_loss: 0.5855 - val_acc: 0.8452\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1636 - acc: 0.9431 - val_loss: 0.6262 - val_acc: 0.7613\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1287 - acc: 0.9573 - val_loss: 0.3429 - val_acc: 0.8903\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1401 - acc: 0.9523 - val_loss: 0.4709 - val_acc: 0.8516\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1132 - acc: 0.9596 - val_loss: 0.3788 - val_acc: 0.8839\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1153 - acc: 0.9610 - val_loss: 0.3659 - val_acc: 0.8645\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1614 - acc: 0.9505 - val_loss: 0.4022 - val_acc: 0.8710\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1321 - acc: 0.9546 - val_loss: 0.4317 - val_acc: 0.8581\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1194 - acc: 0.9628 - val_loss: 0.3845 - val_acc: 0.8774\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1722 - acc: 0.9367 - val_loss: 0.4486 - val_acc: 0.8387\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1367 - acc: 0.9550 - val_loss: 0.4099 - val_acc: 0.8581\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.0992 - acc: 0.9716 - val_loss: 0.4364 - val_acc: 0.8516\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.0904 - acc: 0.9748 - val_loss: 0.4185 - val_acc: 0.8581\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.0753 - acc: 0.9821 - val_loss: 0.4936 - val_acc: 0.8323\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0565 - acc: 0.9872 - val_loss: 0.7402 - val_acc: 0.7355\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0953 - acc: 0.9725 - val_loss: 3.6521 - val_acc: 0.5161\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0702 - acc: 0.9794 - val_loss: 0.3659 - val_acc: 0.8839\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0623 - acc: 0.9839 - val_loss: 0.4299 - val_acc: 0.8645\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0498 - acc: 0.9908 - val_loss: 0.4641 - val_acc: 0.8516\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0770 - acc: 0.9766 - val_loss: 0.4152 - val_acc: 0.8581\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0783 - acc: 0.9757 - val_loss: 0.3698 - val_acc: 0.8968\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0739 - acc: 0.9771 - val_loss: 0.3449 - val_acc: 0.8903\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.1163 - acc: 0.9592 - val_loss: 0.3980 - val_acc: 0.8387\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.1022 - acc: 0.9633 - val_loss: 0.3955 - val_acc: 0.8645\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0580 - acc: 0.9858 - val_loss: 0.3913 - val_acc: 0.8903\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0417 - acc: 0.9927 - val_loss: 0.3741 - val_acc: 0.8839\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0658 - acc: 0.9807 - val_loss: 0.4085 - val_acc: 0.8516\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0466 - acc: 0.9899 - val_loss: 0.4103 - val_acc: 0.8903\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0466 - acc: 0.9899 - val_loss: 0.4442 - val_acc: 0.8645\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0390 - acc: 0.9917 - val_loss: 0.3966 - val_acc: 0.8774\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0437 - acc: 0.9908 - val_loss: 0.4074 - val_acc: 0.8774\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0754 - acc: 0.9817 - val_loss: 1.4684 - val_acc: 0.7032\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0847 - acc: 0.9729 - val_loss: 0.5160 - val_acc: 0.8710\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.1048 - acc: 0.9587 - val_loss: 0.4482 - val_acc: 0.8581\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0713 - acc: 0.9757 - val_loss: 0.4470 - val_acc: 0.8516\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0523 - acc: 0.9881 - val_loss: 0.4588 - val_acc: 0.8645\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0439 - acc: 0.9862 - val_loss: 0.4415 - val_acc: 0.8645\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0358 - acc: 0.9913 - val_loss: 0.5160 - val_acc: 0.8323\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0316 - acc: 0.9940 - val_loss: 0.4385 - val_acc: 0.8839\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0304 - acc: 0.9931 - val_loss: 0.4447 - val_acc: 0.8581\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0734 - acc: 0.9720 - val_loss: 0.5407 - val_acc: 0.8645\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0692 - acc: 0.9807 - val_loss: 0.4426 - val_acc: 0.8710\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0453 - acc: 0.9885 - val_loss: 0.4170 - val_acc: 0.8839\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0348 - acc: 0.9940 - val_loss: 0.4473 - val_acc: 0.8968\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0410 - acc: 0.9894 - val_loss: 0.4094 - val_acc: 0.8839\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0276 - acc: 0.9950 - val_loss: 0.5647 - val_acc: 0.8581\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0367 - acc: 0.9922 - val_loss: 0.5660 - val_acc: 0.8581\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0318 - acc: 0.9940 - val_loss: 0.5245 - val_acc: 0.8774\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0635 - acc: 0.9775 - val_loss: 0.3996 - val_acc: 0.8968\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0476 - acc: 0.9858 - val_loss: 0.4628 - val_acc: 0.8710\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0243 - acc: 0.9959 - val_loss: 0.4949 - val_acc: 0.8839\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0437 - acc: 0.9867 - val_loss: 0.4922 - val_acc: 0.8452\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0296 - acc: 0.9936 - val_loss: 0.4355 - val_acc: 0.8194\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0666 - acc: 0.9803 - val_loss: 0.6463 - val_acc: 0.8774\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0684 - acc: 0.9784 - val_loss: 0.5812 - val_acc: 0.8645\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0996 - acc: 0.9628 - val_loss: 0.6595 - val_acc: 0.8581\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0372 - acc: 0.9894 - val_loss: 0.8990 - val_acc: 0.7484\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0722 - acc: 0.9775 - val_loss: 0.4166 - val_acc: 0.8516\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0585 - acc: 0.9821 - val_loss: 0.4181 - val_acc: 0.8581\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0388 - acc: 0.9908 - val_loss: 0.4710 - val_acc: 0.8323\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0317 - acc: 0.9917 - val_loss: 0.5315 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0269 - acc: 0.9954 - val_loss: 0.3977 - val_acc: 0.8710\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0251 - acc: 0.9972 - val_loss: 0.3898 - val_acc: 0.8774\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0156 - acc: 0.9986 - val_loss: 0.3819 - val_acc: 0.8774\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0129 - acc: 0.9977 - val_loss: 0.3901 - val_acc: 0.8774\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0139 - acc: 0.9995 - val_loss: 0.3922 - val_acc: 0.8710\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0179 - acc: 0.9972 - val_loss: 0.4061 - val_acc: 0.8710\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0131 - acc: 1.0000 - val_loss: 0.4326 - val_acc: 0.8903\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0145 - acc: 0.9982 - val_loss: 0.4208 - val_acc: 0.8774\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0164 - acc: 0.9977 - val_loss: 0.3901 - val_acc: 0.8710\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0119 - acc: 0.9995 - val_loss: 0.4098 - val_acc: 0.8774\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 1.0090 - acc: 0.5803 - val_loss: 1.3999 - val_acc: 0.2452\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6217 - acc: 0.7803 - val_loss: 1.4564 - val_acc: 0.2774\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4934 - acc: 0.8193 - val_loss: 1.8575 - val_acc: 0.2516\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4428 - acc: 0.8427 - val_loss: 2.4845 - val_acc: 0.2516\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4226 - acc: 0.8477 - val_loss: 2.6103 - val_acc: 0.2645\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.3729 - acc: 0.8647 - val_loss: 1.6633 - val_acc: 0.2903\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3688 - acc: 0.8665 - val_loss: 0.9924 - val_acc: 0.5742\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3501 - acc: 0.8674 - val_loss: 0.5110 - val_acc: 0.8194\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3257 - acc: 0.8817 - val_loss: 0.6823 - val_acc: 0.7226\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3287 - acc: 0.8771 - val_loss: 0.4028 - val_acc: 0.8129\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3120 - acc: 0.8853 - val_loss: 1.3663 - val_acc: 0.4452\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3016 - acc: 0.8849 - val_loss: 0.4446 - val_acc: 0.7935\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.2873 - acc: 0.8927 - val_loss: 0.4315 - val_acc: 0.8452\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.3216 - acc: 0.8803 - val_loss: 0.2656 - val_acc: 0.8774\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2909 - acc: 0.8959 - val_loss: 0.2994 - val_acc: 0.8903\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2683 - acc: 0.9023 - val_loss: 0.6696 - val_acc: 0.7355\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2560 - acc: 0.9069 - val_loss: 2.0682 - val_acc: 0.6194\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2646 - acc: 0.9037 - val_loss: 0.4900 - val_acc: 0.8258\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2408 - acc: 0.9124 - val_loss: 0.4454 - val_acc: 0.8323\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2332 - acc: 0.9161 - val_loss: 0.6277 - val_acc: 0.7742\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2462 - acc: 0.9069 - val_loss: 0.3841 - val_acc: 0.8516\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2252 - acc: 0.9188 - val_loss: 0.5999 - val_acc: 0.7484\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2082 - acc: 0.9275 - val_loss: 0.3923 - val_acc: 0.8452\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2208 - acc: 0.9229 - val_loss: 0.3487 - val_acc: 0.8774\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.1872 - acc: 0.9271 - val_loss: 0.3440 - val_acc: 0.8903\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2217 - acc: 0.9252 - val_loss: 0.3730 - val_acc: 0.8452\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2051 - acc: 0.9303 - val_loss: 0.3178 - val_acc: 0.8839\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.1858 - acc: 0.9372 - val_loss: 0.4190 - val_acc: 0.8387\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1871 - acc: 0.9358 - val_loss: 0.3536 - val_acc: 0.8323\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.2006 - acc: 0.9257 - val_loss: 1.0059 - val_acc: 0.6645\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1654 - acc: 0.9427 - val_loss: 0.6847 - val_acc: 0.7806\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1766 - acc: 0.9413 - val_loss: 0.3257 - val_acc: 0.8774\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1554 - acc: 0.9459 - val_loss: 0.4468 - val_acc: 0.8387\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1753 - acc: 0.9394 - val_loss: 0.3557 - val_acc: 0.8516\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1375 - acc: 0.9546 - val_loss: 0.4911 - val_acc: 0.8065\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1497 - acc: 0.9472 - val_loss: 0.2893 - val_acc: 0.8903\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1365 - acc: 0.9560 - val_loss: 0.3254 - val_acc: 0.8645\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1321 - acc: 0.9596 - val_loss: 0.3886 - val_acc: 0.8258\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1330 - acc: 0.9541 - val_loss: 0.4116 - val_acc: 0.8645\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1161 - acc: 0.9633 - val_loss: 0.2978 - val_acc: 0.8903\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.0983 - acc: 0.9711 - val_loss: 0.3720 - val_acc: 0.8452\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.0967 - acc: 0.9697 - val_loss: 0.3958 - val_acc: 0.8387\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1277 - acc: 0.9587 - val_loss: 0.3657 - val_acc: 0.8839\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.0997 - acc: 0.9697 - val_loss: 0.2812 - val_acc: 0.8710\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.0866 - acc: 0.9734 - val_loss: 0.4325 - val_acc: 0.8129\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.0827 - acc: 0.9720 - val_loss: 0.4942 - val_acc: 0.8258\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1175 - acc: 0.9606 - val_loss: 0.3502 - val_acc: 0.8903\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.1012 - acc: 0.9647 - val_loss: 0.3170 - val_acc: 0.8581\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0949 - acc: 0.9670 - val_loss: 0.4377 - val_acc: 0.8387\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.1110 - acc: 0.9633 - val_loss: 0.3317 - val_acc: 0.8774\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0950 - acc: 0.9683 - val_loss: 0.3438 - val_acc: 0.8645\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1023 - acc: 0.9706 - val_loss: 0.4594 - val_acc: 0.8258\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0889 - acc: 0.9757 - val_loss: 0.3641 - val_acc: 0.8903\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0943 - acc: 0.9725 - val_loss: 0.3944 - val_acc: 0.8452\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.1214 - acc: 0.9615 - val_loss: 0.4479 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0813 - acc: 0.9771 - val_loss: 0.2497 - val_acc: 0.8968\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0567 - acc: 0.9885 - val_loss: 0.2563 - val_acc: 0.8903\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0481 - acc: 0.9908 - val_loss: 0.2646 - val_acc: 0.8774\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0500 - acc: 0.9885 - val_loss: 0.2693 - val_acc: 0.8645\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0510 - acc: 0.9913 - val_loss: 0.2834 - val_acc: 0.8774\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0429 - acc: 0.9922 - val_loss: 0.2793 - val_acc: 0.8774\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0452 - acc: 0.9940 - val_loss: 0.2868 - val_acc: 0.8774\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0388 - acc: 0.9945 - val_loss: 0.2948 - val_acc: 0.8645\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0412 - acc: 0.9950 - val_loss: 0.2971 - val_acc: 0.8774\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0379 - acc: 0.9950 - val_loss: 0.2963 - val_acc: 0.8839\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0442 - acc: 0.9917 - val_loss: 0.2938 - val_acc: 0.8774\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0390 - acc: 0.9959 - val_loss: 0.2969 - val_acc: 0.8774\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0391 - acc: 0.9954 - val_loss: 0.2928 - val_acc: 0.8710\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0399 - acc: 0.9950 - val_loss: 0.3127 - val_acc: 0.8710\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0373 - acc: 0.9968 - val_loss: 0.3040 - val_acc: 0.8710\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0343 - acc: 0.9963 - val_loss: 0.3086 - val_acc: 0.8710\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0316 - acc: 0.9977 - val_loss: 0.3205 - val_acc: 0.8710\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0347 - acc: 0.9968 - val_loss: 0.3079 - val_acc: 0.8710\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0340 - acc: 0.9963 - val_loss: 0.3059 - val_acc: 0.8645\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0319 - acc: 0.9945 - val_loss: 0.3006 - val_acc: 0.8710\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0314 - acc: 0.9968 - val_loss: 0.3024 - val_acc: 0.8839\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0309 - acc: 0.9959 - val_loss: 0.3100 - val_acc: 0.8710\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0299 - acc: 0.9972 - val_loss: 0.3102 - val_acc: 0.8710\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0326 - acc: 0.9963 - val_loss: 0.3130 - val_acc: 0.8645\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0278 - acc: 0.9972 - val_loss: 0.3250 - val_acc: 0.8645\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0266 - acc: 0.9977 - val_loss: 0.3282 - val_acc: 0.8645\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0292 - acc: 0.9963 - val_loss: 0.3095 - val_acc: 0.8645\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0320 - acc: 0.9959 - val_loss: 0.3057 - val_acc: 0.8645\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0332 - acc: 0.9963 - val_loss: 0.3286 - val_acc: 0.8645\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0309 - acc: 0.9959 - val_loss: 0.3257 - val_acc: 0.8645\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0350 - acc: 0.9950 - val_loss: 0.3299 - val_acc: 0.8645\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0271 - acc: 0.9972 - val_loss: 0.3319 - val_acc: 0.8645\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0287 - acc: 0.9968 - val_loss: 0.3048 - val_acc: 0.8710\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0268 - acc: 0.9972 - val_loss: 0.3163 - val_acc: 0.8516\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0316 - acc: 0.9950 - val_loss: 0.3050 - val_acc: 0.8581\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0307 - acc: 0.9954 - val_loss: 0.2942 - val_acc: 0.8839\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0287 - acc: 0.9968 - val_loss: 0.3031 - val_acc: 0.8903\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0304 - acc: 0.9972 - val_loss: 0.3151 - val_acc: 0.8839\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0245 - acc: 0.9991 - val_loss: 0.3196 - val_acc: 0.8645\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0235 - acc: 0.9982 - val_loss: 0.3250 - val_acc: 0.8774\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0218 - acc: 0.9991 - val_loss: 0.3184 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0250 - acc: 0.9982 - val_loss: 0.3158 - val_acc: 0.8645\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0281 - acc: 0.9954 - val_loss: 0.3146 - val_acc: 0.8645\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0256 - acc: 0.9982 - val_loss: 0.3146 - val_acc: 0.8645\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0271 - acc: 0.9977 - val_loss: 0.3177 - val_acc: 0.8645\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0254 - acc: 0.9977 - val_loss: 0.3198 - val_acc: 0.8645\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0253 - acc: 0.9982 - val_loss: 0.3169 - val_acc: 0.8645\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0269 - acc: 0.9977 - val_loss: 0.3185 - val_acc: 0.8645\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0233 - acc: 0.9982 - val_loss: 0.3219 - val_acc: 0.8645\n",
      "Epoch 105/1000\n",
      "69/69 - 0s - loss: 0.0262 - acc: 0.9977 - val_loss: 0.3219 - val_acc: 0.8581\n",
      "Epoch 106/1000\n",
      "69/69 - 0s - loss: 0.0227 - acc: 0.9995 - val_loss: 0.3205 - val_acc: 0.8645\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9252 - acc: 0.6427 - val_loss: 1.4082 - val_acc: 0.2452\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5598 - acc: 0.7995 - val_loss: 1.4471 - val_acc: 0.2452\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4825 - acc: 0.8206 - val_loss: 1.4299 - val_acc: 0.2452\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4386 - acc: 0.8367 - val_loss: 1.8500 - val_acc: 0.2452\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.3952 - acc: 0.8564 - val_loss: 2.5207 - val_acc: 0.2452\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.3570 - acc: 0.8706 - val_loss: 2.6494 - val_acc: 0.2452\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3590 - acc: 0.8670 - val_loss: 2.4888 - val_acc: 0.3226\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3717 - acc: 0.8615 - val_loss: 1.5288 - val_acc: 0.4968\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3370 - acc: 0.8739 - val_loss: 0.7608 - val_acc: 0.6903\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3545 - acc: 0.8706 - val_loss: 0.5822 - val_acc: 0.7871\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3051 - acc: 0.8890 - val_loss: 0.4753 - val_acc: 0.8258\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3046 - acc: 0.8862 - val_loss: 0.5628 - val_acc: 0.7677\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.2865 - acc: 0.9000 - val_loss: 1.4197 - val_acc: 0.5613\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.2653 - acc: 0.9023 - val_loss: 0.6843 - val_acc: 0.7355\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2846 - acc: 0.9014 - val_loss: 1.3082 - val_acc: 0.6516\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2664 - acc: 0.8991 - val_loss: 0.6789 - val_acc: 0.7226\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2559 - acc: 0.9041 - val_loss: 0.4930 - val_acc: 0.7935\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2439 - acc: 0.9110 - val_loss: 0.6836 - val_acc: 0.7742\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2383 - acc: 0.9142 - val_loss: 0.6780 - val_acc: 0.7742\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2290 - acc: 0.9197 - val_loss: 0.6495 - val_acc: 0.7677\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2264 - acc: 0.9174 - val_loss: 0.5638 - val_acc: 0.7677\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2230 - acc: 0.9202 - val_loss: 0.7240 - val_acc: 0.7161\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2184 - acc: 0.9124 - val_loss: 0.6328 - val_acc: 0.7806\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2170 - acc: 0.9202 - val_loss: 0.6082 - val_acc: 0.7613\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2512 - acc: 0.9138 - val_loss: 0.6246 - val_acc: 0.7806\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.1970 - acc: 0.9252 - val_loss: 0.7645 - val_acc: 0.7935\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.1938 - acc: 0.9349 - val_loss: 0.5615 - val_acc: 0.7677\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.1867 - acc: 0.9353 - val_loss: 0.6057 - val_acc: 0.7742\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1601 - acc: 0.9417 - val_loss: 0.6326 - val_acc: 0.7806\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1755 - acc: 0.9349 - val_loss: 0.7482 - val_acc: 0.7419\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1679 - acc: 0.9408 - val_loss: 0.5152 - val_acc: 0.7871\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1760 - acc: 0.9381 - val_loss: 0.7167 - val_acc: 0.8065\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1387 - acc: 0.9550 - val_loss: 0.6580 - val_acc: 0.8000\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1696 - acc: 0.9399 - val_loss: 0.6193 - val_acc: 0.7935\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1610 - acc: 0.9417 - val_loss: 0.8956 - val_acc: 0.7355\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1499 - acc: 0.9532 - val_loss: 0.5656 - val_acc: 0.8194\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1228 - acc: 0.9583 - val_loss: 0.8218 - val_acc: 0.7419\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1399 - acc: 0.9532 - val_loss: 0.6346 - val_acc: 0.7742\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1546 - acc: 0.9417 - val_loss: 1.0651 - val_acc: 0.6968\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1368 - acc: 0.9528 - val_loss: 0.5848 - val_acc: 0.7871\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1108 - acc: 0.9665 - val_loss: 0.6207 - val_acc: 0.8065\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1210 - acc: 0.9619 - val_loss: 0.7608 - val_acc: 0.7548\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1118 - acc: 0.9665 - val_loss: 0.7091 - val_acc: 0.7742\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.0937 - acc: 0.9739 - val_loss: 0.7217 - val_acc: 0.7742\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1385 - acc: 0.9500 - val_loss: 0.8501 - val_acc: 0.7871\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1057 - acc: 0.9656 - val_loss: 0.7666 - val_acc: 0.8129\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.0779 - acc: 0.9798 - val_loss: 0.7100 - val_acc: 0.7677\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0996 - acc: 0.9647 - val_loss: 0.7842 - val_acc: 0.8129\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0682 - acc: 0.9839 - val_loss: 0.6959 - val_acc: 0.8129\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0754 - acc: 0.9784 - val_loss: 0.6952 - val_acc: 0.7871\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0693 - acc: 0.9789 - val_loss: 0.8963 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0478 - acc: 0.9936 - val_loss: 0.7252 - val_acc: 0.8000\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0542 - acc: 0.9899 - val_loss: 0.7043 - val_acc: 0.8129\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0499 - acc: 0.9904 - val_loss: 0.6795 - val_acc: 0.8323\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0421 - acc: 0.9950 - val_loss: 0.6814 - val_acc: 0.8323\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0413 - acc: 0.9927 - val_loss: 0.6918 - val_acc: 0.8194\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0396 - acc: 0.9940 - val_loss: 0.7002 - val_acc: 0.8129\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0386 - acc: 0.9959 - val_loss: 0.6950 - val_acc: 0.8129\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0391 - acc: 0.9954 - val_loss: 0.6765 - val_acc: 0.8323\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0379 - acc: 0.9959 - val_loss: 0.6854 - val_acc: 0.8323\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0351 - acc: 0.9959 - val_loss: 0.7061 - val_acc: 0.8258\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0385 - acc: 0.9936 - val_loss: 0.6950 - val_acc: 0.8258\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0346 - acc: 0.9954 - val_loss: 0.7172 - val_acc: 0.8258\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0337 - acc: 0.9954 - val_loss: 0.6785 - val_acc: 0.8258\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.0290 - acc: 0.9982 - val_loss: 0.6894 - val_acc: 0.8387\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.0356 - acc: 0.9954 - val_loss: 0.6977 - val_acc: 0.8194\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0335 - acc: 0.9959 - val_loss: 0.7066 - val_acc: 0.8387\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0341 - acc: 0.9954 - val_loss: 0.7088 - val_acc: 0.8258\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0351 - acc: 0.9959 - val_loss: 0.7059 - val_acc: 0.8258\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0348 - acc: 0.9954 - val_loss: 0.6931 - val_acc: 0.8258\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0338 - acc: 0.9950 - val_loss: 0.7410 - val_acc: 0.8258\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0349 - acc: 0.9963 - val_loss: 0.7302 - val_acc: 0.8194\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0365 - acc: 0.9927 - val_loss: 0.7336 - val_acc: 0.8258\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0340 - acc: 0.9963 - val_loss: 0.7092 - val_acc: 0.8387\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0281 - acc: 0.9963 - val_loss: 0.7213 - val_acc: 0.8129\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0318 - acc: 0.9963 - val_loss: 0.7197 - val_acc: 0.8129\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0331 - acc: 0.9963 - val_loss: 0.7567 - val_acc: 0.8258\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0288 - acc: 0.9991 - val_loss: 0.7185 - val_acc: 0.8129\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0274 - acc: 0.9986 - val_loss: 0.7166 - val_acc: 0.8129\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0300 - acc: 0.9968 - val_loss: 0.7056 - val_acc: 0.8065\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0273 - acc: 0.9977 - val_loss: 0.6923 - val_acc: 0.8194\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0315 - acc: 0.9963 - val_loss: 0.6919 - val_acc: 0.8258\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0341 - acc: 0.9945 - val_loss: 0.7422 - val_acc: 0.8258\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0295 - acc: 0.9972 - val_loss: 0.7141 - val_acc: 0.8258\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0263 - acc: 0.9982 - val_loss: 0.7319 - val_acc: 0.8387\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0254 - acc: 0.9982 - val_loss: 0.7232 - val_acc: 0.8258\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0258 - acc: 0.9986 - val_loss: 0.7146 - val_acc: 0.8258\n",
      "Epoch 88/1000\n",
      "69/69 - 0s - loss: 0.0294 - acc: 0.9959 - val_loss: 0.7452 - val_acc: 0.8258\n",
      "Epoch 89/1000\n",
      "69/69 - 0s - loss: 0.0289 - acc: 0.9963 - val_loss: 0.7260 - val_acc: 0.8258\n",
      "Epoch 90/1000\n",
      "69/69 - 0s - loss: 0.0222 - acc: 0.9986 - val_loss: 0.7341 - val_acc: 0.8323\n",
      "Epoch 91/1000\n",
      "69/69 - 0s - loss: 0.0252 - acc: 0.9986 - val_loss: 0.6968 - val_acc: 0.8323\n",
      "Epoch 92/1000\n",
      "69/69 - 0s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.7459 - val_acc: 0.8065\n",
      "Epoch 93/1000\n",
      "69/69 - 0s - loss: 0.0259 - acc: 0.9977 - val_loss: 0.7006 - val_acc: 0.8258\n",
      "Epoch 94/1000\n",
      "69/69 - 0s - loss: 0.0306 - acc: 0.9963 - val_loss: 0.7254 - val_acc: 0.8323\n",
      "Epoch 95/1000\n",
      "69/69 - 0s - loss: 0.0265 - acc: 0.9982 - val_loss: 0.7244 - val_acc: 0.8387\n",
      "Epoch 96/1000\n",
      "69/69 - 0s - loss: 0.0241 - acc: 0.9972 - val_loss: 0.7453 - val_acc: 0.8323\n",
      "Epoch 97/1000\n",
      "69/69 - 0s - loss: 0.0262 - acc: 0.9982 - val_loss: 0.7540 - val_acc: 0.8323\n",
      "Epoch 98/1000\n",
      "69/69 - 0s - loss: 0.0237 - acc: 0.9977 - val_loss: 0.7530 - val_acc: 0.8323\n",
      "Epoch 99/1000\n",
      "69/69 - 0s - loss: 0.0249 - acc: 0.9972 - val_loss: 0.7220 - val_acc: 0.8129\n",
      "Epoch 100/1000\n",
      "69/69 - 0s - loss: 0.0242 - acc: 0.9968 - val_loss: 0.7502 - val_acc: 0.8258\n",
      "Epoch 101/1000\n",
      "69/69 - 0s - loss: 0.0272 - acc: 0.9977 - val_loss: 0.7439 - val_acc: 0.8258\n",
      "Epoch 102/1000\n",
      "69/69 - 0s - loss: 0.0232 - acc: 0.9977 - val_loss: 0.7484 - val_acc: 0.8258\n",
      "Epoch 103/1000\n",
      "69/69 - 0s - loss: 0.0214 - acc: 0.9982 - val_loss: 0.7491 - val_acc: 0.8258\n",
      "Epoch 104/1000\n",
      "69/69 - 0s - loss: 0.0239 - acc: 0.9959 - val_loss: 0.7452 - val_acc: 0.8387\n",
      "Epoch 105/1000\n",
      "69/69 - 0s - loss: 0.0239 - acc: 0.9977 - val_loss: 0.7916 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 106/1000\n",
      "69/69 - 0s - loss: 0.0218 - acc: 0.9991 - val_loss: 0.7760 - val_acc: 0.8258\n",
      "Epoch 107/1000\n",
      "69/69 - 0s - loss: 0.0209 - acc: 0.9982 - val_loss: 0.7694 - val_acc: 0.8258\n",
      "Epoch 108/1000\n",
      "69/69 - 0s - loss: 0.0190 - acc: 0.9982 - val_loss: 0.7586 - val_acc: 0.8323\n",
      "Epoch 109/1000\n",
      "69/69 - 0s - loss: 0.0273 - acc: 0.9963 - val_loss: 0.7556 - val_acc: 0.8258\n",
      "Epoch 110/1000\n",
      "69/69 - 0s - loss: 0.0223 - acc: 0.9991 - val_loss: 0.7552 - val_acc: 0.8323\n",
      "Epoch 111/1000\n",
      "69/69 - 0s - loss: 0.0212 - acc: 0.9982 - val_loss: 0.7571 - val_acc: 0.8387\n",
      "Epoch 112/1000\n",
      "69/69 - 0s - loss: 0.0195 - acc: 0.9995 - val_loss: 0.7520 - val_acc: 0.8323\n",
      "Epoch 113/1000\n",
      "69/69 - 0s - loss: 0.0196 - acc: 0.9986 - val_loss: 0.7490 - val_acc: 0.8323\n",
      "Epoch 114/1000\n",
      "69/69 - 0s - loss: 0.0193 - acc: 0.9968 - val_loss: 0.7467 - val_acc: 0.8323\n",
      "Epoch 115/1000\n",
      "69/69 - 0s - loss: 0.0217 - acc: 0.9991 - val_loss: 0.7478 - val_acc: 0.8258\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 1.0054 - acc: 0.5844 - val_loss: 1.4146 - val_acc: 0.2581\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.6197 - acc: 0.7720 - val_loss: 1.4590 - val_acc: 0.2516\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.5232 - acc: 0.8115 - val_loss: 1.8835 - val_acc: 0.2516\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4551 - acc: 0.8376 - val_loss: 2.0132 - val_acc: 0.2452\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4303 - acc: 0.8394 - val_loss: 2.5246 - val_acc: 0.2452\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.4187 - acc: 0.8463 - val_loss: 2.8024 - val_acc: 0.3226\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3923 - acc: 0.8550 - val_loss: 2.9461 - val_acc: 0.2774\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3722 - acc: 0.8628 - val_loss: 1.0054 - val_acc: 0.6194\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3686 - acc: 0.8638 - val_loss: 1.7419 - val_acc: 0.5097\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3482 - acc: 0.8697 - val_loss: 0.7553 - val_acc: 0.6645\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3385 - acc: 0.8702 - val_loss: 0.6662 - val_acc: 0.7419\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3277 - acc: 0.8784 - val_loss: 0.2695 - val_acc: 0.9226\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.3134 - acc: 0.8853 - val_loss: 0.8482 - val_acc: 0.6774\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.2950 - acc: 0.8899 - val_loss: 0.3277 - val_acc: 0.8710\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.2958 - acc: 0.8876 - val_loss: 0.3045 - val_acc: 0.8839\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2890 - acc: 0.8995 - val_loss: 0.2891 - val_acc: 0.8710\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2846 - acc: 0.8972 - val_loss: 0.5426 - val_acc: 0.7677\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2756 - acc: 0.8995 - val_loss: 0.9055 - val_acc: 0.6774\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2635 - acc: 0.9078 - val_loss: 0.4016 - val_acc: 0.8387\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2525 - acc: 0.9106 - val_loss: 0.5109 - val_acc: 0.7935\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2501 - acc: 0.9055 - val_loss: 0.3210 - val_acc: 0.8710\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2562 - acc: 0.9069 - val_loss: 0.2773 - val_acc: 0.8968\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2436 - acc: 0.9060 - val_loss: 0.4411 - val_acc: 0.8258\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2242 - acc: 0.9188 - val_loss: 0.3654 - val_acc: 0.8258\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.2092 - acc: 0.9257 - val_loss: 0.4008 - val_acc: 0.8258\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.2004 - acc: 0.9307 - val_loss: 0.3186 - val_acc: 0.8581\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.2072 - acc: 0.9197 - val_loss: 0.4010 - val_acc: 0.8194\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.2087 - acc: 0.9266 - val_loss: 0.7531 - val_acc: 0.7484\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.1885 - acc: 0.9294 - val_loss: 0.3654 - val_acc: 0.8581\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1980 - acc: 0.9317 - val_loss: 0.3771 - val_acc: 0.8774\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1693 - acc: 0.9367 - val_loss: 0.5785 - val_acc: 0.7806\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1685 - acc: 0.9454 - val_loss: 0.5601 - val_acc: 0.7935\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1558 - acc: 0.9495 - val_loss: 0.3323 - val_acc: 0.8839\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1475 - acc: 0.9505 - val_loss: 0.3613 - val_acc: 0.8581\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1714 - acc: 0.9417 - val_loss: 0.4718 - val_acc: 0.8258\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1802 - acc: 0.9321 - val_loss: 0.3517 - val_acc: 0.8645\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1688 - acc: 0.9381 - val_loss: 0.2356 - val_acc: 0.9032\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1390 - acc: 0.9541 - val_loss: 0.4598 - val_acc: 0.8258\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1235 - acc: 0.9578 - val_loss: 0.4323 - val_acc: 0.8387\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1246 - acc: 0.9615 - val_loss: 0.7499 - val_acc: 0.7677\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1208 - acc: 0.9619 - val_loss: 0.4118 - val_acc: 0.8452\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1120 - acc: 0.9651 - val_loss: 0.4309 - val_acc: 0.8258\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1083 - acc: 0.9661 - val_loss: 0.3591 - val_acc: 0.8710\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1328 - acc: 0.9555 - val_loss: 0.4899 - val_acc: 0.7806\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1538 - acc: 0.9422 - val_loss: 0.6903 - val_acc: 0.7677\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1291 - acc: 0.9642 - val_loss: 0.3317 - val_acc: 0.8710\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.1005 - acc: 0.9683 - val_loss: 0.4124 - val_acc: 0.8452\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0852 - acc: 0.9743 - val_loss: 0.2939 - val_acc: 0.8903\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0926 - acc: 0.9739 - val_loss: 0.2529 - val_acc: 0.8968\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.0800 - acc: 0.9789 - val_loss: 0.4946 - val_acc: 0.8323\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0729 - acc: 0.9817 - val_loss: 0.3007 - val_acc: 0.9161\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.0656 - acc: 0.9858 - val_loss: 0.3390 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0476 - acc: 0.9913 - val_loss: 0.3591 - val_acc: 0.8516\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0430 - acc: 0.9945 - val_loss: 0.3327 - val_acc: 0.8968\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0392 - acc: 0.9936 - val_loss: 0.3213 - val_acc: 0.8903\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0406 - acc: 0.9950 - val_loss: 0.2934 - val_acc: 0.8968\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0433 - acc: 0.9917 - val_loss: 0.3018 - val_acc: 0.8903\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0392 - acc: 0.9936 - val_loss: 0.3158 - val_acc: 0.8839\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0349 - acc: 0.9968 - val_loss: 0.3311 - val_acc: 0.8903\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0402 - acc: 0.9945 - val_loss: 0.3260 - val_acc: 0.8774\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0372 - acc: 0.9950 - val_loss: 0.3111 - val_acc: 0.8903\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0362 - acc: 0.9963 - val_loss: 0.2996 - val_acc: 0.8968\n",
      "Epoch 1/1000\n",
      "69/69 - 0s - loss: 0.9375 - acc: 0.6330 - val_loss: 1.3999 - val_acc: 0.2452\n",
      "Epoch 2/1000\n",
      "69/69 - 0s - loss: 0.5863 - acc: 0.7904 - val_loss: 1.3980 - val_acc: 0.2452\n",
      "Epoch 3/1000\n",
      "69/69 - 0s - loss: 0.4743 - acc: 0.8239 - val_loss: 1.7199 - val_acc: 0.2452\n",
      "Epoch 4/1000\n",
      "69/69 - 0s - loss: 0.4405 - acc: 0.8372 - val_loss: 2.3993 - val_acc: 0.2452\n",
      "Epoch 5/1000\n",
      "69/69 - 0s - loss: 0.4028 - acc: 0.8518 - val_loss: 2.0482 - val_acc: 0.2645\n",
      "Epoch 6/1000\n",
      "69/69 - 0s - loss: 0.3964 - acc: 0.8495 - val_loss: 2.0114 - val_acc: 0.2710\n",
      "Epoch 7/1000\n",
      "69/69 - 0s - loss: 0.3471 - acc: 0.8739 - val_loss: 1.3823 - val_acc: 0.4645\n",
      "Epoch 8/1000\n",
      "69/69 - 0s - loss: 0.3563 - acc: 0.8656 - val_loss: 1.3951 - val_acc: 0.5097\n",
      "Epoch 9/1000\n",
      "69/69 - 0s - loss: 0.3260 - acc: 0.8784 - val_loss: 1.1833 - val_acc: 0.5613\n",
      "Epoch 10/1000\n",
      "69/69 - 0s - loss: 0.3346 - acc: 0.8757 - val_loss: 0.4237 - val_acc: 0.8323\n",
      "Epoch 11/1000\n",
      "69/69 - 0s - loss: 0.3144 - acc: 0.8867 - val_loss: 0.4761 - val_acc: 0.8129\n",
      "Epoch 12/1000\n",
      "69/69 - 0s - loss: 0.3035 - acc: 0.8849 - val_loss: 0.3699 - val_acc: 0.8645\n",
      "Epoch 13/1000\n",
      "69/69 - 0s - loss: 0.2901 - acc: 0.9005 - val_loss: 0.3848 - val_acc: 0.8452\n",
      "Epoch 14/1000\n",
      "69/69 - 0s - loss: 0.2984 - acc: 0.8862 - val_loss: 0.4575 - val_acc: 0.8065\n",
      "Epoch 15/1000\n",
      "69/69 - 0s - loss: 0.3028 - acc: 0.8917 - val_loss: 0.4081 - val_acc: 0.8516\n",
      "Epoch 16/1000\n",
      "69/69 - 0s - loss: 0.2726 - acc: 0.8950 - val_loss: 0.5103 - val_acc: 0.7935\n",
      "Epoch 17/1000\n",
      "69/69 - 0s - loss: 0.2645 - acc: 0.9009 - val_loss: 0.7219 - val_acc: 0.7419\n",
      "Epoch 18/1000\n",
      "69/69 - 0s - loss: 0.2417 - acc: 0.9193 - val_loss: 0.3940 - val_acc: 0.8516\n",
      "Epoch 19/1000\n",
      "69/69 - 0s - loss: 0.2365 - acc: 0.9202 - val_loss: 0.4596 - val_acc: 0.8323\n",
      "Epoch 20/1000\n",
      "69/69 - 0s - loss: 0.2471 - acc: 0.9092 - val_loss: 0.5065 - val_acc: 0.8323\n",
      "Epoch 21/1000\n",
      "69/69 - 0s - loss: 0.2630 - acc: 0.9023 - val_loss: 0.5452 - val_acc: 0.8194\n",
      "Epoch 22/1000\n",
      "69/69 - 0s - loss: 0.2274 - acc: 0.9193 - val_loss: 0.5463 - val_acc: 0.8129\n",
      "Epoch 23/1000\n",
      "69/69 - 0s - loss: 0.2352 - acc: 0.9161 - val_loss: 0.4030 - val_acc: 0.8452\n",
      "Epoch 24/1000\n",
      "69/69 - 0s - loss: 0.2035 - acc: 0.9298 - val_loss: 0.3643 - val_acc: 0.8645\n",
      "Epoch 25/1000\n",
      "69/69 - 0s - loss: 0.1923 - acc: 0.9335 - val_loss: 0.3646 - val_acc: 0.8774\n",
      "Epoch 26/1000\n",
      "69/69 - 0s - loss: 0.1995 - acc: 0.9312 - val_loss: 0.5275 - val_acc: 0.7871\n",
      "Epoch 27/1000\n",
      "69/69 - 0s - loss: 0.1818 - acc: 0.9330 - val_loss: 0.4454 - val_acc: 0.8323\n",
      "Epoch 28/1000\n",
      "69/69 - 0s - loss: 0.1912 - acc: 0.9339 - val_loss: 0.3998 - val_acc: 0.8774\n",
      "Epoch 29/1000\n",
      "69/69 - 0s - loss: 0.2017 - acc: 0.9289 - val_loss: 0.4330 - val_acc: 0.8516\n",
      "Epoch 30/1000\n",
      "69/69 - 0s - loss: 0.1995 - acc: 0.9294 - val_loss: 0.4383 - val_acc: 0.8581\n",
      "Epoch 31/1000\n",
      "69/69 - 0s - loss: 0.1600 - acc: 0.9468 - val_loss: 0.3331 - val_acc: 0.8516\n",
      "Epoch 32/1000\n",
      "69/69 - 0s - loss: 0.1603 - acc: 0.9459 - val_loss: 0.5074 - val_acc: 0.8129\n",
      "Epoch 33/1000\n",
      "69/69 - 0s - loss: 0.1504 - acc: 0.9495 - val_loss: 0.3699 - val_acc: 0.8645\n",
      "Epoch 34/1000\n",
      "69/69 - 0s - loss: 0.1662 - acc: 0.9394 - val_loss: 0.4035 - val_acc: 0.8581\n",
      "Epoch 35/1000\n",
      "69/69 - 0s - loss: 0.1680 - acc: 0.9390 - val_loss: 0.4554 - val_acc: 0.8387\n",
      "Epoch 36/1000\n",
      "69/69 - 0s - loss: 0.1334 - acc: 0.9583 - val_loss: 0.4188 - val_acc: 0.8516\n",
      "Epoch 37/1000\n",
      "69/69 - 0s - loss: 0.1439 - acc: 0.9546 - val_loss: 0.3519 - val_acc: 0.8839\n",
      "Epoch 38/1000\n",
      "69/69 - 0s - loss: 0.1265 - acc: 0.9573 - val_loss: 0.3578 - val_acc: 0.8452\n",
      "Epoch 39/1000\n",
      "69/69 - 0s - loss: 0.1166 - acc: 0.9633 - val_loss: 0.4771 - val_acc: 0.8194\n",
      "Epoch 40/1000\n",
      "69/69 - 0s - loss: 0.1062 - acc: 0.9661 - val_loss: 0.4596 - val_acc: 0.8194\n",
      "Epoch 41/1000\n",
      "69/69 - 0s - loss: 0.1091 - acc: 0.9624 - val_loss: 0.4461 - val_acc: 0.8581\n",
      "Epoch 42/1000\n",
      "69/69 - 0s - loss: 0.1183 - acc: 0.9601 - val_loss: 0.4442 - val_acc: 0.8645\n",
      "Epoch 43/1000\n",
      "69/69 - 0s - loss: 0.1397 - acc: 0.9486 - val_loss: 0.6315 - val_acc: 0.8065\n",
      "Epoch 44/1000\n",
      "69/69 - 0s - loss: 0.1339 - acc: 0.9518 - val_loss: 0.3708 - val_acc: 0.8581\n",
      "Epoch 45/1000\n",
      "69/69 - 0s - loss: 0.1496 - acc: 0.9486 - val_loss: 0.4471 - val_acc: 0.8516\n",
      "Epoch 46/1000\n",
      "69/69 - 0s - loss: 0.1035 - acc: 0.9683 - val_loss: 0.4631 - val_acc: 0.8387\n",
      "Epoch 47/1000\n",
      "69/69 - 0s - loss: 0.0959 - acc: 0.9702 - val_loss: 0.4133 - val_acc: 0.8581\n",
      "Epoch 48/1000\n",
      "69/69 - 0s - loss: 0.0735 - acc: 0.9794 - val_loss: 0.4120 - val_acc: 0.8452\n",
      "Epoch 49/1000\n",
      "69/69 - 0s - loss: 0.0917 - acc: 0.9711 - val_loss: 0.3523 - val_acc: 0.8387\n",
      "Epoch 50/1000\n",
      "69/69 - 0s - loss: 0.1087 - acc: 0.9596 - val_loss: 0.4044 - val_acc: 0.8581\n",
      "Epoch 51/1000\n",
      "69/69 - 0s - loss: 0.0926 - acc: 0.9716 - val_loss: 0.4529 - val_acc: 0.8323\n",
      "Epoch 52/1000\n",
      "69/69 - 0s - loss: 0.1024 - acc: 0.9683 - val_loss: 0.2989 - val_acc: 0.8710\n",
      "Epoch 53/1000\n",
      "69/69 - 0s - loss: 0.0788 - acc: 0.9752 - val_loss: 0.4639 - val_acc: 0.8323\n",
      "Epoch 54/1000\n",
      "69/69 - 0s - loss: 0.0862 - acc: 0.9729 - val_loss: 0.4204 - val_acc: 0.8710\n",
      "Epoch 55/1000\n",
      "69/69 - 0s - loss: 0.0742 - acc: 0.9798 - val_loss: 0.4043 - val_acc: 0.8452\n",
      "Epoch 56/1000\n",
      "69/69 - 0s - loss: 0.0618 - acc: 0.9862 - val_loss: 0.4194 - val_acc: 0.8516\n",
      "Epoch 57/1000\n",
      "69/69 - 0s - loss: 0.0494 - acc: 0.9876 - val_loss: 0.7694 - val_acc: 0.7935\n",
      "Epoch 58/1000\n",
      "69/69 - 0s - loss: 0.0658 - acc: 0.9830 - val_loss: 0.4510 - val_acc: 0.8516\n",
      "Epoch 59/1000\n",
      "69/69 - 0s - loss: 0.0542 - acc: 0.9881 - val_loss: 0.4469 - val_acc: 0.8581\n",
      "Epoch 60/1000\n",
      "69/69 - 0s - loss: 0.0751 - acc: 0.9812 - val_loss: 0.4712 - val_acc: 0.8516\n",
      "Epoch 61/1000\n",
      "69/69 - 0s - loss: 0.0456 - acc: 0.9899 - val_loss: 0.5362 - val_acc: 0.8387\n",
      "Epoch 62/1000\n",
      "69/69 - 0s - loss: 0.0516 - acc: 0.9876 - val_loss: 0.5211 - val_acc: 0.8258\n",
      "Epoch 63/1000\n",
      "69/69 - 0s - loss: 0.0445 - acc: 0.9890 - val_loss: 0.4131 - val_acc: 0.8516\n",
      "Epoch 64/1000\n",
      "69/69 - 0s - loss: 0.0880 - acc: 0.9679 - val_loss: 0.6216 - val_acc: 0.8129\n",
      "Epoch 65/1000\n",
      "69/69 - 0s - loss: 0.1080 - acc: 0.9596 - val_loss: 0.5337 - val_acc: 0.8129\n",
      "Epoch 66/1000\n",
      "69/69 - 0s - loss: 0.1165 - acc: 0.9569 - val_loss: 0.3663 - val_acc: 0.8710\n",
      "Epoch 67/1000\n",
      "69/69 - 0s - loss: 0.0738 - acc: 0.9789 - val_loss: 0.4519 - val_acc: 0.8452\n",
      "Epoch 68/1000\n",
      "69/69 - 0s - loss: 0.0772 - acc: 0.9729 - val_loss: 0.3681 - val_acc: 0.8710\n",
      "Epoch 69/1000\n",
      "69/69 - 0s - loss: 0.0744 - acc: 0.9771 - val_loss: 0.4305 - val_acc: 0.8516\n",
      "Epoch 70/1000\n",
      "69/69 - 0s - loss: 0.0903 - acc: 0.9706 - val_loss: 0.5965 - val_acc: 0.8065\n",
      "Epoch 71/1000\n",
      "69/69 - 0s - loss: 0.0554 - acc: 0.9844 - val_loss: 0.4698 - val_acc: 0.8452\n",
      "Epoch 72/1000\n",
      "69/69 - 0s - loss: 0.0401 - acc: 0.9931 - val_loss: 0.4262 - val_acc: 0.8323\n",
      "Epoch 73/1000\n",
      "69/69 - 0s - loss: 0.0315 - acc: 0.9954 - val_loss: 0.5540 - val_acc: 0.8194\n",
      "Epoch 74/1000\n",
      "69/69 - 0s - loss: 0.0270 - acc: 0.9945 - val_loss: 0.4356 - val_acc: 0.8452\n",
      "Epoch 75/1000\n",
      "69/69 - 0s - loss: 0.0278 - acc: 0.9945 - val_loss: 0.4799 - val_acc: 0.8387\n",
      "Epoch 76/1000\n",
      "69/69 - 0s - loss: 0.0512 - acc: 0.9839 - val_loss: 1.2384 - val_acc: 0.7290\n",
      "Epoch 77/1000\n",
      "69/69 - 0s - loss: 0.0329 - acc: 0.9927 - val_loss: 0.4668 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 78/1000\n",
      "69/69 - 0s - loss: 0.0348 - acc: 0.9908 - val_loss: 0.4509 - val_acc: 0.8452\n",
      "Epoch 79/1000\n",
      "69/69 - 0s - loss: 0.0186 - acc: 0.9991 - val_loss: 0.4366 - val_acc: 0.8516\n",
      "Epoch 80/1000\n",
      "69/69 - 0s - loss: 0.0178 - acc: 0.9986 - val_loss: 0.4567 - val_acc: 0.8516\n",
      "Epoch 81/1000\n",
      "69/69 - 0s - loss: 0.0194 - acc: 0.9977 - val_loss: 0.4410 - val_acc: 0.8581\n",
      "Epoch 82/1000\n",
      "69/69 - 0s - loss: 0.0184 - acc: 0.9986 - val_loss: 0.4299 - val_acc: 0.8516\n",
      "Epoch 83/1000\n",
      "69/69 - 0s - loss: 0.0179 - acc: 0.9986 - val_loss: 0.4425 - val_acc: 0.8323\n",
      "Epoch 84/1000\n",
      "69/69 - 0s - loss: 0.0159 - acc: 0.9986 - val_loss: 0.4370 - val_acc: 0.8452\n",
      "Epoch 85/1000\n",
      "69/69 - 0s - loss: 0.0180 - acc: 0.9986 - val_loss: 0.4475 - val_acc: 0.8452\n",
      "Epoch 86/1000\n",
      "69/69 - 0s - loss: 0.0195 - acc: 0.9986 - val_loss: 0.4407 - val_acc: 0.8387\n",
      "Epoch 87/1000\n",
      "69/69 - 0s - loss: 0.0182 - acc: 0.9982 - val_loss: 0.4466 - val_acc: 0.8516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3657322188218434"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = 0\n",
    "result = 0\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 82, shuffle = True)\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "for train_index, valid_index in skf.split(train2,train[\"target\"]):\n",
    "    X_train, X_valid = train2[train_index], train2[valid_index]\n",
    "    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[valid_index]\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(2,2),activation = \"relu\", input_shape = (8,4,1), padding = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(2,2),activation = \"relu\", padding = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128,(2,2),activation = \"relu\", padding = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(4,activation = \"softmax\"))\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = \"acc\")\n",
    "    es = EarlyStopping(patience = 50, restore_best_weights = True, monitor = \"val_acc\", mode = \"max\")\n",
    "    rl = ReduceLROnPlateau(patience = 40, verbose = 1, monitor = \"val_acc\", mode = \"max\")\n",
    "    history = model.fit(X_train,y_train, validation_data = (X_valid,y_valid), epochs = 1000, callbacks = [es,rl], verbose = 2)\n",
    "    best_score += np.min(history.history[\"val_loss\"]) / 10\n",
    "    result += model.predict(test2) / 10\n",
    "best_score\n",
    "#0.36387131214141843 val_acc에 초점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e588442-c18f-4f91-a35c-cbe78f946b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 1.0411 - acc: 0.5697 - val_loss: 1.4311 - val_acc: 0.2521\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.6581 - acc: 0.7696 - val_loss: 1.6243 - val_acc: 0.2436\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4871 - acc: 0.8239 - val_loss: 2.1108 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4408 - acc: 0.8439 - val_loss: 3.4982 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4025 - acc: 0.8510 - val_loss: 5.7744 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3806 - acc: 0.8596 - val_loss: 6.2042 - val_acc: 0.2436\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3604 - acc: 0.8734 - val_loss: 4.9778 - val_acc: 0.2564\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3444 - acc: 0.8734 - val_loss: 2.7145 - val_acc: 0.4744\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3399 - acc: 0.8767 - val_loss: 4.3254 - val_acc: 0.3803\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.2958 - acc: 0.8934 - val_loss: 1.6883 - val_acc: 0.6239\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.2807 - acc: 0.8967 - val_loss: 0.8164 - val_acc: 0.7222\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.2577 - acc: 0.9077 - val_loss: 0.4727 - val_acc: 0.8248\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.2539 - acc: 0.9072 - val_loss: 0.4368 - val_acc: 0.8632\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2498 - acc: 0.9153 - val_loss: 0.4545 - val_acc: 0.8419\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2519 - acc: 0.9067 - val_loss: 0.3742 - val_acc: 0.8846\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2495 - acc: 0.9105 - val_loss: 0.4540 - val_acc: 0.8590\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2520 - acc: 0.9110 - val_loss: 0.4068 - val_acc: 0.8761\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2423 - acc: 0.9143 - val_loss: 0.3933 - val_acc: 0.8846\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2523 - acc: 0.9129 - val_loss: 0.3838 - val_acc: 0.8846\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2359 - acc: 0.9158 - val_loss: 0.4316 - val_acc: 0.8675\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2366 - acc: 0.9191 - val_loss: 0.4153 - val_acc: 0.8846\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2286 - acc: 0.9219 - val_loss: 0.3911 - val_acc: 0.8761\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2351 - acc: 0.9210 - val_loss: 0.4254 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2248 - acc: 0.9229 - val_loss: 0.4024 - val_acc: 0.8761\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.2225 - acc: 0.9262 - val_loss: 0.3970 - val_acc: 0.8718\n",
      "Epoch 1/1000\n",
      "66/66 - 1s - loss: 1.0018 - acc: 0.5802 - val_loss: 1.4486 - val_acc: 0.2521\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.6255 - acc: 0.7725 - val_loss: 1.8345 - val_acc: 0.2521\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.5140 - acc: 0.8115 - val_loss: 2.4753 - val_acc: 0.2436\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4919 - acc: 0.8210 - val_loss: 4.9149 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4252 - acc: 0.8477 - val_loss: 6.0373 - val_acc: 0.2436\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.4167 - acc: 0.8482 - val_loss: 6.7776 - val_acc: 0.2436\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3868 - acc: 0.8505 - val_loss: 5.5769 - val_acc: 0.2479\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3914 - acc: 0.8596 - val_loss: 2.2281 - val_acc: 0.3504\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3642 - acc: 0.8648 - val_loss: 1.1346 - val_acc: 0.5214\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3457 - acc: 0.8724 - val_loss: 0.8350 - val_acc: 0.7009\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3223 - acc: 0.8848 - val_loss: 0.9438 - val_acc: 0.6068\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.3233 - acc: 0.8777 - val_loss: 2.4277 - val_acc: 0.4145\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.3193 - acc: 0.8820 - val_loss: 1.6889 - val_acc: 0.5897\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2993 - acc: 0.8958 - val_loss: 0.7399 - val_acc: 0.7393\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.3051 - acc: 0.8943 - val_loss: 0.4816 - val_acc: 0.8120\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2767 - acc: 0.9000 - val_loss: 0.7958 - val_acc: 0.7051\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2758 - acc: 0.8991 - val_loss: 0.5926 - val_acc: 0.7735\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2566 - acc: 0.9077 - val_loss: 0.4240 - val_acc: 0.8590\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2503 - acc: 0.9139 - val_loss: 0.4286 - val_acc: 0.8291\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2478 - acc: 0.9067 - val_loss: 0.5171 - val_acc: 0.8291\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2353 - acc: 0.9224 - val_loss: 0.5056 - val_acc: 0.8205\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2187 - acc: 0.9200 - val_loss: 0.4909 - val_acc: 0.8077\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2175 - acc: 0.9272 - val_loss: 0.5058 - val_acc: 0.8248\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2206 - acc: 0.9158 - val_loss: 0.6785 - val_acc: 0.7692\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.2061 - acc: 0.9243 - val_loss: 0.5274 - val_acc: 0.8120\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.1904 - acc: 0.9329 - val_loss: 0.3854 - val_acc: 0.8333\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1766 - acc: 0.9405 - val_loss: 0.3817 - val_acc: 0.8590\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1766 - acc: 0.9372 - val_loss: 0.4196 - val_acc: 0.8590\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1546 - acc: 0.9434 - val_loss: 0.7465 - val_acc: 0.7863\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1599 - acc: 0.9419 - val_loss: 0.5788 - val_acc: 0.8376\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1425 - acc: 0.9524 - val_loss: 0.3878 - val_acc: 0.8462\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1485 - acc: 0.9491 - val_loss: 0.6362 - val_acc: 0.7863\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.1481 - acc: 0.9467 - val_loss: 0.4892 - val_acc: 0.8162\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.1382 - acc: 0.9538 - val_loss: 0.4068 - val_acc: 0.8632\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.1226 - acc: 0.9634 - val_loss: 0.4759 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 36/1000\n",
      "66/66 - 0s - loss: 0.0923 - acc: 0.9762 - val_loss: 0.3756 - val_acc: 0.8632\n",
      "Epoch 37/1000\n",
      "66/66 - 0s - loss: 0.0772 - acc: 0.9810 - val_loss: 0.3823 - val_acc: 0.8419\n",
      "Epoch 38/1000\n",
      "66/66 - 0s - loss: 0.0687 - acc: 0.9881 - val_loss: 0.3593 - val_acc: 0.8718\n",
      "Epoch 39/1000\n",
      "66/66 - 0s - loss: 0.0735 - acc: 0.9819 - val_loss: 0.3719 - val_acc: 0.8803\n",
      "Epoch 40/1000\n",
      "66/66 - 0s - loss: 0.0655 - acc: 0.9867 - val_loss: 0.3740 - val_acc: 0.8718\n",
      "Epoch 41/1000\n",
      "66/66 - 0s - loss: 0.0628 - acc: 0.9881 - val_loss: 0.3654 - val_acc: 0.8846\n",
      "Epoch 42/1000\n",
      "66/66 - 0s - loss: 0.0669 - acc: 0.9871 - val_loss: 0.3767 - val_acc: 0.8718\n",
      "Epoch 43/1000\n",
      "66/66 - 0s - loss: 0.0598 - acc: 0.9900 - val_loss: 0.3542 - val_acc: 0.8761\n",
      "Epoch 44/1000\n",
      "66/66 - 0s - loss: 0.0640 - acc: 0.9886 - val_loss: 0.3622 - val_acc: 0.8718\n",
      "Epoch 45/1000\n",
      "66/66 - 0s - loss: 0.0611 - acc: 0.9881 - val_loss: 0.4039 - val_acc: 0.8590\n",
      "Epoch 46/1000\n",
      "66/66 - 0s - loss: 0.0594 - acc: 0.9910 - val_loss: 0.3535 - val_acc: 0.8718\n",
      "Epoch 47/1000\n",
      "66/66 - 0s - loss: 0.0657 - acc: 0.9881 - val_loss: 0.3573 - val_acc: 0.8462\n",
      "Epoch 48/1000\n",
      "66/66 - 0s - loss: 0.0577 - acc: 0.9895 - val_loss: 0.3713 - val_acc: 0.8718\n",
      "Epoch 49/1000\n",
      "66/66 - 0s - loss: 0.0621 - acc: 0.9886 - val_loss: 0.3596 - val_acc: 0.8675\n",
      "Epoch 50/1000\n",
      "66/66 - 0s - loss: 0.0596 - acc: 0.9891 - val_loss: 0.3984 - val_acc: 0.8675\n",
      "Epoch 51/1000\n",
      "66/66 - 0s - loss: 0.0638 - acc: 0.9881 - val_loss: 0.3635 - val_acc: 0.8675\n",
      "Epoch 52/1000\n",
      "66/66 - 0s - loss: 0.0502 - acc: 0.9938 - val_loss: 0.3570 - val_acc: 0.8675\n",
      "Epoch 53/1000\n",
      "66/66 - 0s - loss: 0.0552 - acc: 0.9895 - val_loss: 0.3658 - val_acc: 0.8846\n",
      "Epoch 54/1000\n",
      "66/66 - 0s - loss: 0.0555 - acc: 0.9900 - val_loss: 0.3691 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 55/1000\n",
      "66/66 - 0s - loss: 0.0514 - acc: 0.9938 - val_loss: 0.3735 - val_acc: 0.8803\n",
      "Epoch 56/1000\n",
      "66/66 - 0s - loss: 0.0458 - acc: 0.9948 - val_loss: 0.3728 - val_acc: 0.8803\n",
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 0.9506 - acc: 0.6207 - val_loss: 1.4145 - val_acc: 0.2521\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.5934 - acc: 0.7915 - val_loss: 1.5679 - val_acc: 0.2521\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4849 - acc: 0.8291 - val_loss: 2.0496 - val_acc: 0.2521\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4502 - acc: 0.8372 - val_loss: 2.4231 - val_acc: 0.2521\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4183 - acc: 0.8477 - val_loss: 4.2969 - val_acc: 0.2521\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3772 - acc: 0.8696 - val_loss: 2.4530 - val_acc: 0.2521\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3712 - acc: 0.8677 - val_loss: 2.5378 - val_acc: 0.2607\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3680 - acc: 0.8710 - val_loss: 1.8851 - val_acc: 0.4957\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3555 - acc: 0.8701 - val_loss: 0.7021 - val_acc: 0.7393\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3453 - acc: 0.8820 - val_loss: 0.5849 - val_acc: 0.7949\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3159 - acc: 0.8805 - val_loss: 0.6360 - val_acc: 0.7479\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.3237 - acc: 0.8839 - val_loss: 0.9775 - val_acc: 0.6624\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.3136 - acc: 0.8872 - val_loss: 1.3089 - val_acc: 0.5812\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2967 - acc: 0.8915 - val_loss: 0.6163 - val_acc: 0.7607\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2821 - acc: 0.8986 - val_loss: 0.4495 - val_acc: 0.8333\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2893 - acc: 0.8886 - val_loss: 0.4519 - val_acc: 0.8291\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2751 - acc: 0.9015 - val_loss: 0.8238 - val_acc: 0.7094\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2540 - acc: 0.9053 - val_loss: 0.5528 - val_acc: 0.8120\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2518 - acc: 0.9072 - val_loss: 0.4675 - val_acc: 0.8162\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2439 - acc: 0.9115 - val_loss: 0.5928 - val_acc: 0.7991\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2421 - acc: 0.9081 - val_loss: 0.5362 - val_acc: 0.7735\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2358 - acc: 0.9124 - val_loss: 0.5037 - val_acc: 0.8248\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2224 - acc: 0.9200 - val_loss: 0.5150 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.1801 - acc: 0.9424 - val_loss: 0.4470 - val_acc: 0.8120\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.1656 - acc: 0.9472 - val_loss: 0.4140 - val_acc: 0.8248\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.1603 - acc: 0.9500 - val_loss: 0.4112 - val_acc: 0.8291\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1586 - acc: 0.9472 - val_loss: 0.4231 - val_acc: 0.8333\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1716 - acc: 0.9453 - val_loss: 0.4240 - val_acc: 0.8205\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1550 - acc: 0.9515 - val_loss: 0.4108 - val_acc: 0.8376\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1533 - acc: 0.9524 - val_loss: 0.4150 - val_acc: 0.8248\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1400 - acc: 0.9581 - val_loss: 0.4120 - val_acc: 0.8504\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1411 - acc: 0.9600 - val_loss: 0.4051 - val_acc: 0.8419\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.1433 - acc: 0.9562 - val_loss: 0.4192 - val_acc: 0.8462\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.1457 - acc: 0.9505 - val_loss: 0.4260 - val_acc: 0.8376\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.1339 - acc: 0.9586 - val_loss: 0.4126 - val_acc: 0.8291\n",
      "Epoch 36/1000\n",
      "66/66 - 0s - loss: 0.1328 - acc: 0.9595 - val_loss: 0.4134 - val_acc: 0.8333\n",
      "Epoch 37/1000\n",
      "66/66 - 0s - loss: 0.1387 - acc: 0.9562 - val_loss: 0.4149 - val_acc: 0.8333\n",
      "Epoch 38/1000\n",
      "66/66 - 0s - loss: 0.1501 - acc: 0.9538 - val_loss: 0.4478 - val_acc: 0.8291\n",
      "Epoch 39/1000\n",
      "66/66 - 0s - loss: 0.1260 - acc: 0.9595 - val_loss: 0.4230 - val_acc: 0.8504\n",
      "Epoch 40/1000\n",
      "66/66 - 0s - loss: 0.1377 - acc: 0.9591 - val_loss: 0.4341 - val_acc: 0.8376\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 41/1000\n",
      "66/66 - 0s - loss: 0.1327 - acc: 0.9653 - val_loss: 0.4294 - val_acc: 0.8419\n",
      "Epoch 42/1000\n",
      "66/66 - 0s - loss: 0.1164 - acc: 0.9667 - val_loss: 0.4282 - val_acc: 0.8333\n",
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 0.9995 - acc: 0.6040 - val_loss: 1.4302 - val_acc: 0.2393\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.6058 - acc: 0.7853 - val_loss: 1.6907 - val_acc: 0.2393\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4881 - acc: 0.8258 - val_loss: 3.3478 - val_acc: 0.2393\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4278 - acc: 0.8458 - val_loss: 4.3855 - val_acc: 0.2393\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.3966 - acc: 0.8491 - val_loss: 4.0951 - val_acc: 0.2393\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.4017 - acc: 0.8486 - val_loss: 5.3747 - val_acc: 0.2393\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3613 - acc: 0.8663 - val_loss: 4.6562 - val_acc: 0.2393\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3712 - acc: 0.8615 - val_loss: 2.9240 - val_acc: 0.2949\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3336 - acc: 0.8782 - val_loss: 1.3538 - val_acc: 0.4915\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3182 - acc: 0.8815 - val_loss: 0.5972 - val_acc: 0.7821\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3221 - acc: 0.8777 - val_loss: 0.5504 - val_acc: 0.7564\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.3131 - acc: 0.8867 - val_loss: 0.5124 - val_acc: 0.8162\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.3048 - acc: 0.8905 - val_loss: 0.4977 - val_acc: 0.7863\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2868 - acc: 0.8948 - val_loss: 0.5438 - val_acc: 0.7949\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2892 - acc: 0.8934 - val_loss: 0.4979 - val_acc: 0.8034\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2697 - acc: 0.9015 - val_loss: 0.5192 - val_acc: 0.8120\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2570 - acc: 0.9081 - val_loss: 0.5029 - val_acc: 0.8162\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2447 - acc: 0.9124 - val_loss: 0.4717 - val_acc: 0.8504\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2418 - acc: 0.9181 - val_loss: 0.3806 - val_acc: 0.8632\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2412 - acc: 0.9153 - val_loss: 0.4396 - val_acc: 0.8291\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2272 - acc: 0.9205 - val_loss: 0.5853 - val_acc: 0.8120\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2172 - acc: 0.9267 - val_loss: 0.5227 - val_acc: 0.7735\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2083 - acc: 0.9286 - val_loss: 0.3844 - val_acc: 0.8590\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2076 - acc: 0.9262 - val_loss: 0.4217 - val_acc: 0.8419\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.1981 - acc: 0.9343 - val_loss: 0.4206 - val_acc: 0.8376\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.2066 - acc: 0.9262 - val_loss: 0.4452 - val_acc: 0.8376\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1854 - acc: 0.9324 - val_loss: 0.4145 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1476 - acc: 0.9524 - val_loss: 0.3919 - val_acc: 0.8675\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1330 - acc: 0.9600 - val_loss: 0.3699 - val_acc: 0.8761\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1179 - acc: 0.9676 - val_loss: 0.3914 - val_acc: 0.8547\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1198 - acc: 0.9667 - val_loss: 0.3579 - val_acc: 0.8846\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1114 - acc: 0.9681 - val_loss: 0.3904 - val_acc: 0.8761\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.1188 - acc: 0.9681 - val_loss: 0.3790 - val_acc: 0.8761\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.1126 - acc: 0.9719 - val_loss: 0.3733 - val_acc: 0.8632\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.1067 - acc: 0.9724 - val_loss: 0.3714 - val_acc: 0.8632\n",
      "Epoch 36/1000\n",
      "66/66 - 0s - loss: 0.1035 - acc: 0.9733 - val_loss: 0.3828 - val_acc: 0.8675\n",
      "Epoch 37/1000\n",
      "66/66 - 0s - loss: 0.1041 - acc: 0.9729 - val_loss: 0.3715 - val_acc: 0.8675\n",
      "Epoch 38/1000\n",
      "66/66 - 0s - loss: 0.1012 - acc: 0.9743 - val_loss: 0.3718 - val_acc: 0.8718\n",
      "Epoch 39/1000\n",
      "66/66 - 0s - loss: 0.1077 - acc: 0.9719 - val_loss: 0.3554 - val_acc: 0.8718\n",
      "Epoch 40/1000\n",
      "66/66 - 0s - loss: 0.1087 - acc: 0.9714 - val_loss: 0.3666 - val_acc: 0.8803\n",
      "Epoch 41/1000\n",
      "66/66 - 0s - loss: 0.1054 - acc: 0.9719 - val_loss: 0.3621 - val_acc: 0.8803\n",
      "Epoch 42/1000\n",
      "66/66 - 0s - loss: 0.0998 - acc: 0.9743 - val_loss: 0.4038 - val_acc: 0.8675\n",
      "Epoch 43/1000\n",
      "66/66 - 0s - loss: 0.1082 - acc: 0.9695 - val_loss: 0.3716 - val_acc: 0.8718\n",
      "Epoch 44/1000\n",
      "66/66 - 0s - loss: 0.0944 - acc: 0.9762 - val_loss: 0.3758 - val_acc: 0.8632\n",
      "Epoch 45/1000\n",
      "66/66 - 0s - loss: 0.0953 - acc: 0.9767 - val_loss: 0.3957 - val_acc: 0.8462\n",
      "Epoch 46/1000\n",
      "66/66 - 0s - loss: 0.0954 - acc: 0.9752 - val_loss: 0.3967 - val_acc: 0.8504\n",
      "Epoch 47/1000\n",
      "66/66 - 0s - loss: 0.0972 - acc: 0.9767 - val_loss: 0.4002 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 48/1000\n",
      "66/66 - 0s - loss: 0.0900 - acc: 0.9791 - val_loss: 0.3873 - val_acc: 0.8632\n",
      "Epoch 49/1000\n",
      "66/66 - 0s - loss: 0.0857 - acc: 0.9810 - val_loss: 0.3845 - val_acc: 0.8632\n",
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 0.9919 - acc: 0.5812 - val_loss: 1.4033 - val_acc: 0.2564\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.6261 - acc: 0.7677 - val_loss: 1.5713 - val_acc: 0.2564\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4998 - acc: 0.8158 - val_loss: 2.0859 - val_acc: 0.2564\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4565 - acc: 0.8363 - val_loss: 2.8620 - val_acc: 0.2436\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4139 - acc: 0.8453 - val_loss: 3.3511 - val_acc: 0.2479\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3913 - acc: 0.8563 - val_loss: 3.4107 - val_acc: 0.2564\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3957 - acc: 0.8548 - val_loss: 2.3976 - val_acc: 0.4359\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3636 - acc: 0.8582 - val_loss: 2.2901 - val_acc: 0.5983\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3535 - acc: 0.8663 - val_loss: 1.1171 - val_acc: 0.6282\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3406 - acc: 0.8686 - val_loss: 0.7655 - val_acc: 0.7436\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3357 - acc: 0.8720 - val_loss: 0.9313 - val_acc: 0.6923\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.3278 - acc: 0.8782 - val_loss: 0.5908 - val_acc: 0.8077\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.3053 - acc: 0.8829 - val_loss: 0.7163 - val_acc: 0.7393\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2927 - acc: 0.8910 - val_loss: 0.5987 - val_acc: 0.7564\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.3022 - acc: 0.8891 - val_loss: 1.3074 - val_acc: 0.5855\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2820 - acc: 0.8948 - val_loss: 1.0033 - val_acc: 0.6325\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2760 - acc: 0.9039 - val_loss: 2.0870 - val_acc: 0.5128\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2522 - acc: 0.9043 - val_loss: 1.0086 - val_acc: 0.6624\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2476 - acc: 0.9081 - val_loss: 0.6402 - val_acc: 0.7650\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2443 - acc: 0.9077 - val_loss: 0.5406 - val_acc: 0.8333\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2373 - acc: 0.9115 - val_loss: 1.0278 - val_acc: 0.6453\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2464 - acc: 0.9096 - val_loss: 0.4948 - val_acc: 0.8291\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2319 - acc: 0.9158 - val_loss: 0.6821 - val_acc: 0.7521\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2164 - acc: 0.9181 - val_loss: 0.5650 - val_acc: 0.8333\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.2089 - acc: 0.9238 - val_loss: 0.4575 - val_acc: 0.8376\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.1911 - acc: 0.9367 - val_loss: 0.4799 - val_acc: 0.8547\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1824 - acc: 0.9381 - val_loss: 0.5425 - val_acc: 0.7949\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1689 - acc: 0.9348 - val_loss: 0.5617 - val_acc: 0.8077\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1742 - acc: 0.9362 - val_loss: 0.4688 - val_acc: 0.8333\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1704 - acc: 0.9457 - val_loss: 1.5710 - val_acc: 0.6282\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1660 - acc: 0.9438 - val_loss: 0.6898 - val_acc: 0.7863\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1459 - acc: 0.9495 - val_loss: 0.5415 - val_acc: 0.8333\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.1377 - acc: 0.9553 - val_loss: 0.5331 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.1154 - acc: 0.9657 - val_loss: 0.4738 - val_acc: 0.8462\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.0962 - acc: 0.9743 - val_loss: 0.4800 - val_acc: 0.8547\n",
      "Epoch 1/1000\n",
      "66/66 - 1s - loss: 0.9670 - acc: 0.6104 - val_loss: 1.4046 - val_acc: 0.2532\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.5986 - acc: 0.7916 - val_loss: 1.4989 - val_acc: 0.2575\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.5327 - acc: 0.8069 - val_loss: 2.5302 - val_acc: 0.2575\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4617 - acc: 0.8316 - val_loss: 3.6033 - val_acc: 0.2575\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4294 - acc: 0.8354 - val_loss: 4.2304 - val_acc: 0.2575\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.4128 - acc: 0.8473 - val_loss: 4.8635 - val_acc: 0.2575\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3990 - acc: 0.8544 - val_loss: 4.4606 - val_acc: 0.2747\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3710 - acc: 0.8673 - val_loss: 3.8588 - val_acc: 0.2961\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3674 - acc: 0.8658 - val_loss: 1.5070 - val_acc: 0.5966\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3102 - acc: 0.8853 - val_loss: 0.7347 - val_acc: 0.7167\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.2863 - acc: 0.8911 - val_loss: 0.4717 - val_acc: 0.8155\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.2895 - acc: 0.8953 - val_loss: 0.4328 - val_acc: 0.8369\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.2814 - acc: 0.8977 - val_loss: 0.4072 - val_acc: 0.8369\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2741 - acc: 0.8939 - val_loss: 0.3522 - val_acc: 0.8541\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2615 - acc: 0.9077 - val_loss: 0.3517 - val_acc: 0.8498\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2561 - acc: 0.9148 - val_loss: 0.3491 - val_acc: 0.8627\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2578 - acc: 0.9087 - val_loss: 0.3441 - val_acc: 0.8584\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2486 - acc: 0.9139 - val_loss: 0.3615 - val_acc: 0.8670\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2484 - acc: 0.9091 - val_loss: 0.3685 - val_acc: 0.8670\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2575 - acc: 0.9096 - val_loss: 0.3539 - val_acc: 0.8712\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2439 - acc: 0.9129 - val_loss: 0.3534 - val_acc: 0.8498\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2479 - acc: 0.9077 - val_loss: 0.3443 - val_acc: 0.8541\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2389 - acc: 0.9201 - val_loss: 0.3412 - val_acc: 0.8798\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2398 - acc: 0.9139 - val_loss: 0.3696 - val_acc: 0.8712\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.2321 - acc: 0.9210 - val_loss: 0.3409 - val_acc: 0.8670\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.2364 - acc: 0.9206 - val_loss: 0.3454 - val_acc: 0.8755\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.2225 - acc: 0.9248 - val_loss: 0.3856 - val_acc: 0.8541\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.2335 - acc: 0.9163 - val_loss: 0.3559 - val_acc: 0.8498\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.2265 - acc: 0.9248 - val_loss: 0.3419 - val_acc: 0.8755\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.2232 - acc: 0.9196 - val_loss: 0.3592 - val_acc: 0.8712\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.2175 - acc: 0.9291 - val_loss: 0.3523 - val_acc: 0.8712\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.2115 - acc: 0.9291 - val_loss: 0.3333 - val_acc: 0.8798\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.2183 - acc: 0.9263 - val_loss: 0.4026 - val_acc: 0.8498\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.2101 - acc: 0.9272 - val_loss: 0.3495 - val_acc: 0.8670\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.2041 - acc: 0.9358 - val_loss: 0.3550 - val_acc: 0.8584\n",
      "Epoch 36/1000\n",
      "66/66 - 0s - loss: 0.2153 - acc: 0.9191 - val_loss: 0.3425 - val_acc: 0.8841\n",
      "Epoch 37/1000\n",
      "66/66 - 0s - loss: 0.2056 - acc: 0.9324 - val_loss: 0.3752 - val_acc: 0.8584\n",
      "Epoch 38/1000\n",
      "66/66 - 0s - loss: 0.2094 - acc: 0.9353 - val_loss: 0.3619 - val_acc: 0.8755\n",
      "Epoch 39/1000\n",
      "66/66 - 0s - loss: 0.2103 - acc: 0.9296 - val_loss: 0.3489 - val_acc: 0.8712\n",
      "Epoch 40/1000\n",
      "66/66 - 0s - loss: 0.1867 - acc: 0.9386 - val_loss: 0.3628 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 41/1000\n",
      "66/66 - 0s - loss: 0.1969 - acc: 0.9391 - val_loss: 0.3461 - val_acc: 0.8584\n",
      "Epoch 42/1000\n",
      "66/66 - 0s - loss: 0.1920 - acc: 0.9377 - val_loss: 0.3412 - val_acc: 0.8712\n",
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 0.9269 - acc: 0.6246 - val_loss: 1.4104 - val_acc: 0.2532\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.5807 - acc: 0.7892 - val_loss: 1.5886 - val_acc: 0.2446\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4836 - acc: 0.8230 - val_loss: 1.9488 - val_acc: 0.3047\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4281 - acc: 0.8482 - val_loss: 2.3665 - val_acc: 0.2446\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.3900 - acc: 0.8587 - val_loss: 2.9217 - val_acc: 0.3176\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3885 - acc: 0.8611 - val_loss: 2.6065 - val_acc: 0.3004\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3597 - acc: 0.8692 - val_loss: 2.6769 - val_acc: 0.3348\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3411 - acc: 0.8758 - val_loss: 1.7740 - val_acc: 0.6309\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3271 - acc: 0.8839 - val_loss: 1.6341 - val_acc: 0.6352\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.2863 - acc: 0.9020 - val_loss: 0.9859 - val_acc: 0.7082\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.2713 - acc: 0.9049 - val_loss: 0.5241 - val_acc: 0.8240\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.2606 - acc: 0.9053 - val_loss: 0.3829 - val_acc: 0.8627\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.2607 - acc: 0.9077 - val_loss: 0.3656 - val_acc: 0.8670\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2482 - acc: 0.9129 - val_loss: 0.3705 - val_acc: 0.8627\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2436 - acc: 0.9196 - val_loss: 0.3560 - val_acc: 0.8670\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2472 - acc: 0.9139 - val_loss: 0.3575 - val_acc: 0.8755\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2351 - acc: 0.9206 - val_loss: 0.3867 - val_acc: 0.8498\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2441 - acc: 0.9120 - val_loss: 0.3558 - val_acc: 0.8712\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2329 - acc: 0.9244 - val_loss: 0.3620 - val_acc: 0.8712\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2415 - acc: 0.9182 - val_loss: 0.3879 - val_acc: 0.8498\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2245 - acc: 0.9248 - val_loss: 0.3743 - val_acc: 0.8627\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2134 - acc: 0.9263 - val_loss: 0.3924 - val_acc: 0.8584\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2350 - acc: 0.9163 - val_loss: 0.3808 - val_acc: 0.8627\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2211 - acc: 0.9253 - val_loss: 0.3717 - val_acc: 0.8584\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.2204 - acc: 0.9263 - val_loss: 0.3868 - val_acc: 0.8627\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.2210 - acc: 0.9210 - val_loss: 0.3706 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.2082 - acc: 0.9267 - val_loss: 0.3687 - val_acc: 0.8670\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.2067 - acc: 0.9286 - val_loss: 0.3661 - val_acc: 0.8584\n",
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 0.9542 - acc: 0.6037 - val_loss: 1.3919 - val_acc: 0.2575\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.5772 - acc: 0.7969 - val_loss: 1.4104 - val_acc: 0.3004\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4688 - acc: 0.8416 - val_loss: 1.8192 - val_acc: 0.2446\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4402 - acc: 0.8387 - val_loss: 3.4863 - val_acc: 0.2446\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4092 - acc: 0.8525 - val_loss: 2.9101 - val_acc: 0.2446\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3731 - acc: 0.8716 - val_loss: 1.0772 - val_acc: 0.5193\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3658 - acc: 0.8692 - val_loss: 2.8923 - val_acc: 0.2833\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3478 - acc: 0.8716 - val_loss: 1.0665 - val_acc: 0.5408\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3388 - acc: 0.8763 - val_loss: 0.7113 - val_acc: 0.7468\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3616 - acc: 0.8658 - val_loss: 1.4195 - val_acc: 0.5579\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3286 - acc: 0.8768 - val_loss: 0.8598 - val_acc: 0.6695\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.3081 - acc: 0.8868 - val_loss: 0.9002 - val_acc: 0.6910\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.3010 - acc: 0.8982 - val_loss: 2.2578 - val_acc: 0.5494\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2990 - acc: 0.8901 - val_loss: 0.6962 - val_acc: 0.7382\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2914 - acc: 0.8901 - val_loss: 1.9323 - val_acc: 0.5107\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2758 - acc: 0.8944 - val_loss: 0.6895 - val_acc: 0.7210\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2611 - acc: 0.9068 - val_loss: 1.0685 - val_acc: 0.6567\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2646 - acc: 0.9001 - val_loss: 0.6729 - val_acc: 0.6953\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2451 - acc: 0.9087 - val_loss: 0.3737 - val_acc: 0.8627\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.2503 - acc: 0.9020 - val_loss: 0.6826 - val_acc: 0.7339\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.2423 - acc: 0.9106 - val_loss: 0.8253 - val_acc: 0.7210\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.2261 - acc: 0.9144 - val_loss: 0.4267 - val_acc: 0.8197\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.2174 - acc: 0.9225 - val_loss: 0.7617 - val_acc: 0.7039\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.2139 - acc: 0.9229 - val_loss: 0.5069 - val_acc: 0.7940\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.2020 - acc: 0.9343 - val_loss: 0.2937 - val_acc: 0.9099\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.2022 - acc: 0.9277 - val_loss: 0.4538 - val_acc: 0.8326\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1933 - acc: 0.9305 - val_loss: 0.4149 - val_acc: 0.8584\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1727 - acc: 0.9439 - val_loss: 0.4905 - val_acc: 0.8283\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1748 - acc: 0.9424 - val_loss: 0.3965 - val_acc: 0.8498\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1683 - acc: 0.9443 - val_loss: 0.3491 - val_acc: 0.8712\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1770 - acc: 0.9363 - val_loss: 0.4228 - val_acc: 0.8541\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1756 - acc: 0.9377 - val_loss: 0.4300 - val_acc: 0.8412\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.1633 - acc: 0.9424 - val_loss: 0.3723 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.1215 - acc: 0.9624 - val_loss: 0.3032 - val_acc: 0.8841\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.1098 - acc: 0.9691 - val_loss: 0.2893 - val_acc: 0.8884\n",
      "Epoch 36/1000\n",
      "66/66 - 0s - loss: 0.0941 - acc: 0.9781 - val_loss: 0.2829 - val_acc: 0.8927\n",
      "Epoch 37/1000\n",
      "66/66 - 0s - loss: 0.1093 - acc: 0.9653 - val_loss: 0.2793 - val_acc: 0.9013\n",
      "Epoch 38/1000\n",
      "66/66 - 0s - loss: 0.0977 - acc: 0.9729 - val_loss: 0.2723 - val_acc: 0.9056\n",
      "Epoch 39/1000\n",
      "66/66 - 0s - loss: 0.0977 - acc: 0.9748 - val_loss: 0.2735 - val_acc: 0.9013\n",
      "Epoch 40/1000\n",
      "66/66 - 0s - loss: 0.0967 - acc: 0.9700 - val_loss: 0.2809 - val_acc: 0.9013\n",
      "Epoch 41/1000\n",
      "66/66 - 0s - loss: 0.0990 - acc: 0.9738 - val_loss: 0.2780 - val_acc: 0.9013\n",
      "Epoch 42/1000\n",
      "66/66 - 0s - loss: 0.0885 - acc: 0.9757 - val_loss: 0.2812 - val_acc: 0.8970\n",
      "Epoch 43/1000\n",
      "66/66 - 0s - loss: 0.0902 - acc: 0.9791 - val_loss: 0.2859 - val_acc: 0.8970\n",
      "Epoch 44/1000\n",
      "66/66 - 0s - loss: 0.0809 - acc: 0.9805 - val_loss: 0.2839 - val_acc: 0.8884\n",
      "Epoch 45/1000\n",
      "66/66 - 0s - loss: 0.0888 - acc: 0.9776 - val_loss: 0.2814 - val_acc: 0.8970\n",
      "Epoch 46/1000\n",
      "66/66 - 0s - loss: 0.0878 - acc: 0.9781 - val_loss: 0.2805 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 47/1000\n",
      "66/66 - 0s - loss: 0.0901 - acc: 0.9767 - val_loss: 0.2827 - val_acc: 0.9013\n",
      "Epoch 48/1000\n",
      "66/66 - 0s - loss: 0.0770 - acc: 0.9814 - val_loss: 0.2822 - val_acc: 0.9013\n",
      "Epoch 1/1000\n",
      "66/66 - 1s - loss: 0.9546 - acc: 0.6237 - val_loss: 1.4298 - val_acc: 0.2446\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.6033 - acc: 0.7726 - val_loss: 1.7770 - val_acc: 0.2446\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.5136 - acc: 0.8173 - val_loss: 2.1412 - val_acc: 0.2446\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4444 - acc: 0.8421 - val_loss: 3.3096 - val_acc: 0.2446\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4244 - acc: 0.8444 - val_loss: 2.1892 - val_acc: 0.2446\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3913 - acc: 0.8606 - val_loss: 3.0813 - val_acc: 0.2446\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3718 - acc: 0.8677 - val_loss: 2.5559 - val_acc: 0.2532\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3612 - acc: 0.8735 - val_loss: 1.5989 - val_acc: 0.3605\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3406 - acc: 0.8811 - val_loss: 0.6443 - val_acc: 0.7425\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3411 - acc: 0.8711 - val_loss: 0.5175 - val_acc: 0.7983\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3157 - acc: 0.8777 - val_loss: 0.4848 - val_acc: 0.8069\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.2990 - acc: 0.8887 - val_loss: 1.6778 - val_acc: 0.4635\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.3006 - acc: 0.8896 - val_loss: 0.8824 - val_acc: 0.6567\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2835 - acc: 0.8896 - val_loss: 0.5024 - val_acc: 0.8069\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2745 - acc: 0.8963 - val_loss: 0.5111 - val_acc: 0.8069\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2555 - acc: 0.9044 - val_loss: 0.5388 - val_acc: 0.8069\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2405 - acc: 0.9125 - val_loss: 0.8253 - val_acc: 0.7296\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2446 - acc: 0.9091 - val_loss: 0.8243 - val_acc: 0.6910\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2513 - acc: 0.9082 - val_loss: 0.5173 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.1915 - acc: 0.9358 - val_loss: 0.4367 - val_acc: 0.8326\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.1840 - acc: 0.9382 - val_loss: 0.4324 - val_acc: 0.8412\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.1749 - acc: 0.9363 - val_loss: 0.4250 - val_acc: 0.8455\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.1717 - acc: 0.9443 - val_loss: 0.4593 - val_acc: 0.8326\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.1613 - acc: 0.9472 - val_loss: 0.4350 - val_acc: 0.8455\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.1606 - acc: 0.9448 - val_loss: 0.4416 - val_acc: 0.8412\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.1575 - acc: 0.9510 - val_loss: 0.4356 - val_acc: 0.8498\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1554 - acc: 0.9524 - val_loss: 0.4356 - val_acc: 0.8326\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1586 - acc: 0.9467 - val_loss: 0.4338 - val_acc: 0.8412\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1563 - acc: 0.9524 - val_loss: 0.4420 - val_acc: 0.8369\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1513 - acc: 0.9543 - val_loss: 0.4499 - val_acc: 0.8326\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1487 - acc: 0.9558 - val_loss: 0.4410 - val_acc: 0.8326\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1470 - acc: 0.9486 - val_loss: 0.4386 - val_acc: 0.8369\n",
      "Epoch 1/1000\n",
      "66/66 - 0s - loss: 0.9744 - acc: 0.5994 - val_loss: 1.4474 - val_acc: 0.2532\n",
      "Epoch 2/1000\n",
      "66/66 - 0s - loss: 0.5868 - acc: 0.7921 - val_loss: 1.6516 - val_acc: 0.2532\n",
      "Epoch 3/1000\n",
      "66/66 - 0s - loss: 0.4965 - acc: 0.8221 - val_loss: 2.0604 - val_acc: 0.2532\n",
      "Epoch 4/1000\n",
      "66/66 - 0s - loss: 0.4443 - acc: 0.8387 - val_loss: 2.3830 - val_acc: 0.2532\n",
      "Epoch 5/1000\n",
      "66/66 - 0s - loss: 0.4179 - acc: 0.8482 - val_loss: 3.2288 - val_acc: 0.2661\n",
      "Epoch 6/1000\n",
      "66/66 - 0s - loss: 0.3768 - acc: 0.8639 - val_loss: 2.5417 - val_acc: 0.2661\n",
      "Epoch 7/1000\n",
      "66/66 - 0s - loss: 0.3808 - acc: 0.8544 - val_loss: 2.3557 - val_acc: 0.4292\n",
      "Epoch 8/1000\n",
      "66/66 - 0s - loss: 0.3586 - acc: 0.8735 - val_loss: 1.0441 - val_acc: 0.5708\n",
      "Epoch 9/1000\n",
      "66/66 - 0s - loss: 0.3278 - acc: 0.8820 - val_loss: 0.7700 - val_acc: 0.6953\n",
      "Epoch 10/1000\n",
      "66/66 - 0s - loss: 0.3199 - acc: 0.8834 - val_loss: 0.6804 - val_acc: 0.7468\n",
      "Epoch 11/1000\n",
      "66/66 - 0s - loss: 0.3274 - acc: 0.8768 - val_loss: 0.3503 - val_acc: 0.8712\n",
      "Epoch 12/1000\n",
      "66/66 - 0s - loss: 0.3053 - acc: 0.8853 - val_loss: 0.7593 - val_acc: 0.7511\n",
      "Epoch 13/1000\n",
      "66/66 - 0s - loss: 0.2885 - acc: 0.8858 - val_loss: 0.5126 - val_acc: 0.7983\n",
      "Epoch 14/1000\n",
      "66/66 - 0s - loss: 0.2825 - acc: 0.9010 - val_loss: 1.1060 - val_acc: 0.5622\n",
      "Epoch 15/1000\n",
      "66/66 - 0s - loss: 0.2692 - acc: 0.9006 - val_loss: 0.4580 - val_acc: 0.8283\n",
      "Epoch 16/1000\n",
      "66/66 - 0s - loss: 0.2641 - acc: 0.9058 - val_loss: 0.5221 - val_acc: 0.7940\n",
      "Epoch 17/1000\n",
      "66/66 - 0s - loss: 0.2712 - acc: 0.8977 - val_loss: 0.5298 - val_acc: 0.7940\n",
      "Epoch 18/1000\n",
      "66/66 - 0s - loss: 0.2529 - acc: 0.9029 - val_loss: 0.7493 - val_acc: 0.7210\n",
      "Epoch 19/1000\n",
      "66/66 - 0s - loss: 0.2341 - acc: 0.9110 - val_loss: 0.3872 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/1000\n",
      "66/66 - 0s - loss: 0.1943 - acc: 0.9286 - val_loss: 0.3365 - val_acc: 0.8798\n",
      "Epoch 21/1000\n",
      "66/66 - 0s - loss: 0.1889 - acc: 0.9372 - val_loss: 0.2882 - val_acc: 0.9099\n",
      "Epoch 22/1000\n",
      "66/66 - 0s - loss: 0.1778 - acc: 0.9410 - val_loss: 0.2849 - val_acc: 0.9099\n",
      "Epoch 23/1000\n",
      "66/66 - 0s - loss: 0.1782 - acc: 0.9420 - val_loss: 0.2794 - val_acc: 0.9142\n",
      "Epoch 24/1000\n",
      "66/66 - 0s - loss: 0.1660 - acc: 0.9439 - val_loss: 0.2789 - val_acc: 0.9099\n",
      "Epoch 25/1000\n",
      "66/66 - 0s - loss: 0.1778 - acc: 0.9372 - val_loss: 0.2781 - val_acc: 0.9099\n",
      "Epoch 26/1000\n",
      "66/66 - 0s - loss: 0.1675 - acc: 0.9448 - val_loss: 0.2847 - val_acc: 0.9142\n",
      "Epoch 27/1000\n",
      "66/66 - 0s - loss: 0.1582 - acc: 0.9543 - val_loss: 0.2807 - val_acc: 0.9056\n",
      "Epoch 28/1000\n",
      "66/66 - 0s - loss: 0.1621 - acc: 0.9481 - val_loss: 0.2848 - val_acc: 0.8927\n",
      "Epoch 29/1000\n",
      "66/66 - 0s - loss: 0.1563 - acc: 0.9510 - val_loss: 0.2830 - val_acc: 0.9099\n",
      "Epoch 30/1000\n",
      "66/66 - 0s - loss: 0.1562 - acc: 0.9496 - val_loss: 0.2820 - val_acc: 0.9099\n",
      "Epoch 31/1000\n",
      "66/66 - 0s - loss: 0.1551 - acc: 0.9510 - val_loss: 0.2899 - val_acc: 0.8970\n",
      "Epoch 32/1000\n",
      "66/66 - 0s - loss: 0.1514 - acc: 0.9505 - val_loss: 0.2820 - val_acc: 0.9013\n",
      "Epoch 33/1000\n",
      "66/66 - 0s - loss: 0.1548 - acc: 0.9477 - val_loss: 0.2912 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 34/1000\n",
      "66/66 - 0s - loss: 0.1416 - acc: 0.9572 - val_loss: 0.2840 - val_acc: 0.9013\n",
      "Epoch 35/1000\n",
      "66/66 - 0s - loss: 0.1476 - acc: 0.9553 - val_loss: 0.2843 - val_acc: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36102750599384303"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score2 = 0\n",
    "result2 = 0\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 82, shuffle = True)\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "for train_index, valid_index in skf.split(train2,train[\"target\"]):\n",
    "    X_train, X_valid = train2[train_index], train2[valid_index]\n",
    "    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[valid_index]\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(2,2),activation = \"relu\", input_shape = (8,4,1), padding = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(2,2),activation = \"relu\" ,padding = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128,(2,2),activation = \"relu\" ,padding = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(4,activation = \"softmax\"))\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = \"acc\")\n",
    "    es = EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "    rl = ReduceLROnPlateau(patience = 8, verbose = 1)\n",
    "    history = model.fit(X_train,y_train, validation_data = (X_valid,y_valid), epochs = 1000, callbacks = [es,rl], verbose = 2)\n",
    "    best_score2 += np.min(history.history[\"val_loss\"]) / 10\n",
    "    result2 += model.predict(test2) / 10\n",
    "best_score2 #0.88888, 0.36387131214141843, splits = 5\n",
    "#0.89038, 0.3512733042240143, splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89eebaca-b284-461c-9676-bee787d726a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = 0\n",
    "# result = 0\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf = StratifiedKFold(n_splits = 5, random_state = 82, shuffle = True)\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras import *\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# for train_index, valid_index in skf.split(train2,train[\"target\"]):\n",
    "#     X_train, X_valid = train2[train_index], train2[valid_index]\n",
    "#     y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[valid_index]\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32,(2,2),activation = \"relu\", input_shape = (8,4,1), padding = \"same\"))\n",
    "#     model.add(MaxPooling2D(2,2))\n",
    "#     model.add(Conv2D(64,(2,2),activation = \"relu\" ,padding = \"same\"))\n",
    "#     model.add(MaxPooling2D(2,2))\n",
    "#     model.add(GlobalAveragePooling2D())\n",
    "#     model.add(Dense(4,activation = \"softmax\"))\n",
    "#     model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = \"acc\")\n",
    "#     es = EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "#     rl = ReduceLROnPlateau(patience = 8, verbose = 1)\n",
    "#     history = model.fit(X_train,y_train, validation_data = (X_valid,y_valid), epochs = 1000, callbacks = [es,rl], verbose = 2)\n",
    "#     best_score += np.min(history.history[\"val_loss\"]) / 5\n",
    "#     result += model.predict(test2) / 5\n",
    "# best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f87bab-08bd-44b3-95a5-49c132fd10bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2180, 8, 4, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5b25122-bc30-4106-8c44-c9262e39ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = all_data2[:len(train)]\n",
    "test2 = all_data2[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f15a1f70-5a43-485f-938b-ec00021541f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3578648\ttest: 1.3619872\tbest: 1.3619872 (0)\ttotal: 14.7ms\tremaining: 4h 5m 25s\n",
      "200:\tlearn: 0.3955585\ttest: 0.5652860\tbest: 0.5652860 (200)\ttotal: 1.38s\tremaining: 1h 54m 13s\n",
      "400:\tlearn: 0.2091501\ttest: 0.4549931\tbest: 0.4549931 (400)\ttotal: 2.75s\tremaining: 1h 54m 14s\n",
      "600:\tlearn: 0.1276049\ttest: 0.4276065\tbest: 0.4276065 (600)\ttotal: 4.11s\tremaining: 1h 53m 56s\n",
      "800:\tlearn: 0.0857094\ttest: 0.4116210\tbest: 0.4113112 (798)\ttotal: 5.47s\tremaining: 1h 53m 40s\n",
      "bestTest = 0.4052961056\n",
      "bestIteration = 913\n",
      "Shrink model to first 914 iterations.\n",
      "0:\tlearn: 1.3586232\ttest: 1.3588854\tbest: 1.3588854 (0)\ttotal: 7.58ms\tremaining: 2h 6m 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3873021\ttest: 0.6162133\tbest: 0.6162133 (200)\ttotal: 1.38s\tremaining: 1h 54m 44s\n",
      "400:\tlearn: 0.2032873\ttest: 0.5325871\tbest: 0.5325055 (399)\ttotal: 2.75s\tremaining: 1h 54m 12s\n",
      "bestTest = 0.5135640853\n",
      "bestIteration = 502\n",
      "Shrink model to first 503 iterations.\n",
      "0:\tlearn: 1.3574503\ttest: 1.3626451\tbest: 1.3626451 (0)\ttotal: 7.46ms\tremaining: 2h 4m 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3913806\ttest: 0.5642407\tbest: 0.5642407 (200)\ttotal: 1.38s\tremaining: 1h 54m 46s\n",
      "400:\tlearn: 0.2080663\ttest: 0.4494273\tbest: 0.4494273 (400)\ttotal: 2.75s\tremaining: 1h 54m 5s\n",
      "600:\tlearn: 0.1273175\ttest: 0.4031696\tbest: 0.4028276 (596)\ttotal: 4.11s\tremaining: 1h 53m 46s\n",
      "bestTest = 0.3994134267\n",
      "bestIteration = 632\n",
      "Shrink model to first 633 iterations.\n",
      "0:\tlearn: 1.3596317\ttest: 1.3585006\tbest: 1.3585006 (0)\ttotal: 7.74ms\tremaining: 2h 8m 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3920254\ttest: 0.6030142\tbest: 0.6030142 (200)\ttotal: 1.4s\tremaining: 1h 55m 50s\n",
      "400:\tlearn: 0.2069287\ttest: 0.4973815\tbest: 0.4973815 (400)\ttotal: 2.79s\tremaining: 1h 56m 1s\n",
      "600:\tlearn: 0.1273317\ttest: 0.4705059\tbest: 0.4705059 (600)\ttotal: 4.17s\tremaining: 1h 55m 40s\n",
      "bestTest = 0.4612447788\n",
      "bestIteration = 737\n",
      "Shrink model to first 738 iterations.\n",
      "0:\tlearn: 1.3583100\ttest: 1.3608402\tbest: 1.3608402 (0)\ttotal: 7.67ms\tremaining: 2h 7m 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3942954\ttest: 0.5872478\tbest: 0.5872478 (200)\ttotal: 1.41s\tremaining: 1h 56m 59s\n",
      "400:\tlearn: 0.2092019\ttest: 0.4961807\tbest: 0.4961807 (400)\ttotal: 2.79s\tremaining: 1h 55m 42s\n",
      "600:\tlearn: 0.1293771\ttest: 0.4625301\tbest: 0.4625301 (600)\ttotal: 4.2s\tremaining: 1h 56m 15s\n",
      "800:\tlearn: 0.0863398\ttest: 0.4383181\tbest: 0.4377384 (795)\ttotal: 5.6s\tremaining: 1h 56m 21s\n",
      "1000:\tlearn: 0.0623051\ttest: 0.4264726\tbest: 0.4259574 (975)\ttotal: 6.99s\tremaining: 1h 56m 16s\n",
      "bestTest = 0.4259574108\n",
      "bestIteration = 975\n",
      "Shrink model to first 976 iterations.\n",
      "0:\tlearn: 1.3586010\ttest: 1.3596788\tbest: 1.3596788 (0)\ttotal: 7.68ms\tremaining: 2h 7m 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3939183\ttest: 0.5904504\tbest: 0.5904504 (200)\ttotal: 1.39s\tremaining: 1h 55m 13s\n",
      "400:\tlearn: 0.2072903\ttest: 0.5105848\tbest: 0.5096115 (398)\ttotal: 2.79s\tremaining: 1h 55m 53s\n",
      "600:\tlearn: 0.1269043\ttest: 0.4730404\tbest: 0.4730404 (600)\ttotal: 4.17s\tremaining: 1h 55m 36s\n",
      "bestTest = 0.4634278615\n",
      "bestIteration = 714\n",
      "Shrink model to first 715 iterations.\n",
      "0:\tlearn: 1.3572128\ttest: 1.3660904\tbest: 1.3660904 (0)\ttotal: 8.64ms\tremaining: 2h 24m 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3914290\ttest: 0.6319760\tbest: 0.6319760 (200)\ttotal: 1.39s\tremaining: 1h 55m 33s\n",
      "400:\tlearn: 0.2070061\ttest: 0.5384813\tbest: 0.5384813 (400)\ttotal: 2.76s\tremaining: 1h 54m 33s\n",
      "600:\tlearn: 0.1262871\ttest: 0.5047812\tbest: 0.5042161 (591)\ttotal: 4.14s\tremaining: 1h 54m 52s\n",
      "800:\tlearn: 0.0850013\ttest: 0.4896176\tbest: 0.4894928 (783)\ttotal: 5.53s\tremaining: 1h 54m 52s\n",
      "bestTest = 0.4843355815\n",
      "bestIteration = 861\n",
      "Shrink model to first 862 iterations.\n",
      "0:\tlearn: 1.3583298\ttest: 1.3561667\tbest: 1.3561667 (0)\ttotal: 7.71ms\tremaining: 2h 8m 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3928995\ttest: 0.5685088\tbest: 0.5685088 (200)\ttotal: 1.4s\tremaining: 1h 56m 2s\n",
      "400:\tlearn: 0.2049220\ttest: 0.4781967\tbest: 0.4781342 (399)\ttotal: 2.8s\tremaining: 1h 56m 15s\n",
      "600:\tlearn: 0.1260356\ttest: 0.4427988\tbest: 0.4427988 (600)\ttotal: 4.21s\tremaining: 1h 56m 36s\n",
      "bestTest = 0.4397962032\n",
      "bestIteration = 622\n",
      "Shrink model to first 623 iterations.\n",
      "0:\tlearn: 1.3576574\ttest: 1.3615550\tbest: 1.3615550 (0)\ttotal: 7.47ms\tremaining: 2h 4m 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3844408\ttest: 0.6540252\tbest: 0.6540252 (200)\ttotal: 1.41s\tremaining: 1h 56m 57s\n",
      "400:\tlearn: 0.2046240\ttest: 0.5509718\tbest: 0.5509643 (398)\ttotal: 2.83s\tremaining: 1h 57m 42s\n",
      "bestTest = 0.5273580307\n",
      "bestIteration = 528\n",
      "Shrink model to first 529 iterations.\n",
      "0:\tlearn: 1.3591635\ttest: 1.3640235\tbest: 1.3640235 (0)\ttotal: 8.15ms\tremaining: 2h 15m 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3882734\ttest: 0.6299476\tbest: 0.6299476 (200)\ttotal: 1.42s\tremaining: 1h 57m 46s\n",
      "400:\tlearn: 0.2055145\ttest: 0.5296471\tbest: 0.5295607 (399)\ttotal: 2.82s\tremaining: 1h 57m 7s\n",
      "600:\tlearn: 0.1270440\ttest: 0.4910828\tbest: 0.4910595 (598)\ttotal: 4.21s\tremaining: 1h 56m 44s\n",
      "800:\tlearn: 0.0856525\ttest: 0.4796923\tbest: 0.4791046 (782)\ttotal: 5.61s\tremaining: 1h 56m 43s\n",
      "bestTest = 0.4728084466\n",
      "bestIteration = 913\n",
      "Shrink model to first 914 iterations.\n",
      "0:\tlearn: 1.3606272\ttest: 1.3619271\tbest: 1.3619271 (0)\ttotal: 7.71ms\tremaining: 2h 8m 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.4013597\ttest: 0.5048749\tbest: 0.5048749 (200)\ttotal: 1.38s\tremaining: 1h 54m 39s\n",
      "400:\tlearn: 0.2110196\ttest: 0.3893760\tbest: 0.3893760 (400)\ttotal: 2.78s\tremaining: 1h 55m 36s\n",
      "600:\tlearn: 0.1319448\ttest: 0.3543027\tbest: 0.3543027 (600)\ttotal: 4.15s\tremaining: 1h 55m 6s\n",
      "800:\tlearn: 0.0874276\ttest: 0.3304218\tbest: 0.3302861 (799)\ttotal: 5.52s\tremaining: 1h 54m 47s\n",
      "1000:\tlearn: 0.0629801\ttest: 0.3172851\tbest: 0.3172851 (1000)\ttotal: 6.88s\tremaining: 1h 54m 25s\n",
      "bestTest = 0.3108369889\n",
      "bestIteration = 1146\n",
      "Shrink model to first 1147 iterations.\n",
      "0:\tlearn: 1.3587183\ttest: 1.3588610\tbest: 1.3588610 (0)\ttotal: 7.48ms\tremaining: 2h 4m 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3924782\ttest: 0.5452953\tbest: 0.5452953 (200)\ttotal: 1.37s\tremaining: 1h 53m 26s\n",
      "400:\tlearn: 0.2111435\ttest: 0.4433258\tbest: 0.4433258 (400)\ttotal: 2.73s\tremaining: 1h 53m 29s\n",
      "600:\tlearn: 0.1303716\ttest: 0.4079390\tbest: 0.4072581 (587)\ttotal: 4.1s\tremaining: 1h 53m 38s\n",
      "800:\tlearn: 0.0892508\ttest: 0.3897972\tbest: 0.3887637 (785)\ttotal: 5.51s\tremaining: 1h 54m 29s\n",
      "1000:\tlearn: 0.0639535\ttest: 0.3759809\tbest: 0.3756310 (995)\ttotal: 6.91s\tremaining: 1h 54m 59s\n",
      "1200:\tlearn: 0.0479116\ttest: 0.3656578\tbest: 0.3655306 (1183)\ttotal: 8.28s\tremaining: 1h 54m 46s\n",
      "bestTest = 0.3644219429\n",
      "bestIteration = 1227\n",
      "Shrink model to first 1228 iterations.\n",
      "0:\tlearn: 1.3570316\ttest: 1.3633378\tbest: 1.3633378 (0)\ttotal: 7.33ms\tremaining: 2h 2m 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3841769\ttest: 0.6792638\tbest: 0.6792638 (200)\ttotal: 1.37s\tremaining: 1h 53m 25s\n",
      "400:\tlearn: 0.2059408\ttest: 0.6030557\tbest: 0.6028752 (394)\ttotal: 2.73s\tremaining: 1h 53m 24s\n",
      "600:\tlearn: 0.1262613\ttest: 0.5764480\tbest: 0.5764480 (600)\ttotal: 4.09s\tremaining: 1h 53m 24s\n",
      "bestTest = 0.5681137085\n",
      "bestIteration = 675\n",
      "Shrink model to first 676 iterations.\n",
      "0:\tlearn: 1.3573485\ttest: 1.3619639\tbest: 1.3619639 (0)\ttotal: 7.61ms\tremaining: 2h 6m 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3968086\ttest: 0.5975473\tbest: 0.5975473 (200)\ttotal: 1.37s\tremaining: 1h 53m 51s\n",
      "400:\tlearn: 0.2101691\ttest: 0.4824009\tbest: 0.4824009 (400)\ttotal: 2.79s\tremaining: 1h 55m 47s\n",
      "600:\tlearn: 0.1285600\ttest: 0.4340417\tbest: 0.4337641 (595)\ttotal: 4.17s\tremaining: 1h 55m 37s\n",
      "800:\tlearn: 0.0864827\ttest: 0.4105267\tbest: 0.4105267 (800)\ttotal: 5.54s\tremaining: 1h 55m 7s\n",
      "1000:\tlearn: 0.0625067\ttest: 0.3974513\tbest: 0.3973511 (999)\ttotal: 6.9s\tremaining: 1h 54m 47s\n",
      "1200:\tlearn: 0.0472364\ttest: 0.3814640\tbest: 0.3814640 (1200)\ttotal: 8.27s\tremaining: 1h 54m 39s\n",
      "bestTest = 0.3805022455\n",
      "bestIteration = 1206\n",
      "Shrink model to first 1207 iterations.\n",
      "0:\tlearn: 1.3596707\ttest: 1.3600201\tbest: 1.3600201 (0)\ttotal: 7.53ms\tremaining: 2h 5m 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 350 Total: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:\tlearn: 0.3913036\ttest: 0.6399718\tbest: 0.6399718 (200)\ttotal: 1.38s\tremaining: 1h 54m 3s\n",
      "400:\tlearn: 0.2075664\ttest: 0.5419483\tbest: 0.5419483 (400)\ttotal: 2.78s\tremaining: 1h 55m 41s\n",
      "600:\tlearn: 0.1269615\ttest: 0.5053779\tbest: 0.5048761 (599)\ttotal: 4.15s\tremaining: 1h 54m 56s\n",
      "bestTest = 0.4898797804\n",
      "bestIteration = 752\n",
      "Shrink model to first 753 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4471304397961638"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score3 = 0\n",
    "result3 = 0\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 15, random_state = 82, shuffle = True)\n",
    "from catboost import CatBoostClassifier\n",
    "cbc = CatBoostClassifier(task_type = \"GPU\", verbose = 200, iterations = 1000000, learning_rate = 0.061114)\n",
    "for train_index, valid_index in skf.split(train2, train[\"target\"]):\n",
    "    X_train, X_valid = train2.iloc[train_index], train2.iloc[valid_index]\n",
    "    y_train, y_valid = train[\"target\"].iloc[train_index], train['target'].iloc[valid_index]\n",
    "    cbc.fit(X_train,y_train, eval_set = (X_valid,y_valid), early_stopping_rounds = 30)\n",
    "    result3 += cbc.predict_proba(test2) / 15\n",
    "    best_score3 += cbc.best_score_['validation']['MultiClass'] / 15\n",
    "best_score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd3be94-b476-4e58-8c27-748b5be3081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "448cefa6-cd24-41f5-8659-919af4af124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.46136449e-01, 1.43532192e-02, 3.48447592e-02, 4.66549556e-03],\n",
       "       [9.92951846e-01, 5.47450433e-04, 2.95794026e-03, 3.54267763e-03],\n",
       "       [4.54745140e-02, 8.77301863e-01, 4.26862779e-02, 3.45373420e-02],\n",
       "       ...,\n",
       "       [3.55874444e-01, 6.07380338e-04, 6.33082789e-01, 1.04354317e-02],\n",
       "       [9.97930046e-01, 2.28559862e-04, 1.08148475e-03, 7.59908748e-04],\n",
       "       [1.90146563e-05, 3.87945830e-03, 3.30148823e-03, 9.92799912e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final =  result * 0.45 + result2 * 0.45 + result3 * 0.1\n",
    "final # 0.8921"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bb256df-9614-40b4-8a02-a4e8bc9914c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>9339</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9339</th>\n",
       "      <td>9340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9340</th>\n",
       "      <td>9341</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9341</th>\n",
       "      <td>9342</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9342</th>\n",
       "      <td>9343</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9343 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  target\n",
       "0        1       0\n",
       "1        2       0\n",
       "2        3       1\n",
       "3        4       3\n",
       "4        5       2\n",
       "...    ...     ...\n",
       "9338  9339       3\n",
       "9339  9340       1\n",
       "9340  9341       2\n",
       "9341  9342       0\n",
       "9342  9343       3\n",
       "\n",
       "[9343 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"sample_Submission.csv\")\n",
    "sub[\"target\"] = final\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13dab85b-0d7a-496f-a1d4-7d5d63babe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sub1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af835d-9ae6-415e-b00e-4a568ac80f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8c8fa-6575-46ac-9456-1d07c9f99a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
